{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "#probably not needed\n",
    "tf.keras.utils.set_random_seed(42)  # sets seeds for base-python, numpy and tf\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "import random\n",
    "random.seed(42)\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import plotly.offline as pyo\n",
    "from plotly import subplots\n",
    "import plotly.graph_objects as go\n",
    "from baseFunctions import *\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.tsa.api as smt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_log_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense,Input,concatenate\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../train.csv')\n",
    "oil = pd.read_csv('../oil.csv')\n",
    "stores = pd.read_csv('../stores.csv')\n",
    "transactions = pd.read_csv('../transactions.csv')\n",
    "test = pd.read_csv('../test.csv')\n",
    "holidays = pd.read_csv('../holidays_events.csv')\n",
    "sampleSub = pd.read_csv('../sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# merge data (ignore transactions for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "\n",
    "train['dataT'] = 'train'\n",
    "test['dataT'] = 'test'\n",
    "train['date'] = pd.to_datetime(train['date'])\n",
    "test['date'] = pd.to_datetime(test['date'])\n",
    "data = pd.concat([train, test])\n",
    "data['date'] = pd.to_datetime(data['date'])\n",
    "\n",
    "data0 = pd.merge(data, stores, on=['store_nbr'], how='outer')\n",
    "print(data.shape, data0.shape)\n",
    "\n",
    "oil['date'] = pd.to_datetime(oil['date'])\n",
    "oil.set_index('date',inplace=True)\n",
    "oil_resampled = oil.resample('1D').asfreq()\n",
    "print(oil_resampled.isna().sum())\n",
    "oil_resampled.interpolate(inplace=True,limit_direction='both')\n",
    "oil_resampled.reset_index(inplace=True)\n",
    "print(oil_resampled.isna().sum())\n",
    "\n",
    "data0['date'] = pd.to_datetime(data0['date'])\n",
    "data1 = pd.merge(data0, oil_resampled, on=['date'], how='left')\n",
    "print(data1.shape, data0.shape)\n",
    "\n",
    "holidays['date'] = pd.to_datetime(holidays['date'])\n",
    "cityHolidays = holidays.loc[holidays.locale =='Local']#.locale_name.value_counts()\n",
    "cityHolidays.drop('locale', axis = 1, inplace=True)\n",
    "cityHolidays['description'] = 'Fundacion'\n",
    "cityHolidays.rename(columns={'locale_name':'city','type':'holidayType'}, inplace=True)\n",
    "cityHolidays.drop(264, axis = 0, inplace=True) # we have some duplicates\n",
    "data2 = pd.merge(data1, cityHolidays, on=['date','city'], how='left')\n",
    "#train2.dropna(inplace=True)\n",
    "print(data1.shape, data2.shape)\n",
    "\n",
    "regionalHolidays = holidays.loc[holidays.locale =='Regional']#.locale_name.value_counts()\n",
    "regionalHolidays.drop('locale', axis = 1, inplace=True)\n",
    "regionalHolidays['description'] = 'Provincializacion'\n",
    "regionalHolidays.rename(columns={'locale_name':'state'}, inplace=True)\n",
    "\n",
    "data3 = pd.merge(data2, regionalHolidays, on=['date','state'], how='left', suffixes=('','_reg'))\n",
    "print(data3.shape, data2.shape)\n",
    "\n",
    "nationalHolidays = holidays.loc[holidays.locale =='National']#.locale_name.value_counts()\n",
    "nationalHolidays.drop(['locale','locale_name'], axis = 1, inplace=True)\n",
    "nationalHolidays.description.unique()\n",
    "groups = ['Navidad', 'Mundial de futbol Brasil','Terremoto Manabi','dia del ano','Puente Dia de Difuntos','Grito de Independencia','Independencia de Guayaquil','Dia de la Madre','Batalla de Pichincha']\n",
    "for group in groups:\n",
    "    mask = nationalHolidays['description'].str.contains(group)\n",
    "    nationalHolidays.loc[mask, 'description'] = group\n",
    "nationalHolidays = nationalHolidays.drop_duplicates(subset=['date'], keep='first')\n",
    "\n",
    "data4 = pd.merge(data3, nationalHolidays, on=['date'], how='left', suffixes=('','_nat'))\n",
    "print(data3.shape, data4.shape)\n",
    "\n",
    "\n",
    "data5 = data4.copy()\n",
    "data5['holidayType'] = data5['holidayType'].combine_first(data5['type_reg'])\n",
    "data5['holidayType'] = data5['holidayType'].combine_first(data5['type_nat'])\n",
    "\n",
    "data5['description'] = data5['description'].combine_first(data5['description_reg'])\n",
    "data5['description'] = data5['description'].combine_first(data5['description_nat'])\n",
    "\n",
    "data5['transferred'] = data5['transferred'].combine_first(data5['transferred_reg'])\n",
    "data5['transferred'] = data5['transferred'].combine_first(data5['transferred_nat'])\n",
    "\n",
    "data5 = data5.drop(columns=['type_reg','type_nat','description_reg','description_nat','transferred_reg','transferred_nat'])\n",
    "\n",
    "print(data4.shape, data5.shape)\n",
    "\n",
    "data6 = data5.copy()\n",
    "propDicts = {}\n",
    "for f in ['family','city','state','type','holidayType','description','transferred']:\n",
    "    unique = data6[f].unique()\n",
    "    category_dict = {category: index for index, category in enumerate(unique)}\n",
    "    data6[f] = data6[f].map(category_dict)\n",
    "    propDicts[f] = category_dict\n",
    "\n",
    "flippedPropDicts = {}\n",
    "for key,value in propDicts.items():\n",
    "    flippedPropDicts[key] = {value: key for key, value in propDicts[key].items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data, data0, data1, data2, data3, data4, data5, transactions, train, oil, test, sampleSub, holidays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that the city holidays are the same in the holiday and store df\n",
    "uniqueLocalsHolidays = holidays.loc[holidays.locale =='Local'].locale_name.unique()\n",
    "uniqueLocalCities = stores.city.unique()\n",
    "\n",
    "intersection = set(uniqueLocalsHolidays).intersection(set(uniqueLocalCities))\n",
    "not_intersection_list1 = set(uniqueLocalsHolidays).difference(intersection)\n",
    "not_intersection_list2 = set(uniqueLocalCities).difference(intersection)\n",
    "\n",
    "print(intersection)\n",
    "print(not_intersection_list1)\n",
    "print(not_intersection_list2)\n",
    "#result: we have a couple cities without holidays, but that is fine, the rest is the same "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check, somewhat ok\n",
    "rows = train4.shape[0]\n",
    "holTypes = rows - train4.holidayType.isna().sum()\n",
    "holTypes1 = rows - train4.type_reg.isna().sum()\n",
    "holTypes2 = rows - train4.type_nat.isna().sum()\n",
    "sumTypes = holTypes + holTypes1 + holTypes2\n",
    "print(sumTypes, rows - train5.holidayType.isna().sum())\n",
    "\n",
    "holTypes = rows - train4.description.isna().sum()\n",
    "holTypes1 = rows - train4.description_reg.isna().sum()\n",
    "holTypes2 = rows - train4.description_nat.isna().sum()\n",
    "sumTypes = holTypes + holTypes1 + holTypes2\n",
    "print(sumTypes, rows - train5.description.isna().sum())\n",
    "\n",
    "holTypes = rows - train4.transferred.isna().sum()\n",
    "holTypes1 = rows - train4.transferred_reg.isna().sum()\n",
    "holTypes2 = rows - train4.transferred_nat.isna().sum()\n",
    "sumTypes = holTypes + holTypes1 + holTypes2\n",
    "print(sumTypes, rows - train5.transferred.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check how to make data staionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testPair = train.loc[(train.store_nbr == 6) & (train.family == 'BEAUTY')]\n",
    "testPair.set_index('date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testPair.sales.diff(14).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# peak at 14 -> predict 2 week difference\n",
    "# ACF: MA (moving average) part = how many last errors we include, e.g. error at t-1, t-2,.. -> 3-4 last erors\n",
    "# PACF: AR (autoregressive) part = how many lags we include                                  -> 3 lags\n",
    "tsplot(testPair.sales.diff(14).dropna(),lags = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stationarity(testPair.sales.diff(14).dropna())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testPair['target'] = testPair.sales.diff(14)\n",
    "testPair['shiftedSales14'] = testPair.sales.shift(14)\n",
    "\n",
    "n_lags = 3\n",
    "for n in range(n_lags):\n",
    "    l = n+1\n",
    "    testPair['target_lag'+str(l)] = testPair['target'].shift(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testPair['date'] = pd.to_datetime(testPair['date'])\n",
    "mask = testPair.date < pd.to_datetime(\"2017-01-1\")\n",
    "y_train = testPair['sales'][mask]\n",
    "y_test = testPair['sales'][~mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predicting stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arima\n",
    "\n",
    "~ 0.7 rmsle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_log_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p, d, q = 3,0,3\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Initialize an empty list to store ARIMA models\n",
    "arima_models = []\n",
    "time_index = testPair.index\n",
    "for train_index, test_index in tscv.split(testPair):\n",
    "    train_data = testPair.iloc[train_index]['target']\n",
    "    test_data = testPair.iloc[test_index]['target']\n",
    "    salesTrain = testPair.iloc[train_index]['sales']\n",
    "    shiftedSalesTest = testPair.iloc[test_index]['shiftedSales14']\n",
    "\n",
    "    # Fit ARIMA model\n",
    "    model = ARIMA(train_data, order=(p, d, q))\n",
    "    model_fit = model.fit()\n",
    "\n",
    "    # Store the trained model\n",
    "    arima_models.append(model_fit)\n",
    "\n",
    "    forecast_steps = len(test_data)\n",
    "    forecast = np.clip(model_fit.forecast(steps=forecast_steps), 0, 1e19)\n",
    "\n",
    "    gtSales = test_data+ shiftedSalesTest\n",
    "    predictedSales = forecast.values+ shiftedSalesTest\n",
    "\n",
    "    # Calculate RMSLE\n",
    "    rmsle = np.sqrt(mean_squared_log_error(test_data+ shiftedSalesTest, forecast.values+ shiftedSalesTest))\n",
    "    print(rmsle)\n",
    "\n",
    "    # Plot actual vs. predicted values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(time_index[train_index], salesTrain, label='Train')\n",
    "    plt.plot(time_index[test_index], predictedSales, label='Predicted')\n",
    "    plt.plot(time_index[test_index], gtSales, label='actual test')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Value')\n",
    "    plt.title(f'ARIMA Forecast (RMSLE: {rmsle:.4f})')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "~0.6-0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense,Input,concatenate\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testPair.columns\n",
    "featuresTrain = [\n",
    "    #'sales', \n",
    "    'onpromotion', \n",
    "    #'target',\n",
    "       'target_lag1', 'target_lag2', 'target_lag3']#, 'shiftedSales14']\n",
    "allF = featuresTrain + ['shiftedSales14','target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# window for lstm approach\n",
    "look_back = 100\n",
    "sequences = []\n",
    "labels = []\n",
    "offset = []\n",
    "for i in range(testPair.shape[0]-look_back):\n",
    "    window = testPair.iloc[i : i + look_back][['target','onpromotion']]\n",
    "    label = testPair.iloc[i + look_back]['target']#['target']  # Next data point as label\n",
    "    off = testPair.iloc[i + look_back][['shiftedSales14','sales']]\n",
    "    sequences.append(window)\n",
    "    labels.append(label)\n",
    "    offset.append(off)\n",
    "sequences, labels, offsets = np.array(sequences), np.array(labels), np.array(offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 5\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "n_features = sequences.shape[2]\n",
    "\n",
    "testPair.dropna(inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "for train_index, test_index in tscv.split(sequences):\n",
    "    X_train, X_test = sequences[train_index], sequences[test_index]\n",
    "    y_train, y_test = labels[train_index], labels[test_index]\n",
    "\n",
    "    # Create an LSTM model\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(look_back,n_features)))\n",
    "    model.add(LSTM(64, activation='relu', return_sequences=True))\n",
    "    model.add(LSTM(units=32, return_sequences=False))\n",
    "    model.add(Dense(1))  # Single output for univariate forecasting\n",
    "\n",
    "    # Compile and train the model\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    model.fit(X_train, y_train, epochs=50, batch_size=32)\n",
    "\n",
    "    # Evaluate on the test set\n",
    "    forecast = model.predict(X_test)\n",
    "    print(f'RMSLE for fold: {rmsle:.4f}')\n",
    "\n",
    "    gtSales = offsets[test_index,1]\n",
    "    predictedSales = np.clip(np.reshape(forecast, (forecast.shape[0])) +offsets[test_index,0], 0,1e19)\n",
    "\n",
    "    # Calculate RMSLE\n",
    "    rmsle = np.sqrt(mean_squared_log_error(gtSales, predictedSales))\n",
    "    print(rmsle)\n",
    "\n",
    "    # Plot actual vs. predicted values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(time_index[train_index], offsets[train_index,1], label='Train')\n",
    "    plt.plot(time_index[test_index], predictedSales, label='Predicted')\n",
    "    plt.plot(time_index[test_index], gtSales, label='actual test')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Value')\n",
    "    plt.title(f'ARIMA Forecast (RMSLE: {rmsle:.4f})')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM with direct values (no diff)\n",
    "- use loss directly to train with\n",
    "- sometimes just predicts 0\n",
    "\n",
    "~0.2-0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# window for lstm approach\n",
    "look_back = 100\n",
    "sequences = []\n",
    "labels = []\n",
    "for i in range(testPair.shape[0]-look_back):\n",
    "    window = testPair.iloc[i : i + look_back][['sales','onpromotion']]\n",
    "    label = testPair.iloc[i + look_back]['sales']#['target']  # Next data point as label\n",
    "    sequences.append(window)\n",
    "    labels.append(label)\n",
    "    offset.append(off)\n",
    "sequences, labels = np.array(sequences), np.array(labels)\n",
    "\n",
    "n_splits = 5\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "n_features = sequences.shape[2]\n",
    "\n",
    "testPair.dropna(inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "for train_index, test_index in tscv.split(sequences):\n",
    "    X_train, X_test = sequences[train_index], sequences[test_index]\n",
    "    y_train, y_test = labels[train_index], labels[test_index]\n",
    "\n",
    "    # Create an LSTM model\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(look_back,n_features)))\n",
    "    model.add(LSTM(64, activation='relu', return_sequences=False))\n",
    "    #model.add(LSTM(units=32, return_sequences=False))\n",
    "    model.add(Dense(1, activation='relu'))  # Single output for univariate forecasting\n",
    "\n",
    "    # Compile and train the model\n",
    "    model.compile(optimizer='adam', loss=tf.keras.losses.MSLE, metrics=['mse'])\n",
    "    model.fit(X_train, y_train, epochs=50, batch_size=320,validation_data=(X_test, y_test))\n",
    "\n",
    "    # Evaluate on the test set\n",
    "    forecast = model.predict(X_test)\n",
    "    print(f'RMSLE for fold: {rmsle:.4f}')\n",
    "\n",
    "    # Plot actual vs. predicted values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(time_index[train_index], labels[train_index], label='Train')\n",
    "    plt.plot(time_index[test_index], forecast, label='Predicted')\n",
    "    plt.plot(time_index[test_index], labels[test_index], label='actual test')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Value')\n",
    "    plt.title(f'ARIMA Forecast (RMSLE: {rmsle:.4f})')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict next 15 values (direct)\n",
    "~0.5 / 0.6 (val)\n",
    "\n",
    "- using MSE results in bad predictions!! use MSLE\n",
    "\n",
    "- use as second input the train values of the 16 timestamps!! like on promotion,.. right now we don't have that info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore the test data\n",
    "train = data6.loc[data6.dataT == 'test']\n",
    "testPair = train.loc[(train.store_nbr == 25) & (train.family == propDicts['family']['BEAUTY'])]\n",
    "print(flippedPropDicts['city'][8],flippedPropDicts['state'][7], flippedPropDicts['holidayType'][1],flippedPropDicts['description'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data6.loc[data6.dataT == 'train']\n",
    "testPair = train.loc[(train.store_nbr == 6) & (train.family == propDicts['family']['BEAUTY'])]\n",
    "testPair.set_index('date', inplace=True)\n",
    "time_index = testPair.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.set_random_seed(42)  # sets seeds for base-python, numpy and tf\n",
    "\n",
    "# window for lstm approach\n",
    "look_back = 100\n",
    "n_predictedValues = 16 # I need to predict 16 values\n",
    "\n",
    "sequences = []\n",
    "labels = []\n",
    "zScoreScaling = False\n",
    "minMaxScaling = False\n",
    "for i in range(testPair.shape[0]-look_back-n_predictedValues):\n",
    "    if zScoreScaling:\n",
    "            mean = testPair.sales.mean()\n",
    "            stdDev = testPair.sales.std()\n",
    "            testPair.sales = (testPair.sales - mean) / stdDev\n",
    "    elif minMaxScaling:\n",
    "            max0 = max(testPair.sales)\n",
    "            min0 = min(testPair.sales)\n",
    "            testPair.sales = (testPair.sales - min0) / (max0 - min0) * 100\n",
    "    window = testPair.iloc[i : i + look_back][['sales','onpromotion','holidayType']] #,'dcoilwtico',  'description','transferred']]\n",
    "    label = testPair.iloc[(i + look_back) : (i + look_back + n_predictedValues)]['sales'].values#['target']  # Next data point as label\n",
    "    sequences.append(window)\n",
    "    labels.append(label)\n",
    "sequences, labels = np.array(sequences), np.array(labels)\n",
    "\n",
    "n_splits = 10\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "n_features = sequences.shape[2]\n",
    "\n",
    "testPair.dropna(inplace=True)\n",
    "\n",
    "# Create an LSTM model\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(look_back,n_features)))\n",
    "model.add(LSTM(64, activation='relu', return_sequences=False,\n",
    "               kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros'))\n",
    "#model.add(LSTM(units=32, return_sequences=False))\n",
    "model.add(Dense(n_predictedValues, activation='relu'))  # Single output for univariate forecasting\n",
    "# Compile and train the model\n",
    "model.compile(optimizer='adam', loss=tf.keras.losses.MSLE, metrics=['mse'])\n",
    "\n",
    "for train_index, test_index in tscv.split(sequences):\n",
    "    X_train, X_test = sequences[train_index], sequences[test_index]\n",
    "    y_train, y_test = labels[train_index], labels[test_index]\n",
    "\n",
    "    model.fit(X_train, y_train, epochs=5, batch_size=32,validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate on the test set\n",
    "forecast = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# de-scaling functionality\n",
    "evalTrain = True\n",
    "X0 = X_test\n",
    "yE = y_test\n",
    "if evalTrain:\n",
    "    X0 = X_train\n",
    "    yE = y_train\n",
    "predZ = model.predict(X0)\n",
    "\n",
    "if zScoreScaling:\n",
    "    print('zScoreScaling')\n",
    "    pred = np.clip((predZ * stdDev) + mean, 0,1e60)\n",
    "    yT = np.clip((yE * stdDev) + mean, 0, 1e60)\n",
    "elif minMaxScaling:\n",
    "    print('minMaxScaling')\n",
    "    pred =  predZ / 100 * (max0 - min0) + min0\n",
    "    yT   =  yE/ 100 * (max0 - min0) + min0\n",
    "else:\n",
    "    pred = predZ\n",
    "    yT = yE\n",
    "\n",
    "rmsle = np.sqrt(mean_squared_log_error(pred, yT))\n",
    "print(rmsle)\n",
    "#pred, yT, predZ, yE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results - only sales & on promotion\n",
    "# 0.30/0.28 - 10 folds, 5 epochs, 32bs \n",
    "# 1.21/1.02 - 10 folds, 5 epochs, 320 bs\n",
    "# 0.45/0.50 - 10 folds, 10 epochs, 32bs\n",
    "# 3.37/4.03 - 50 folds, 5 epochs, 32 bs\n",
    "# 0.31/0.34 - 5 folds, 5 epochs, 32 bs\n",
    "\n",
    "# 10 folds, 5 epochs, 32bs \n",
    "# 0.30/0.28 - 'sales','onpromotion'\n",
    "# 0.30/0.31 - 'sales',\n",
    "# 3.30/4.34 - 'sales','onpromotion','dcoilwtico', 'holidayType', 'description','transferred'\n",
    "# 2.03/4.04 - 'sales','onpromotion', 'holidayType', 'description','transferred' -> oil seems to help for overfitting :o\n",
    "# 0.54/0.61 - 'sales','onpromotion', 'holidayType','transferred'                -> holiday description helps to overfit\n",
    "# 1.50/1.65 - 'sales','onpromotion','holidayType','transferred','dcoilwtico'\n",
    "# 0.24/0.25 - 'sales','onpromotion','holidayType'\n",
    "# 0.48/0.51 - 'sales','onpromotion','transferred'\n",
    "# -> 'sales','onpromotion','holidayType' seem to be best features\n",
    "\n",
    "# investigate zScore & min/max normalization\n",
    "# 10 folds, 5 epochs, 32bs & 'sales','onpromotion','holidayType'\n",
    "# 0.33/0.46 z-score \n",
    "# --/1.45   min-max scaling\n",
    "# -> worse\n",
    "\n",
    "#0.3080/0.2832 -> reproduceable \n",
    "# BITEXACTNESS - need to include set_seed in every function call, otherwise it doesn't work!!! in jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotidx(ind):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    mintimeIdx = test_index[ind] - look_back\n",
    "    plt.plot(time_index[mintimeIdx:test_index[ind]], X_test[ind,:,0], label='Train')\n",
    "    plt.plot(time_index[test_index][ind:ind+n_predictedValues], forecast[ind,:], label='Predicted')\n",
    "    plt.plot(time_index[test_index][ind:ind+n_predictedValues], labels[test_index][ind,:], label='actual test')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Value')\n",
    "    #plt.title(f'ARIMA Forecast (RMSLE: {rmsle:.4f})')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,2):\n",
    "    plotidx(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict next 16 values with last 100 + current 16 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data6.loc[data6.dataT == 'train']\n",
    "testPair = train.loc[(train.store_nbr == 6) & (train.family == propDicts['family']['BEAUTY'])]\n",
    "testPair.set_index('date', inplace=True)\n",
    "time_index = testPair.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.set_random_seed(42)  # sets seeds for base-python, numpy and tf\n",
    "\n",
    "# window for lstm approach\n",
    "look_back = 100\n",
    "n_predictedValues = 16 # I need to predict 16 values\n",
    "\n",
    "sequences = []\n",
    "labels = []\n",
    "futureFeatures = []\n",
    "zScoreScaling = False\n",
    "minMaxScaling = False\n",
    "f1 = [        'onpromotion','holidayType','transferred','description']\n",
    "f0 = ['sales','onpromotion','holidayType','transferred','description']\n",
    "\n",
    "mean = testPair.dcoilwtico.mean()\n",
    "stdDev = testPair.dcoilwtico.std()\n",
    "testPair.dcoilwtico = (testPair.dcoilwtico - mean) / stdDev\n",
    "\n",
    "for i in range(testPair.shape[0]-look_back-n_predictedValues):\n",
    "    if zScoreScaling:\n",
    "            mean = testPair.sales.mean()\n",
    "            stdDev = testPair.sales.std()\n",
    "            testPair.sales = (testPair.sales - mean) / stdDev\n",
    "    elif minMaxScaling:\n",
    "            max0 = max(testPair.sales)\n",
    "            min0 = min(testPair.sales)\n",
    "            testPair.sales = (testPair.sales - min0) / (max0 - min0) * 100\n",
    "    window = testPair.iloc[i : i + look_back][f0] #,'dcoilwtico',  'description','transferred']]\n",
    "    label = testPair.iloc[(i + look_back) : (i + look_back + n_predictedValues)]['sales'].values#['target']  # Next data point as label\n",
    "    fF = testPair.iloc[(i + look_back) : (i + look_back + n_predictedValues)][f1]\n",
    "    sequences.append(window)\n",
    "    labels.append(label)\n",
    "    futureFeatures.append(fF)\n",
    "sequences, labels, futureFeatures = np.array(sequences), np.array(labels), np.array(futureFeatures)\n",
    "\n",
    "n_splits = 10\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "n_features = sequences.shape[2]\n",
    "\n",
    "testPair.dropna(inplace=True)\n",
    "\n",
    "\n",
    "input1 = Input(shape=(look_back, n_features))\n",
    "input2 = Input(shape=(n_predictedValues, n_features-1))\n",
    "\n",
    "\n",
    "lstm1 = LSTM(64, activation='relu', return_sequences=False)(input1)\n",
    "lstm2 = LSTM(64, activation='relu', return_sequences=False)(input2)\n",
    "x = tf.keras.layers.concatenate([lstm1, lstm2])\n",
    "output = Dense(n_predictedValues, activation='relu')(x)\n",
    "\n",
    "# Define the model\n",
    "model = Model(inputs=[input1, input2], outputs=output)\n",
    "\n",
    "# Compile and train the model\n",
    "model.compile(optimizer='adam', loss=tf.keras.losses.MSLE, metrics=['mse'])\n",
    "\n",
    "for train_index, test_index in tscv.split(sequences):\n",
    "    X_train = [sequences[train_index],futureFeatures[train_index]]\n",
    "    X_test  = [sequences[test_index], futureFeatures[test_index]]\n",
    "    y_train, y_test = labels[train_index], labels[test_index]\n",
    "\n",
    "    model.fit(X_train, y_train, epochs=5, batch_size=32,validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate on the test set\n",
    "forecast = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 folds, 5 epochs, 32bs & \n",
    "# 0.53/0.55   -'sales','onpromotion','holidayType'\n",
    "# 0.49/0.55  -'sales','onpromotion','holidayType','dcoilwtico',  'description','transferred' - z scaled oil\n",
    "# 3.30/4.34 - 'sales','onpromotion','holidayType','transferred'\n",
    "# 0.31/0.30 - 'sales','onpromotion','holidayType','transferred','description'\n",
    "\n",
    "#store id 6, AUTOMOTIVE -- # use mean scaled oil feature\n",
    "#   -'sales','onpromotion','holidayType','dcoilwtico','description','transferred'\n",
    "#   -'sales','onpromotion','holidayType','description','transferred'\n",
    "#-> without oil keeps being better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotidx(ind):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    mintimeIdx = test_index[ind] - look_back\n",
    "    plt.plot(time_index[mintimeIdx:test_index[ind]], X_test[0][ind,:,0], label='Train')\n",
    "    plt.plot(time_index[test_index][ind:ind+n_predictedValues], forecast[ind,:], label='Predicted')\n",
    "    plt.plot(time_index[test_index][ind:ind+n_predictedValues], labels[test_index][ind,:], label='actual test')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Value')\n",
    "    #plt.title(f'ARIMA Forecast (RMSLE: {rmsle:.4f})')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "plotidx(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict next 15 values, single model for every store/product pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #def trainModelAndPredict(storeId, familyId, data6):\n",
    "    familyId = propDicts['family']['AUTOMOTIVE']\n",
    "    storeId = 6\n",
    "    pair = data6.loc[(data6.store_nbr == storeId) & (data6.family == familyId)]\n",
    "\n",
    "    #do any transformation\n",
    "    mean = pair.dcoilwtico.mean()\n",
    "    stdDev = pair.dcoilwtico.std()\n",
    "    pair.dcoilwtico = (pair.dcoilwtico - mean) / stdDev\n",
    "\n",
    "    trainPair = pair.loc[pair.dataT == 'train']\n",
    "    trainPair.set_index('date', inplace=True)\n",
    "    trainPair.dropna(inplace=True)\n",
    "    time_index = trainPair.index\n",
    "\n",
    "    tf.keras.utils.set_random_seed(42)  # sets seeds for base-python, numpy and tf\n",
    "\n",
    "    # window for lstm approach\n",
    "    look_back = 100\n",
    "    n_predictedValues = 16 # I need to predict 16 values    \n",
    "\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    futureFeatures = []\n",
    "    f1 = ['onpromotion','holidayType','transferred','description']\n",
    "    f0 = ['sales','onpromotion','holidayType','transferred','description']\n",
    "\n",
    "    for i in range(trainPair.shape[0]-look_back-n_predictedValues):\n",
    "        window = trainPair.iloc[i : i + look_back][f0]\n",
    "        label = trainPair.iloc[(i + look_back) : (i + look_back + n_predictedValues)]['sales'].values\n",
    "        fF = trainPair.iloc[(i + look_back) : (i + look_back + n_predictedValues)][f1]\n",
    "        sequences.append(window)\n",
    "        labels.append(label)\n",
    "        futureFeatures.append(fF)\n",
    "    sequences, labels, futureFeatures = np.array(sequences), np.array(labels), np.array(futureFeatures) \n",
    "\n",
    "    n_splits = 10\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)   \n",
    "\n",
    "    n_features = sequences.shape[2] \n",
    "\n",
    "    input1 = Input(shape=(look_back, n_features))\n",
    "    input2 = Input(shape=(n_predictedValues, n_features))   \n",
    "\n",
    "\n",
    "    lstm1 = LSTM(64, activation='relu', return_sequences=False)(input1)\n",
    "    lstm2 = LSTM(64, activation='relu', return_sequences=False)(input2)\n",
    "    x = tf.keras.layers.concatenate([lstm1, lstm2])\n",
    "    output = Dense(n_predictedValues, activation='relu')(x) \n",
    "\n",
    "    # Define the model\n",
    "    model = Model(inputs=[input1, input2], outputs=output)  \n",
    "\n",
    "    # Compile and train the model\n",
    "    model.compile(optimizer='adam', loss=tf.keras.losses.MSLE, metrics=['mse']) \n",
    "\n",
    "    for train_index, test_index in tscv.split(sequences):\n",
    "        X_train = [sequences[train_index],futureFeatures[train_index]]\n",
    "        X_test  = [sequences[test_index], futureFeatures[test_index]]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]   \n",
    "\n",
    "        model.fit(X_train, y_train, epochs=5, batch_size=32,validation_data=(X_test, y_test))   \n",
    "\n",
    "    # Evaluate on the test set\n",
    "    forecast = model.predict(X_test)\n",
    "    rmsle = np.sqrt(mean_squared_log_error(forecast, y_test))\n",
    "    print(rmsle)\n",
    "\n",
    "    testPair = pair.loc[pair.dataT == 'test']\n",
    "\n",
    "    sequence = trainPair.tail(look_back)[f0]\n",
    "    fF = testPair[f1]\n",
    "\n",
    "    X = [sequence, fF]\n",
    "    pred = model.predict(X)\n",
    "\n",
    "    # predict test data\n",
    "    #return model, rmsle,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b = np.reshape(np.array(sequence), (1,look_back, n_features)), np.reshape(np.array(fF), (1,n_predictedValues,n_features))\n",
    "X = [a,b]\n",
    "pred = model.predict(X)\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build dataset for lstm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "look_back = 100\n",
    "n_predictedValues = 15\n",
    "train = data6.loc[data6.dataT == 'train']\n",
    "train[['holidayType','description','transferred']] = train[['holidayType','description','transferred']].astype(int)\n",
    "train['lag14'] = train.sales.shift(14)\n",
    "train['target'] = train.sales - train.lag14\n",
    "minMaxScaling = False\n",
    "zScoreScaling = False\n",
    "minMaxScalingOnSequence = False\n",
    "zScoreScalingOnSequence = False\n",
    "\n",
    "train.dropna(inplace=True)\n",
    "#del data6\n",
    "sequences = []\n",
    "labels = []\n",
    "categories = []\n",
    "zTransMean = []\n",
    "zTransStd = []\n",
    "\n",
    "dict_zNorm = {}\n",
    "\n",
    "for storeId in train.store_nbr.unique():\n",
    "\n",
    "    if storeId > 1:\n",
    "        break\n",
    "\n",
    "    storeDf = train.loc[train.store_nbr == storeId]\n",
    "    print('store id', storeId)\n",
    "    for famIdx, family in enumerate(storeDf.family.unique()):\n",
    "        print('family', family)\n",
    "        if famIdx > 1:\n",
    "            break\n",
    "        \n",
    "        familyDf = storeDf.loc[storeDf.family == family]\n",
    "\n",
    "        if zScoreScaling:\n",
    "            mean = familyDf.sales.mean()\n",
    "            stdDev = familyDf.sales.std()\n",
    "            familyDf.sales = (familyDf.sales - mean) / stdDev\n",
    "            a = {}\n",
    "            a['mean'] = mean\n",
    "            a['std'] = stdDev\n",
    "            dict_zNorm[str(storeId) + '_'+str(family)] = a\n",
    "        elif minMaxScaling:\n",
    "            max0 = max(familyDf.sales)\n",
    "            min0 = min(familyDf.sales)\n",
    "            familyDf.sales = (familyDf.sales - min0) / (max0 - min0) * 100\n",
    "            a = {}\n",
    "            a['min'] = min0\n",
    "            a['max'] = max0\n",
    "            dict_zNorm[str(storeId) + '_'+str(family)] = a\n",
    "\n",
    "        c = familyDf.city.unique()\n",
    "        if len(c) > 1:\n",
    "            print('somehow wrong!!')\n",
    "        city = c[0]\n",
    "        state = familyDf.state.iloc[0]\n",
    "        type = familyDf.type.iloc[0]\n",
    "        cluster = familyDf.cluster.iloc[0]\n",
    "\n",
    "        # window for lstm approach\n",
    "        for i in range(familyDf.shape[0]-look_back-n_predictedValues):\n",
    "            startS = i\n",
    "            endS = startS+look_back\n",
    "            startP = endS\n",
    "            endP = startP + n_predictedValues\n",
    "            subblock0 = familyDf.iloc[startS:endS] #do normalization based on past values, not future values\n",
    "            subblock = familyDf.iloc[startS:endP]\n",
    "            if not minMaxScaling and not zScoreScaling:\n",
    "                if zScoreScalingOnSequence:\n",
    "                    mean = subblock0.sales.mean()\n",
    "                    stdDev = subblock0.sales.std()\n",
    "                    subblock.sales = (subblock.sales - mean) / stdDev\n",
    "                    a = {}\n",
    "                    a['mean'] = mean\n",
    "                    a['std'] = stdDev\n",
    "                    dict_zNorm[str(storeId) + '_'+str(family)] = a\n",
    "                elif minMaxScalingOnSequence:\n",
    "                    max0 = max(subblock0.sales)\n",
    "                    min0 = min(subblock0.sales)\n",
    "                    subblock.sales = (subblock.sales - min0) / (max0 - min0) * 100\n",
    "                    a = {}\n",
    "                    a['min'] = min0\n",
    "                    a['max'] = max0\n",
    "                    dict_zNorm[str(storeId) + '_'+str(family)] = a\n",
    "\n",
    "\n",
    "            window = subblock.iloc[0:look_back][['sales','onpromotion','dcoilwtico','holidayType','description','transferred']].to_numpy()\n",
    "            label  = subblock.iloc[look_back : (look_back + n_predictedValues)]['sales'].values#['target']  # Next data point as label\n",
    "            if (not np.isnan(label).any()) and (not np.isnan(window).any()):\n",
    "                sequences.append(window)\n",
    "                categories.append([int(storeId), int(family), int(city), int(state), int(type), int(cluster)])\n",
    "                labels.append(label)\n",
    "                if zScoreScaling or zScoreScalingOnSequence:\n",
    "                    zTransMean.append(mean)\n",
    "                    zTransStd.append(stdDev)\n",
    "                elif minMaxScaling or minMaxScalingOnSequence:\n",
    "                    zTransMean.append(min0)\n",
    "                    zTransStd.append(max0)\n",
    "\n",
    "sequences, labels, categories, zTransMean, zTransStd = np.array(sequences), np.array(labels), np.array(categories), np.array(zTransMean),np.array(zTransStd)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences.shape, labels.shape, categories.shape\n",
    "np.savez('windowed_sequences_100Lookback_15predict.npz', arr1=sequences, arr2=labels, arr3=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(sequences).any(),np.isnan(labels).any(),np.isnan(categories).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catNum = categories.shape[1]\n",
    "n_features = sequences.shape[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train 1 lstm for all store/product pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense,Input,concatenate\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leng = sequences.shape[0]\n",
    "div = int(sequences.shape[0]*0.9)\n",
    "X_train = [sequences[0:div,:,:], categories[0:div,:]]\n",
    "X_test =  [sequences[div:leng,:,:], categories[div:leng,:]]\n",
    "\n",
    "y_train = labels[0:div,:]\n",
    "y_test  = labels[div:leng,:]\n",
    "\n",
    "zMean_train = zTransMean[0:div]\n",
    "zStd_train = zTransStd[0:div]\n",
    "zMean_test = zTransMean[div:leng]\n",
    "zStd_test = zTransStd[div:leng]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_input = Input(shape=(look_back,n_features), name='seq_input')\n",
    "feat_input = Input(shape=(catNum,), name='feat_input')\n",
    "\n",
    "lstm_out = LSTM(units=64, activation='relu', return_sequences=False)(seq_input)\n",
    "#lstm_out = LSTM(units=64, activation='relu', return_sequences=False)(lstm_out)\n",
    "#print(lstm_out.shape)\n",
    "#feat_input1 = Dense(catNum, activation='relu')(feat_input)\n",
    "combined_input = concatenate([lstm_out, feat_input])\n",
    "#combined_input = Dense(64, activation='relu')(lstm_out)\n",
    "output = Dense(n_predictedValues, activation='relu')(lstm_out)\n",
    "model = Model(inputs=[seq_input, feat_input], outputs=output)\n",
    "model.compile(optimizer='adam', loss=tf.keras.losses.MSLE, metrics=['mse'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32,validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalTrain = True\n",
    "X0 = X_test\n",
    "std = zStd_test\n",
    "mean = zMean_test\n",
    "yE = y_test\n",
    "if evalTrain:\n",
    "    X0 = X_train\n",
    "    std = zStd_train\n",
    "    mean = zMean_train\n",
    "    yE = y_train\n",
    "predZ = model.predict(X0)\n",
    "\n",
    "if zScoreScaling or zScoreScalingOnSequence:\n",
    "    print('zScoreScaling')\n",
    "    pred = np.clip((predZ * np.reshape(std, (std.shape[0],1))) + np.reshape(mean, (mean.shape[0],1)), 0,1e60)\n",
    "    yT = np.clip((yE * np.reshape(std, (std.shape[0],1))) + np.reshape(mean, (mean.shape[0],1)), 0, 1e60)\n",
    "elif minMaxScaling or minMaxScalingOnSequence:\n",
    "    print('minMaxScaling')\n",
    "    min0 =  np.reshape(mean, (mean.shape[0],1))\n",
    "    max0 =  np.reshape(std, (std.shape[0],1))\n",
    "    pred =  predZ / 100 * (max0 - min0) + min0\n",
    "    yT   =  yE/ 100 * (max0 - min0) + min0\n",
    "else\n",
    "    pred = predZ\n",
    "    yT = yE\n",
    "\n",
    "rmsle = np.sqrt(mean_squared_log_error(pred, yT))\n",
    "print(rmsle)\n",
    "pred, yT, predZ, yE\n",
    "\n",
    "# zScore norm, msle loss\n",
    "# ~0.73 with basic network, ~30 epochs, batch size 32\n",
    "# ~0.94 with more lstm layers 10 epochs, bs 32\n",
    "\n",
    "# min max scaling\n",
    "# ~2.95 with basic network, ~10 epochs, bz 32\n",
    "\n",
    "# zScore scaling\n",
    "# 0.40 with basic network, 10 epochs, bz 32 -> predicts one value per 15 future values, some bug in re-transforming values\n",
    "\n",
    "# min max scaling based on lookback\n",
    "# ~1.64 basic network, 10 epochs, bz 32   // with min/max based on future values as well...\n",
    "# 3.00  basic network, 10 epochs, bz 32   // with min/max based on only past 100 values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X_test)\n",
    "rmsle = np.sqrt(mean_squared_log_error(pred, y_test))\n",
    "print(rmsle)\n",
    "pred, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "60f2b4b2b39245c89a47c8dbe671288aea181e96fbe781c7f5f13eb9eb69cf46"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
