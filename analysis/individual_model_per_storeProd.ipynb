{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "#probably not needed\n",
    "tf.keras.utils.set_random_seed(42)  # sets seeds for base-python, numpy and tf\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "import random\n",
    "random.seed(42)\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import plotly.offline as pyo\n",
    "from plotly import subplots\n",
    "import plotly.graph_objects as go\n",
    "from baseFunctions import *\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.tsa.api as smt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_log_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense,Input,concatenate\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../train.csv')\n",
    "oil = pd.read_csv('../oil.csv')\n",
    "stores = pd.read_csv('../stores.csv')\n",
    "transactions = pd.read_csv('../transactions.csv')\n",
    "test = pd.read_csv('../test.csv')\n",
    "holidays = pd.read_csv('../holidays_events.csv')\n",
    "sampleSub = pd.read_csv('../sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# merge data (ignore transactions for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "\n",
    "train['dataT'] = 'train'\n",
    "test['dataT'] = 'test'\n",
    "train['date'] = pd.to_datetime(train['date'])\n",
    "test['date'] = pd.to_datetime(test['date'])\n",
    "data = pd.concat([train, test])\n",
    "data['date'] = pd.to_datetime(data['date'])\n",
    "\n",
    "data0 = pd.merge(data, stores, on=['store_nbr'], how='outer')\n",
    "print(data.shape, data0.shape)\n",
    "\n",
    "oil['date'] = pd.to_datetime(oil['date'])\n",
    "oil.set_index('date',inplace=True)\n",
    "oil_resampled = oil.resample('1D').asfreq()\n",
    "print(oil_resampled.isna().sum())\n",
    "oil_resampled.interpolate(inplace=True,limit_direction='both')\n",
    "oil_resampled.reset_index(inplace=True)\n",
    "print(oil_resampled.isna().sum())\n",
    "\n",
    "data0['date'] = pd.to_datetime(data0['date'])\n",
    "data1 = pd.merge(data0, oil_resampled, on=['date'], how='left')\n",
    "print(data1.shape, data0.shape)\n",
    "\n",
    "holidays['date'] = pd.to_datetime(holidays['date'])\n",
    "cityHolidays = holidays.loc[holidays.locale =='Local']#.locale_name.value_counts()\n",
    "cityHolidays.drop('locale', axis = 1, inplace=True)\n",
    "cityHolidays['description'] = 'Fundacion'\n",
    "cityHolidays.rename(columns={'locale_name':'city','type':'holidayType'}, inplace=True)\n",
    "cityHolidays.drop(264, axis = 0, inplace=True) # we have some duplicates\n",
    "data2 = pd.merge(data1, cityHolidays, on=['date','city'], how='left')\n",
    "#train2.dropna(inplace=True)\n",
    "print(data1.shape, data2.shape)\n",
    "\n",
    "regionalHolidays = holidays.loc[holidays.locale =='Regional']#.locale_name.value_counts()\n",
    "regionalHolidays.drop('locale', axis = 1, inplace=True)\n",
    "regionalHolidays['description'] = 'Provincializacion'\n",
    "regionalHolidays.rename(columns={'locale_name':'state'}, inplace=True)\n",
    "\n",
    "data3 = pd.merge(data2, regionalHolidays, on=['date','state'], how='left', suffixes=('','_reg'))\n",
    "print(data3.shape, data2.shape)\n",
    "\n",
    "nationalHolidays = holidays.loc[holidays.locale =='National']#.locale_name.value_counts()\n",
    "nationalHolidays.drop(['locale','locale_name'], axis = 1, inplace=True)\n",
    "nationalHolidays.description.unique()\n",
    "groups = ['Navidad', 'Mundial de futbol Brasil','Terremoto Manabi','dia del ano','Puente Dia de Difuntos','Grito de Independencia','Independencia de Guayaquil','Dia de la Madre','Batalla de Pichincha']\n",
    "for group in groups:\n",
    "    mask = nationalHolidays['description'].str.contains(group)\n",
    "    nationalHolidays.loc[mask, 'description'] = group\n",
    "nationalHolidays = nationalHolidays.drop_duplicates(subset=['date'], keep='first')\n",
    "\n",
    "data4 = pd.merge(data3, nationalHolidays, on=['date'], how='left', suffixes=('','_nat'))\n",
    "print(data3.shape, data4.shape)\n",
    "\n",
    "\n",
    "data5 = data4.copy()\n",
    "data5['holidayType'] = data5['holidayType'].combine_first(data5['type_reg'])\n",
    "data5['holidayType'] = data5['holidayType'].combine_first(data5['type_nat'])\n",
    "\n",
    "data5['description'] = data5['description'].combine_first(data5['description_reg'])\n",
    "data5['description'] = data5['description'].combine_first(data5['description_nat'])\n",
    "\n",
    "data5['transferred'] = data5['transferred'].combine_first(data5['transferred_reg'])\n",
    "data5['transferred'] = data5['transferred'].combine_first(data5['transferred_nat'])\n",
    "\n",
    "data5 = data5.drop(columns=['type_reg','type_nat','description_reg','description_nat','transferred_reg','transferred_nat'])\n",
    "\n",
    "print(data4.shape, data5.shape)\n",
    "\n",
    "data6 = data5.copy()\n",
    "propDicts = {}\n",
    "for f in ['family','city','state','type','holidayType','description','transferred']:\n",
    "    unique = data6[f].unique()\n",
    "    category_dict = {category: index for index, category in enumerate(unique)}\n",
    "    data6[f] = data6[f].map(category_dict)\n",
    "    propDicts[f] = category_dict\n",
    "\n",
    "flippedPropDicts = {}\n",
    "for key,value in propDicts.items():\n",
    "    flippedPropDicts[key] = {value: key for key, value in propDicts[key].items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data, data0, data1, data2, data3, data4, data5, transactions, train, oil, test, sampleSub, holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that the city holidays are the same in the holiday and store df\n",
    "uniqueLocalsHolidays = holidays.loc[holidays.locale =='Local'].locale_name.unique()\n",
    "uniqueLocalCities = stores.city.unique()\n",
    "\n",
    "intersection = set(uniqueLocalsHolidays).intersection(set(uniqueLocalCities))\n",
    "not_intersection_list1 = set(uniqueLocalsHolidays).difference(intersection)\n",
    "not_intersection_list2 = set(uniqueLocalCities).difference(intersection)\n",
    "\n",
    "print(intersection)\n",
    "print(not_intersection_list1)\n",
    "print(not_intersection_list2)\n",
    "#result: we have a couple cities without holidays, but that is fine, the rest is the same "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check, somewhat ok\n",
    "rows = train4.shape[0]\n",
    "holTypes = rows - train4.holidayType.isna().sum()\n",
    "holTypes1 = rows - train4.type_reg.isna().sum()\n",
    "holTypes2 = rows - train4.type_nat.isna().sum()\n",
    "sumTypes = holTypes + holTypes1 + holTypes2\n",
    "print(sumTypes, rows - train5.holidayType.isna().sum())\n",
    "\n",
    "holTypes = rows - train4.description.isna().sum()\n",
    "holTypes1 = rows - train4.description_reg.isna().sum()\n",
    "holTypes2 = rows - train4.description_nat.isna().sum()\n",
    "sumTypes = holTypes + holTypes1 + holTypes2\n",
    "print(sumTypes, rows - train5.description.isna().sum())\n",
    "\n",
    "holTypes = rows - train4.transferred.isna().sum()\n",
    "holTypes1 = rows - train4.transferred_reg.isna().sum()\n",
    "holTypes2 = rows - train4.transferred_nat.isna().sum()\n",
    "sumTypes = holTypes + holTypes1 + holTypes2\n",
    "print(sumTypes, rows - train5.transferred.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check how to make data staionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testPair = train.loc[(train.store_nbr == 6) & (train.family == 'BEAUTY')]\n",
    "testPair.set_index('date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testPair.sales.diff(14).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# peak at 14 -> predict 2 week difference\n",
    "# ACF: MA (moving average) part = how many last errors we include, e.g. error at t-1, t-2,.. -> 3-4 last erors\n",
    "# PACF: AR (autoregressive) part = how many lags we include                                  -> 3 lags\n",
    "tsplot(testPair.sales.diff(14).dropna(),lags = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stationarity(testPair.sales.diff(14).dropna())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testPair['target'] = testPair.sales.diff(14)\n",
    "testPair['shiftedSales14'] = testPair.sales.shift(14)\n",
    "\n",
    "n_lags = 3\n",
    "for n in range(n_lags):\n",
    "    l = n+1\n",
    "    testPair['target_lag'+str(l)] = testPair['target'].shift(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testPair['date'] = pd.to_datetime(testPair['date'])\n",
    "mask = testPair.date < pd.to_datetime(\"2017-01-1\")\n",
    "y_train = testPair['sales'][mask]\n",
    "y_test = testPair['sales'][~mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predicting stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arima\n",
    "\n",
    "~ 0.7 rmsle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_log_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p, d, q = 3,0,3\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Initialize an empty list to store ARIMA models\n",
    "arima_models = []\n",
    "time_index = testPair.index\n",
    "for train_index, test_index in tscv.split(testPair):\n",
    "    train_data = testPair.iloc[train_index]['target']\n",
    "    test_data = testPair.iloc[test_index]['target']\n",
    "    salesTrain = testPair.iloc[train_index]['sales']\n",
    "    shiftedSalesTest = testPair.iloc[test_index]['shiftedSales14']\n",
    "\n",
    "    # Fit ARIMA model\n",
    "    model = ARIMA(train_data, order=(p, d, q))\n",
    "    model_fit = model.fit()\n",
    "\n",
    "    # Store the trained model\n",
    "    arima_models.append(model_fit)\n",
    "\n",
    "    forecast_steps = len(test_data)\n",
    "    forecast = np.clip(model_fit.forecast(steps=forecast_steps), 0, 1e19)\n",
    "\n",
    "    gtSales = test_data+ shiftedSalesTest\n",
    "    predictedSales = forecast.values+ shiftedSalesTest\n",
    "\n",
    "    # Calculate RMSLE\n",
    "    rmsle = np.sqrt(mean_squared_log_error(test_data+ shiftedSalesTest, forecast.values+ shiftedSalesTest))\n",
    "    print(rmsle)\n",
    "\n",
    "    # Plot actual vs. predicted values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(time_index[train_index], salesTrain, label='Train')\n",
    "    plt.plot(time_index[test_index], predictedSales, label='Predicted')\n",
    "    plt.plot(time_index[test_index], gtSales, label='actual test')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Value')\n",
    "    plt.title(f'ARIMA Forecast (RMSLE: {rmsle:.4f})')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "~0.6-0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense,Input,concatenate\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testPair.columns\n",
    "featuresTrain = [\n",
    "    #'sales', \n",
    "    'onpromotion', \n",
    "    #'target',\n",
    "       'target_lag1', 'target_lag2', 'target_lag3']#, 'shiftedSales14']\n",
    "allF = featuresTrain + ['shiftedSales14','target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# window for lstm approach\n",
    "look_back = 100\n",
    "sequences = []\n",
    "labels = []\n",
    "offset = []\n",
    "for i in range(testPair.shape[0]-look_back):\n",
    "    window = testPair.iloc[i : i + look_back][['target','onpromotion']]\n",
    "    label = testPair.iloc[i + look_back]['target']#['target']  # Next data point as label\n",
    "    off = testPair.iloc[i + look_back][['shiftedSales14','sales']]\n",
    "    sequences.append(window)\n",
    "    labels.append(label)\n",
    "    offset.append(off)\n",
    "sequences, labels, offsets = np.array(sequences), np.array(labels), np.array(offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 5\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "n_features = sequences.shape[2]\n",
    "\n",
    "testPair.dropna(inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "for train_index, test_index in tscv.split(sequences):\n",
    "    X_train, X_test = sequences[train_index], sequences[test_index]\n",
    "    y_train, y_test = labels[train_index], labels[test_index]\n",
    "\n",
    "    # Create an LSTM model\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(look_back,n_features)))\n",
    "    model.add(LSTM(64, activation='relu', return_sequences=True))\n",
    "    model.add(LSTM(units=32, return_sequences=False))\n",
    "    model.add(Dense(1))  # Single output for univariate forecasting\n",
    "\n",
    "    # Compile and train the model\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    model.fit(X_train, y_train, epochs=50, batch_size=32)\n",
    "\n",
    "    # Evaluate on the test set\n",
    "    forecast = model.predict(X_test)\n",
    "    print(f'RMSLE for fold: {rmsle:.4f}')\n",
    "\n",
    "    gtSales = offsets[test_index,1]\n",
    "    predictedSales = np.clip(np.reshape(forecast, (forecast.shape[0])) +offsets[test_index,0], 0,1e19)\n",
    "\n",
    "    # Calculate RMSLE\n",
    "    rmsle = np.sqrt(mean_squared_log_error(gtSales, predictedSales))\n",
    "    print(rmsle)\n",
    "\n",
    "    # Plot actual vs. predicted values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(time_index[train_index], offsets[train_index,1], label='Train')\n",
    "    plt.plot(time_index[test_index], predictedSales, label='Predicted')\n",
    "    plt.plot(time_index[test_index], gtSales, label='actual test')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Value')\n",
    "    plt.title(f'ARIMA Forecast (RMSLE: {rmsle:.4f})')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM with direct values (no diff)\n",
    "- use loss directly to train with\n",
    "- sometimes just predicts 0\n",
    "\n",
    "~0.2-0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# window for lstm approach\n",
    "look_back = 100\n",
    "sequences = []\n",
    "labels = []\n",
    "for i in range(testPair.shape[0]-look_back):\n",
    "    window = testPair.iloc[i : i + look_back][['sales','onpromotion']]\n",
    "    label = testPair.iloc[i + look_back]['sales']#['target']  # Next data point as label\n",
    "    sequences.append(window)\n",
    "    labels.append(label)\n",
    "    offset.append(off)\n",
    "sequences, labels = np.array(sequences), np.array(labels)\n",
    "\n",
    "n_splits = 5\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "n_features = sequences.shape[2]\n",
    "\n",
    "testPair.dropna(inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "for train_index, test_index in tscv.split(sequences):\n",
    "    X_train, X_test = sequences[train_index], sequences[test_index]\n",
    "    y_train, y_test = labels[train_index], labels[test_index]\n",
    "\n",
    "    # Create an LSTM model\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(look_back,n_features)))\n",
    "    model.add(LSTM(64, activation='relu', return_sequences=False))\n",
    "    #model.add(LSTM(units=32, return_sequences=False))\n",
    "    model.add(Dense(1, activation='relu'))  # Single output for univariate forecasting\n",
    "\n",
    "    # Compile and train the model\n",
    "    model.compile(optimizer='adam', loss=tf.keras.losses.MSLE, metrics=['mse'])\n",
    "    model.fit(X_train, y_train, epochs=50, batch_size=320,validation_data=(X_test, y_test))\n",
    "\n",
    "    # Evaluate on the test set\n",
    "    forecast = model.predict(X_test)\n",
    "    print(f'RMSLE for fold: {rmsle:.4f}')\n",
    "\n",
    "    # Plot actual vs. predicted values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(time_index[train_index], labels[train_index], label='Train')\n",
    "    plt.plot(time_index[test_index], forecast, label='Predicted')\n",
    "    plt.plot(time_index[test_index], labels[test_index], label='actual test')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Value')\n",
    "    plt.title(f'ARIMA Forecast (RMSLE: {rmsle:.4f})')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict next 15 values (direct)\n",
    "~0.5 / 0.6 (val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore the test data\n",
    "train = data6.loc[data6.dataT == 'test']\n",
    "testPair = train.loc[(train.store_nbr == 25) & (train.family == propDicts['family']['BEAUTY'])]\n",
    "print(flippedPropDicts['city'][8],flippedPropDicts['state'][7], flippedPropDicts['holidayType'][1],flippedPropDicts['description'][1])\n",
    "testPair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data6.loc[data6.dataT == 'train']\n",
    "testPair = train.loc[(train.store_nbr == 6) & (train.family == propDicts['family']['BEAUTY'])]\n",
    "testPair.set_index('date', inplace=True)\n",
    "time_index = testPair.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testPair.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.set_random_seed(42)  # sets seeds for base-python, numpy and tf\n",
    "\n",
    "# window for lstm approach\n",
    "look_back = 100\n",
    "sequences = []\n",
    "labels = []\n",
    "n_predictedValues = 16 # I need to predict 16 values\n",
    "for i in range(testPair.shape[0]-look_back-n_predictedValues):\n",
    "    window = testPair.iloc[i : i + look_back][['sales','onpromotion','dcoilwtico', 'holidayType', 'description','transferred']]\n",
    "    label = testPair.iloc[(i + look_back) : (i + look_back + n_predictedValues)]['sales'].values#['target']  # Next data point as label\n",
    "    sequences.append(window)\n",
    "    labels.append(label)\n",
    "sequences, labels = np.array(sequences), np.array(labels)\n",
    "\n",
    "n_splits = 10\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "n_features = sequences.shape[2]\n",
    "\n",
    "testPair.dropna(inplace=True)\n",
    "\n",
    "# Create an LSTM model\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(look_back,n_features)))\n",
    "model.add(LSTM(64, activation='relu', return_sequences=False,\n",
    "               kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros'))\n",
    "#model.add(LSTM(units=32, return_sequences=False))\n",
    "model.add(Dense(n_predictedValues, activation='relu'))  # Single output for univariate forecasting\n",
    "# Compile and train the model\n",
    "model.compile(optimizer='adam', loss=tf.keras.losses.MSLE, metrics=['mse'])\n",
    "\n",
    "for train_index, test_index in tscv.split(sequences):\n",
    "    X_train, X_test = sequences[train_index], sequences[test_index]\n",
    "    y_train, y_test = labels[train_index], labels[test_index]\n",
    "\n",
    "    model.fit(X_train, y_train, epochs=5, batch_size=32,validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate on the test set\n",
    "forecast = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results - only sales & on promotion\n",
    "# 0.30/0.28 - 10 folds, 5 epochs, 32bs \n",
    "# 1.21/1.02 - 10 folds, 5 epochs, 320 bs\n",
    "# 0.45/0.50 - 10 folds, 10 epochs, 32bs\n",
    "# 3.37/4.03 - 50 folds, 5 epochs, 32 bs\n",
    "# 0.31/0.34 - 5 folds, 5 epochs, 32 bs\n",
    "\n",
    "# 10 folds, 5 epochs, 32bs \n",
    "# 0.30/0.28 - 'sales','onpromotion'\n",
    "# 0.30/0.31 - 'sales',\n",
    "# 3.30/4.34 - 'sales','onpromotion','dcoilwtico', 'holidayType', 'description','transferred'\n",
    "# 2.03/4.04 - 'sales','onpromotion', 'holidayType', 'description','transferred' -> oil seems to help for overfitting :o\n",
    "# 0.54/0.61 - 'sales','onpromotion', 'holidayType','transferred'                -> holiday description helps to overfit\n",
    "# 1.50/1.65 - 'sales','onpromotion','holidayType','transferred','dcoilwtico'\n",
    "# 0.24/0.25 - 'sales','onpromotion','holidayType'\n",
    "# 0.48/0.51 - 'sales','onpromotion','transferred'\n",
    "# -> 'sales','onpromotion','holidayType' seem to be best features\n",
    "\n",
    "# investigate zScore & min/max normalization\n",
    "\n",
    "#0.3080/0.2832 -> reproduceable \n",
    "# BITEXACTNESS - need to include set_seed in every function call, otherwise it doesn't work!!! in jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotidx(ind):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    mintimeIdx = test_index[ind] - look_back\n",
    "    plt.plot(time_index[mintimeIdx:test_index[ind]], X_test[ind,:,0], label='Train')\n",
    "    plt.plot(time_index[test_index][ind:ind+n_predictedValues], forecast[ind,:], label='Predicted')\n",
    "    plt.plot(time_index[test_index][ind:ind+n_predictedValues], labels[test_index][ind,:], label='actual test')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Value')\n",
    "    #plt.title(f'ARIMA Forecast (RMSLE: {rmsle:.4f})')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,10):\n",
    "    plotidx(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build dataset for lstm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "look_back = 100\n",
    "n_predictedValues = 15\n",
    "train = data6.loc[data6.dataT == 'train']\n",
    "train[['holidayType','description','transferred']] = train[['holidayType','description','transferred']].astype(int)\n",
    "#del data6\n",
    "sequences = []\n",
    "labels = []\n",
    "categories = []\n",
    "zTransMean = []\n",
    "zTransStd = []\n",
    "\n",
    "dict_zNorm = {}\n",
    "\n",
    "for storeId in train.store_nbr.unique():\n",
    "\n",
    "    if storeId > 1:\n",
    "        break\n",
    "\n",
    "    storeDf = train.loc[train.store_nbr == storeId]\n",
    "\n",
    "    print('store id', storeId)\n",
    "    for family in storeDf.family.unique():\n",
    "        familyDf = storeDf.loc[storeDf.family == family]\n",
    "\n",
    "        mean = familyDf.sales.mean()\n",
    "        stdDev = familyDf.sales.std()\n",
    "\n",
    "        a = {}\n",
    "        a['mean'] = mean\n",
    "        a['std'] = stdDev\n",
    "        dict_zNorm[str(storeId) + '_'+str(family)] = a\n",
    "\n",
    "        familyDf.sales = (familyDf.sales - mean) / stdDev\n",
    "\n",
    "        \n",
    "        #print(stdDev, familyDf.isna().any())\n",
    "\n",
    "\n",
    "        #print('fam id', family)\n",
    "        c = familyDf.city.unique()\n",
    "        if len(c) > 1:\n",
    "            print('somehow wrong!!')\n",
    "        city = c[0]\n",
    "        state = familyDf.state.iloc[0]\n",
    "        type = familyDf.type.iloc[0]\n",
    "        cluster = familyDf.cluster.iloc[0]\n",
    "\n",
    "\n",
    "        # window for lstm approach\n",
    "        \n",
    "        for i in range(familyDf.shape[0]-look_back-n_predictedValues):\n",
    "            window = familyDf.iloc[i : i + look_back][['sales','onpromotion','dcoilwtico','holidayType','description','transferred']].to_numpy()\n",
    "            label = familyDf.iloc[(i + look_back) : (i + look_back + n_predictedValues)]['sales'].values#['target']  # Next data point as label\n",
    "            if (not np.isnan(label).any()) and (not np.isnan(window).any()):\n",
    "                sequences.append(window)\n",
    "                categories.append([int(storeId), int(family), int(city), int(state), int(type), int(cluster)])\n",
    "                labels.append(label)\n",
    "                zTransMean.append(mean)\n",
    "                zTransStd.append(stdDev)\n",
    "    #        break\n",
    "    #    break\n",
    "    #break\n",
    "\n",
    "sequences, labels, categories, zTransMean, zTransStd = np.array(sequences), np.array(labels), np.array(categories), np.array(zTransMean),np.array(zTransStd)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences.shape, labels.shape, categories.shape\n",
    "np.savez('windowed_sequences_.npz', arr1=sequences, arr2=labels, arr3=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(sequences).any(),np.isnan(labels).any(),np.isnan(categories).any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train 1 lstm for all store/product pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leng = sequences.shape[0]\n",
    "div = int(sequences.shape[0]*0.9)\n",
    "X_train = [sequences[0:div,:,:], categories[0:div,:]]\n",
    "X_test =  [sequences[div:leng,:,:], categories[div:leng,:]]\n",
    "\n",
    "y_train = labels[0:div,0:2]\n",
    "y_test  = labels[div:leng,0:2]\n",
    "\n",
    "zMean_test = zTransMean[div:leng]\n",
    "zStd_test = zTransStd[div:leng]\n",
    "\n",
    "catNum = categories.shape[1]\n",
    "n_features = sequences.shape[2]\n",
    "n_predictedValues = y_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_input = Input(shape=(look_back,n_features), name='seq_input')\n",
    "feat_input = Input(shape=(catNum,), name='feat_input')\n",
    "\n",
    "lstm_out = LSTM(units=64, activation='relu', return_sequences=False)(seq_input)\n",
    "#lstm_out = LSTM(units=64, activation='relu', return_sequences=False)(lstm_out)\n",
    "#print(lstm_out.shape)\n",
    "#combined_input = concatenate([lstm_out, feat_input])\n",
    "#combined_input = Dense(128, activation='relu')(combined_input)\n",
    "#combined_input = Dense(128, activation='relu')(combined_input)\n",
    "output = Dense(n_predictedValues, activation='relu')(lstm_out)\n",
    "model = Model(inputs=[seq_input, feat_input], outputs=output)\n",
    "model.compile(optimizer='adam', loss='mse')#tf.keras.losses.MSLE, metrics=['mse'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=32,validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Input(shape=(look_back,n_features)))\n",
    "model.add(LSTM(64, activation='relu', return_sequences=False))\n",
    "#model.add(LSTM(units=32, return_sequences=False))\n",
    "model.add(Dense(n_predictedValues, activation='relu'))  # Single output for univariate forecasting\n",
    "# Compile and train the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.fit(X_train[0], y_train, epochs=5, batch_size=32,validation_data=(X_test[0], y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predZ = model.predict(X_test)\n",
    "pred = (predZ * np.reshape(zStd_test, (zStd_test.shape[0],1))) + np.reshape(zMean_test, (zMean_test.shape[0],1))\n",
    "yT = (y_test * np.reshape(zStd_test, (zStd_test.shape[0],1))) + np.reshape(zMean_test, (zMean_test.shape[0],1))\n",
    "\n",
    "rmsle = np.sqrt(mean_squared_log_error(pred, yT))\n",
    "print(rmsle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred, yT, zMean_test,zStd_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "60f2b4b2b39245c89a47c8dbe671288aea181e96fbe781c7f5f13eb9eb69cf46"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
