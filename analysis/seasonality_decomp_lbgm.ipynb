{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import plotly.offline as pyo\n",
    "from plotly import subplots\n",
    "import plotly.graph_objects as go\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.tsa.api as smt\n",
    "\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from tensorflow.keras.models import Sequential\n",
    "#from tensorflow.keras.layers import LSTM, Dense,Input,concatenate\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import lightgbm as lgb\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "from baseFunctions import *\n",
    "from data_helpers import processData6, featureEngineering, getSequencesFast, removeOutliers, create_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, propDicts, flippedPropDicts = processData6(oneHotEnodeCatFeat=True)\n",
    "data, timeFeatures = featureEngineering(data,splits=[2,2,2,2],oneHotWeekday=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # helps to learn function, but doesn't help with overfitting\n",
    "data['linear_time'] = (data['linear_time'] - data.linear_time.mean()) /data.linear_time.std()\n",
    "data['day_of_year'] = (data['day_of_year'] - data.day_of_year.mean()) /data.day_of_year.std()\n",
    "data['dcoilwtico'] = (data['dcoilwtico'] - data.dcoilwtico.mean()) /data.dcoilwtico.std()\n",
    "data['transactions'] = (data['transactions'] - data.transactions.mean()) /data.transactions.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: build more features of oil & transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addLaggedFutureHolidays(storeDf, features = ['transferred', 'holidayType'], lags = 10):\n",
    "       transferredF = []\n",
    "       for i in range(lags):\n",
    "              lag = i+1 # (1-5)\n",
    "              for f in features:\n",
    "                     f0 = f+'_lag'+str(lag)\n",
    "                     f1 = f+'_lag-'+str(lag)\n",
    "                     storeDf.loc[:,[f0]] = storeDf[f].shift(lag).fillna(0)\n",
    "                     storeDf.loc[:,[f1]] = storeDf[f].shift(-lag).fillna(0)\n",
    "                     transferredF.append(f0)\n",
    "                     transferredF.append(f1)\n",
    "       return storeDf, transferredF\n",
    "\n",
    "\n",
    "def dataProcessing(storeId, family, date_string_val, date_string_test, predictDiff, seasonalFDiff, seasonalLags, targetLags, rolling, initial_lag, prepPred = False):\n",
    "       storeDf = data.loc[(data.store_nbr == storeId) & (data.family == family)]\n",
    "\n",
    "       if not prepPred:\n",
    "              storeDf = storeDf.loc[(storeDf.dataT == 'train')]\n",
    "              mask = (storeDf.date >= date_string_val)\n",
    "              storeDf.loc[mask,['dataT']] = 'val'\n",
    "\n",
    "       # ln tranformation\n",
    "       storeDf.loc[:,['logSales']] = np.log(storeDf.sales + 1)\n",
    "       storeDf.loc[:,['transactions']] = storeDf['transactions'].fillna(0)\n",
    "\n",
    "       \n",
    "       relevantSales = storeDf.loc[storeDf.dataT =='train']\n",
    "       dfLen = storeDf.shape[0]\n",
    "\n",
    "\n",
    "       seasonF = []\n",
    "       for period in [7, 14,21,28,52,104,365]:\n",
    "              for pf in ['logSales','transactions','dcoilwtico']:\n",
    "                     dec = sm.tsa.seasonal_decompose(relevantSales[pf],period = period, model = 'additive')\n",
    "                     #print(period, max(dec.seasonal))\n",
    "                     f = pf+'Seasonality_'+str(period)\n",
    "                     storeDf.loc[:,[f]] = addSeasonality(period, dec, dfLen)\n",
    "                     seasonF.append(f)\n",
    "       seasonF.append('dcoilwtico')\n",
    "       #print('done with seasonal decompose')\n",
    "\n",
    "       if predictDiff:\n",
    "              storeDf.loc[:,['ref']] = storeDf['logSales'].shift(initial_lag)\n",
    "              storeDf.loc[:,['target']] = storeDf['logSales'] - storeDf['ref']\n",
    "       else:\n",
    "              storeDf.loc[:,['target']] = storeDf['logSales']\n",
    "\n",
    "              \n",
    "       seasonDiffF2 = []\n",
    "       for f in seasonF:\n",
    "              newF = f+'_diff'+str(seasonalFDiff)\n",
    "              storeDf.loc[:,[newF]] = storeDf[f].diff(seasonalFDiff)\n",
    "              seasonDiffF2.append(newF)\n",
    "       \n",
    "       # seasonal featuers lags\n",
    "       seasonalFLags = seasonF + seasonDiffF2\n",
    "       featuresForSLag = []#trainF\n",
    "       for i in seasonalLags:\n",
    "              lag = i\n",
    "              newF = [seasonalFLags[j] + '_lag' + str(lag) for j in range(len(seasonalFLags))]\n",
    "              featuresForSLag = featuresForSLag + newF\n",
    "              storeDf.loc[:,newF] = storeDf[seasonalFLags].shift(lag).to_numpy()\n",
    "\n",
    "       seasonalFeatures = seasonF + seasonDiffF2 + featuresForSLag\n",
    "       #print('done with seasonal features')\n",
    "       \n",
    "\n",
    "       \n",
    "       # lag features / how many past datapoints are we tain\n",
    "       featuresForLag = ['target']\n",
    "       targetLagF = []#trainF\n",
    "       for i in targetLags:\n",
    "              lag = i+initial_lag\n",
    "              newF = [featuresForLag[j] + '_lag' + str(lag) for j in range(len(featuresForLag))]\n",
    "              targetLagF = targetLagF + newF\n",
    "              storeDf.loc[:,newF] = storeDf[featuresForLag].shift(lag).to_numpy()\n",
    "       #print('done with target lags')\n",
    "       \n",
    "       #------ also add future holidays!--------\n",
    "       storeDf, transferredF = addLaggedFutureHolidays(storeDf, features=['transferred','holidayType'], lags=10)\n",
    "\n",
    "       # rolling features\n",
    "       createdF = seasonF + targetLagF\n",
    "       rollingF = []\n",
    "       for rol in rolling:\n",
    "              for i in range(len(createdF)):\n",
    "                     #if 'sales_t-16'  in lagF[i]:\n",
    "                     #if createdF[i] in ['target','dcoilwtico']:# or 'dcoilwtico' in lagF[i]:#'target'  in lagF[i]:\n",
    "                            fm = createdF[i]+'_rollingM' + str(rol)\n",
    "                            fs = createdF[i]+'_rollingS' + str(rol)\n",
    "                            rollingF.append(fm)\n",
    "                            rollingF.append(fs)\n",
    "                            storeDf.loc[:,[fm]] = storeDf[createdF[i]].rolling(rol).mean()#.copy()\n",
    "                            storeDf.loc[:,[fs]] = storeDf[createdF[i]].rolling(rol).std()#.copy()\n",
    "              #print('done with rolling:', rol)\n",
    "       #print('done with rolling')\n",
    "\n",
    "\n",
    "       allF = rollingF + timeF + trainF +transferredF +seasonalFeatures + targetLagF\n",
    "       # we get a matrix that predicts only 1 timestamp -> stride it\n",
    "       if len(rolling) == 0:\n",
    "              storeDf = storeDf.iloc[max(max(targetLags), max(seasonalLags))+initial_lag+1:storeDf.shape[0]]\n",
    "       else:\n",
    "              storeDf = storeDf.iloc[max(max(targetLags), max(seasonalLags))+initial_lag+max(rolling)+1:storeDf.shape[0]]\n",
    "\n",
    "       \n",
    "       if prepPred:\n",
    "              train_subDf = storeDf.loc[(storeDf.date < date_string_test) & (storeDf.dataT == 'train')]\n",
    "              test_subDf  = storeDf.loc[(storeDf.date >= date_string_test)& (storeDf.dataT == 'train')]\n",
    "              pred_subDf = storeDf.loc[storeDf.dataT == 'test']\n",
    "       else:\n",
    "              train_subDf = storeDf.loc[storeDf.date < date_string_test]\n",
    "              test_subDf  = storeDf.loc[storeDf.date >= date_string_test]\n",
    "              pred_subDf = storeDf.loc[storeDf.dataT =='val']\n",
    "    \n",
    "       return train_subDf, test_subDf, pred_subDf, allF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" create training data based on lagged features not 2 sequences \"\"\"\n",
    "trainF = [\n",
    "       #'store_nbr', 'family', \n",
    "       #'sales', \n",
    "       'onpromotion',# 'dataT',\n",
    "       #'city', 'state', 'type', 'cluster', \n",
    "       'dcoilwtico', \n",
    "       'holidayType',\n",
    "       'description', \n",
    "       'transferred', \n",
    "       'transactions', \n",
    "       'store_closed',\n",
    "       'weekday_0', 'weekday_1', 'weekday_2',\n",
    "       'weekday_3', 'weekday_4', 'weekday_5', 'weekday_6',\n",
    "       #'type_0','type_1','type_2','type_3','type_4', #store type\n",
    "       'holidayType_0','holidayType_1','holidayType_2','holidayType_3','holidayType_4','holidayType_5','holidayType_6',\n",
    "       'description_0','description_1','description_2','description_3','description_4','description_5','description_6','description_7','description_8','description_9','description_10','description_11','description_12','description_13','description_14','description_15','description_16','description_17','description_18'\n",
    "\n",
    "       ]\n",
    "timeF = [\n",
    "       'linear_time', 'day_of_year', 'day_of_year_f12_0', 'day_of_year_f104_0','day_of_year_f24_0',  'day_of_year_f52_0',\n",
    "       'day_of_year_f12_180', 'day_of_year_f104_180','day_of_year_f24_180','day_of_year_f52_180', \n",
    "       #'weekday', \n",
    "       'month'\n",
    "       ]\n",
    "\n",
    "initial_lag = 16 # 16 = independent from previous predictions\n",
    "seasonalFDiff = 1\n",
    "seasonalLags=[1,2,3,4,5,6,7,14,21,52,104]#range(21)\n",
    "targetLags = [0,1,2,3,4,5,6,7,14,21,52,10]#range(21)# [7,14,21,28,35,42,52] #range(52)#[7,14,21]\n",
    "rolling = [7,14,21,28,35,42,52,104]\n",
    "predictDiff =False\n",
    "logTransform=True\n",
    "\n",
    "# Date string\n",
    "date_string_val = \"2017-07-01\"#\"2017-05-01\"\n",
    "date_string_test = \"2017-01-01\"\n",
    "date_string_val = \"2017-07-15\"#\"2017-05-01\"\n",
    "date_string_test = \"2017-07-01\"\n",
    "\n",
    "\n",
    "for familyId in [12]:#[6]: #data.family.unique():\n",
    "       # start with only some families\n",
    "       print(familyId)\n",
    "       familyDf = data.loc[data.family==familyId]  \n",
    "\n",
    "       for storeId in [44]:#[41]: #data.store_nbr.unique():\n",
    "              print('store',storeId)\n",
    "              storeDf = familyDf.loc[(familyDf.store_nbr == storeId)] \n",
    "              # & (storeDf.date > \"2015-07-01\")]\n",
    "\n",
    "              train_subDf, test_subDf, val_subDf, allF = dataProcessing(storeDf, date_string_val, date_string_test, predictDiff, seasonalFDiff, seasonalLags, targetLags, rolling, initial_lag, prepPred = True)\n",
    "\n",
    "\n",
    "targetF = 'target'\n",
    "if predictDiff:\n",
    "       baseTrain = train_subDf['ref'].to_numpy()\n",
    "       baseTest  = test_subDf['ref'].to_numpy()\n",
    "       baseVal   = val_subDf['ref'].to_numpy()\n",
    "else:\n",
    "       baseTrain, baseTest, baseVal = [],[],[]\n",
    "\n",
    "X_train = train_subDf[allF].to_numpy()\n",
    "y_train = train_subDf[[targetF]].to_numpy()\n",
    "X_test  =  test_subDf[allF].to_numpy()\n",
    "y_test  =  test_subDf[[targetF]].to_numpy()  \n",
    "X_val   =  val_subDf[allF].to_numpy()\n",
    "y_val   =  val_subDf[[targetF]].to_numpy() \n",
    "\n",
    "np.isnan(X_train).any(),np.isnan(X_test).any(),np.isnan(X_val).any(),np.isnan(y_train).any(),np.isnan(y_test).any(),np.isnan(y_val).any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # compare against base lgbm that just predicts always t+16\n",
    "\n",
    "def calcLossLGBM(pred, y, logTransform, predictDiff, base):\n",
    "    if predictDiff:\n",
    "        pred = np.reshape(pred, (y.shape[0])) +  np.reshape(base, (y.shape[0]))\n",
    "        y = np.reshape(y, (y.shape[0])) + np.reshape(base, (y.shape[0]))\n",
    "\n",
    "    if logTransform:\n",
    "        a = np.exp(pred) -1\n",
    "        y = np.exp(y) -1 \n",
    "    else:\n",
    "        a = (pred)\n",
    "        y = (y)\n",
    "\n",
    "    if (a < 0).any():\n",
    "        a = np.clip(a, 0, 1e20)\n",
    "\n",
    "    rmsleTrain = np.sqrt(mean_squared_log_error(a,y))\n",
    "    return rmsleTrain\n",
    "def plotLGBM(i, logTransform, pred, y, predictDiff, base):\n",
    "    if predictDiff:\n",
    "        pred = np.reshape(pred, (y.shape[0])) +  np.reshape(base, (y.shape[0]))\n",
    "        y = np.reshape(y, (y.shape[0])) + np.reshape(base, (y.shape[0]))\n",
    "    if logTransform:\n",
    "        a = np.exp(pred[i:i+16])\n",
    "        y = np.exp(y[i:i+16])\n",
    "    else:\n",
    "        a = (pred[i:i+16])\n",
    "        y = (y[i:i+16])\n",
    "    x = range(len(a))   \n",
    "\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=1, figsize=(8, 6))\n",
    "    axs.plot(x, y, color='blue',label='Original')\n",
    "    axs.plot(x, a, color='red',label='pred')\n",
    "    axs.set_title('index: '+str(i))\n",
    "    fig.subplots_adjust(hspace=0.5)\n",
    "\n",
    "# Set parameters for LGBM model\n",
    "params = {\n",
    "    'boosting':'gbdt',#'gbdt', #'rf' #'dart'\n",
    "    'objective': 'regression',  # Assuming you're doing regression\n",
    "    'metric': 'mse',  # Mean squared error\n",
    "    'num_leaves': 5,\n",
    "    #'lambda_l1': 0.1,\n",
    "    #'lambda_l2': 0.2,\n",
    "    #'max_depth':10,\n",
    "    #'learning_rate': 0.1,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': -1,\n",
    "    'force_col_wise':True,\n",
    "    'num_iterations':100\n",
    "}   \n",
    "\n",
    "# Train the model\n",
    "gbm = lgb.train(params, lgb.Dataset(X_train, label=y_train), valid_sets=[\n",
    "    lgb.Dataset(X_test, label=y_test)\n",
    "    #,lgb.Dataset(X_val, label=y_val)\n",
    "    ]#,num_boost_round=100\n",
    ",callbacks=[lgb.early_stopping(stopping_rounds=100)]\n",
    ")  \n",
    "predtrainLGBM = gbm.predict(X_train)\n",
    "predtestLGBM = gbm.predict(X_test)\n",
    "predvalLGBM = gbm.predict(X_val)\n",
    "\n",
    "print('errors: ', calcLossLGBM(predtrainLGBM, y_train, logTransform, predictDiff, base=baseTrain), calcLossLGBM(predtestLGBM, y_test, logTransform, predictDiff, baseTest), calcLossLGBM(predvalLGBM, y_val, logTransform, predictDiff, baseVal))\n",
    "\n",
    "for i in range(0):\n",
    "    plotLGBM(i*16, logTransform, predtrainLGBM, y_train, predictDiff, baseTrain)\n",
    "    plotLGBM(i*16, logTransform, predtestLGBM, y_test, predictDiff, baseTest)\n",
    "    plotLGBM(i*16, logTransform, predvalLGBM, y_val, predictDiff, baseVal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0.59 on test is arima benchmark (on smaller test set)\n",
    "#0.55 is sarima benchmark (on smaller test set)\n",
    "# -> we are in same ballpark\n",
    "\n",
    "for i in range(1):\n",
    "    plotLGBM(i*16, logTransform, predtrainLGBM, y_train, predictDiff, baseTrain)\n",
    "    plotLGBM(i*16, logTransform, predtestLGBM, y_test, predictDiff, baseTest)\n",
    "    plotLGBM(i*16, logTransform, predvalLGBM, y_val, predictDiff, baseVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = gbm.feature_importance()\n",
    "for name, importance in zip(allF, importances):\n",
    "    print(f'{name}: {importance}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict everything with LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask.distributed import Client, progress,LocalCluster\n",
    "from dask import delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@delayed\n",
    "def fitLGBM(storeId, family, date_string_val, date_string_test, predictDiff, seasonalFDiff, seasonalLags, targetLags, rolling, initial_lag):\n",
    "       train_subDf, test_subDf, pred_subDf, allF = dataProcessing(storeId, family, date_string_val, date_string_test, predictDiff, seasonalFDiff, seasonalLags, targetLags, rolling, initial_lag, prepPred = True)\n",
    "       \n",
    "       targetF = 'target'\n",
    "       if predictDiff:\n",
    "              baseTrain = train_subDf['ref'].to_numpy()\n",
    "              baseTest  = test_subDf['ref'].to_numpy()\n",
    "              baseVal   = pred_subDf['ref'].to_numpy()\n",
    "       else:\n",
    "              baseTrain, baseTest, baseVal = [],[],[]\n",
    "\n",
    "       X_train = train_subDf[allF].to_numpy()\n",
    "       y_train = train_subDf[targetF].to_numpy()\n",
    "       X_test  =  test_subDf[allF].to_numpy()\n",
    "       y_test  =  test_subDf[targetF].to_numpy()  \n",
    "       X_pred   =  pred_subDf[allF].to_numpy()\n",
    "\n",
    "       params = {\n",
    "           'boosting':'gbdt',#'gbdt', #'rf' #'dart'\n",
    "           'objective': 'regression',  # Assuming you're doing regression\n",
    "           'metric': 'mse',  # Mean squared error\n",
    "           'num_leaves': 5,\n",
    "           'feature_fraction': 0.9,\n",
    "           'bagging_fraction': 0.8,\n",
    "           'bagging_freq': 5,\n",
    "           'verbose': -1,\n",
    "           'force_col_wise':True,\n",
    "           #'num_iterations':500\n",
    "       }   \n",
    "\n",
    "       # Train the model\n",
    "       gbm = lgb.train(params, lgb.Dataset(X_train, label=y_train),500, valid_sets=[lgb.Dataset(X_test, label=y_test)],callbacks=[lgb.early_stopping(stopping_rounds=100)])\n",
    "       predtrainLGBM = gbm.predict(X_train)\n",
    "       predtestLGBM = gbm.predict(X_test)\n",
    "       predLGBM = gbm.predict(X_pred)\n",
    "\n",
    "       trainLoss = calcLossLGBM(predtrainLGBM, y_train, logTransform, predictDiff, base=baseTrain)\n",
    "       testLoss  = calcLossLGBM(predtestLGBM, y_test, logTransform, predictDiff, baseTest)\n",
    "       storeId = storeDf.store_nbr.unique()[0]\n",
    "       family = storeDf.family.unique()[0]\n",
    "       print('store',storeId,'family',family,'errors: ', trainLoss, testLoss,gbm.best_iteration)\n",
    "\n",
    "       subDf = pred_subDf[['id','sales']]\n",
    "       subDf.loc[:,['sales']] = np.exp(predLGBM)-1\n",
    "       subDf.loc[:,['trainE']] = trainLoss\n",
    "       subDf.loc[:,['testE']] = testLoss\n",
    "       subDf.loc[:,['bestIt']] = gbm.best_iteration\n",
    "\n",
    "       return subDf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" create training data based on lagged features not 2 sequences \"\"\"\n",
    "trainF = [\n",
    "       #'store_nbr', 'family', \n",
    "       #'sales', \n",
    "       'onpromotion',# 'dataT',\n",
    "       #'city', 'state', 'type', 'cluster', \n",
    "       'dcoilwtico', \n",
    "       'holidayType',\n",
    "       'description', \n",
    "       'transferred', \n",
    "       'transactions', \n",
    "       'store_closed',\n",
    "       'weekday_0', 'weekday_1', 'weekday_2',\n",
    "       'weekday_3', 'weekday_4', 'weekday_5', 'weekday_6',\n",
    "       #'type_0','type_1','type_2','type_3','type_4', #store type\n",
    "       'holidayType_0','holidayType_1','holidayType_2','holidayType_3','holidayType_4','holidayType_5','holidayType_6',\n",
    "       'description_0','description_1','description_2','description_3','description_4','description_5','description_6','description_7','description_8','description_9','description_10','description_11','description_12','description_13','description_14','description_15','description_16','description_17','description_18'\n",
    "\n",
    "       ]\n",
    "timeF = [\n",
    "       'linear_time', 'day_of_year', 'day_of_year_f12_0', 'day_of_year_f104_0','day_of_year_f24_0',  'day_of_year_f52_0',\n",
    "       'day_of_year_f12_180', 'day_of_year_f104_180','day_of_year_f24_180','day_of_year_f52_180', \n",
    "       #'weekday', \n",
    "       'month'\n",
    "       ]\n",
    "\n",
    "initial_lag = 16 # 16 = independent from previous predictions\n",
    "seasonalFDiff = 1\n",
    "seasonalLags=[1,2,3,4,5,6,7,14,21,52,104]#range(21)\n",
    "targetLags = [0,1,2,3,4,5,6,7,14,21,52,10]#range(21)# [7,14,21,28,35,42,52] #range(52)#[7,14,21]\n",
    "rolling = [7,14,21,28,35,42,52,104]\n",
    "predictDiff =False\n",
    "logTransform=True\n",
    "\n",
    "# Date string\n",
    "date_string_val = \"2017-07-15\"#\"2017-05-01\"\n",
    "date_string_test = \"2017-07-31\"\n",
    "\n",
    "cluster = LocalCluster(n_workers=4)\n",
    "client = Client(cluster)\n",
    "#cluster.scale(threads=1, memory=\"4GB\", GPUs=-1)\n",
    "\n",
    "results_list = []\n",
    "for familyId in data.family.unique():\n",
    "       # start with only some families\n",
    "       print('processing',familyId)\n",
    "       familyDf = data.loc[data.family==familyId]  \n",
    "\n",
    "       for storeId in data.store_nbr.unique():\n",
    "              #storeDf = familyDf.loc[(familyDf.store_nbr == storeId)] \n",
    "\n",
    "              result = dask.delayed(fitLGBM)(storeId, familyId, date_string_val, date_string_test, predictDiff, seasonalFDiff, seasonalLags, targetLags, rolling, initial_lag)\n",
    "              results_list.append(result)\n",
    "df_list = dask.compute(*results_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionsParallel = pd.read_csv(\"multiprocess_lgbm.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionsParallel['testE'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionsParallel.set_index('id')['sales'].to_csv('sub_multiprocess.csv') # = 0.52 (vs.  0.41 sarima)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: find difference to sarima & investigate big errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# correlation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_subDf[corrF].isna().sum()).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrF = allF + ['target']\n",
    "print(len(corrF))\n",
    "corr = train_subDf[corrF].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr['target_lag16']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = corr['target'].dropna().sort_values()\n",
    "a[0:60], a[a.shape[0]-50:a.shape[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[a.shape[0]-150:a.shape[0]-100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN / log regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" use a fully connected NN  for sequence with 16\"\"\"\n",
    "from keras import backend as K\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1))\n",
    "\n",
    "tf.keras.utils.set_random_seed(42)\n",
    "\n",
    "n_features = X_train.shape[1]\n",
    "\n",
    "#model = Sequential()\n",
    "#model.add(Dense(1,input_shape=(n_features,), activation='linear'))\n",
    "#model.add(Dense(64))\n",
    "#model.add(Dense(1))\n",
    "input_layer = Input(shape=(n_features,))\n",
    "x = BatchNormalization()(input_layer) # helps to learn function, but doesn't help with overfitting\n",
    "# Define the layers and connect them\n",
    "#idden_layer = Dense(16, activation='relu')(input_layer)\n",
    "\n",
    "output_layer = Dense(1, activation='linear')(x)\n",
    "\n",
    "# Define the model\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=100)\n",
    "model.fit(X_train, y_train, epochs=2000, batch_size=6400,validation_data=(X_test, y_test), callbacks=[early_stopping])\n",
    "\n",
    "predtrain = model.predict(X_train, verbose=False)\n",
    "predtest  = model.predict(X_test, verbose=False)\n",
    "predvalLGBM = model.predict(X_val, verbose=False)\n",
    "#baseTrain, baseTest, baseVal = [],[],[]\n",
    "\n",
    "print('errors: ', calcLossLGBM(predtrain, y_train, logTransform, predictDiff, baseTrain), calcLossLGBM(predtest, y_test, logTransform, predictDiff, baseTest), calcLossLGBM(predvalLGBM, y_val, logTransform, predictDiff, baseVal))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rrors:  0.5459896492508169 0.6072244010093094 0.5857688678529015     0.3687   lags 7,14,21\n",
    "#errors:  0.5412146789623853 0.6114486464169528 0.5711310694066851    0.3739\n",
    "a = {}\n",
    "for i,weight in enumerate(model.layers[2].get_weights()[0]):\n",
    "    a[allF[i]] = weight\n",
    "b = pd.DataFrame(a)\n",
    "c = b.iloc[0].sort_values()\n",
    "c[0:30], c[b.shape[1]-30:b.shape[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predtrain = model.predict(X_train, verbose=False)\n",
    "predtest  = model.predict(X_test, verbose=False)\n",
    "print('errors: ', calcLossLGBM(predtrain, y_train, logTransform, predictDiff, baseTrain), calcLossLGBM(predtest, y_test, logTransform, predictDiff, baseTest))\n",
    "\n",
    "for i in range(1):\n",
    "    plotLGBM(i*16, logTransform, predtrainLGBM, y_train, predictDiff, baseTrain)\n",
    "    plotLGBM(i*16, logTransform, predtestLGBM, y_test, predictDiff, baseTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# some more feature analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in allF:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = ['sales', 'weekday',\n",
    " #'month',\n",
    " 'onpromotion',\n",
    " #'dcoilwtico',\n",
    " 'holidayType',\n",
    " 'description',\n",
    " 'transferred',\n",
    " 'store_closed',\n",
    " #'logSalesSeasonality_7',\n",
    " #'logSalesSeasonality_14',\n",
    " #'logSalesSeasonality_21',\n",
    " #'logSalesSeasonality_28',\n",
    " #'logSalesSeasonality_52',\n",
    " #'logSalesSeasonality_365'\n",
    " ]\n",
    "\n",
    "train_subDf[f].iloc[0:50].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = ['sales', #'weekday',\n",
    " #'month',\n",
    " #'onpromotion',\n",
    " 'dcoilwtico',\n",
    " 'dcoilwtico_diff1',\n",
    " #'holidayType',\n",
    " #'description',\n",
    " #'transferred',\n",
    " #'store_closed',\n",
    " 'logSalesSeasonality_7',\n",
    " 'logSalesSeasonality_14',\n",
    " #'logSalesSeasonality_21',\n",
    " #'logSalesSeasonality_28',\n",
    " #'logSalesSeasonality_52',\n",
    " #'logSalesSeasonality_365'\n",
    " ]\n",
    "\n",
    "train_subDf[f].iloc[0:50].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.13 ('storeSales')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "17f27e0cbed1f29822e509a94c958ed0e80b7d9abe162097755686a614de4732"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
