{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import plotly.offline as pyo\n",
    "from plotly import subplots\n",
    "import plotly.graph_objects as go\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.tsa.api as smt\n",
    "\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from tensorflow.keras.models import Sequential\n",
    "#from tensorflow.keras.layers import LSTM, Dense,Input,concatenate\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import lightgbm as lgb\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "from baseFunctions import *\n",
    "from data_helpers import processData6, featureEngineering, getSequencesFast, removeOutliers, create_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, propDicts, flippedPropDicts = processData6()\n",
    "data, timeFeatures = featureEngineering(data,splits=[2,2,2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" create training data based on lagged features not 2 sequences \"\"\"\n",
    "trainF = [\n",
    "       #'store_nbr', 'family', \n",
    "       #'sales', \n",
    "       'onpromotion',# 'dataT',\n",
    "       #'city', 'state', 'type', 'cluster', \n",
    "       'dcoilwtico', \n",
    "       'holidayType',\n",
    "       'description', \n",
    "       'transferred', \n",
    "       #'transactions', \n",
    "       'store_closed']\n",
    "timeF = [\n",
    "       'linear_time', 'day_of_year', 'day_of_year_f12_0', 'day_of_year_f104_0','day_of_year_f24_0',  'day_of_year_f52_0',\n",
    "       'day_of_year_f12_180', 'day_of_year_f104_180','day_of_year_f24_180','day_of_year_f52_180', \n",
    "       'weekday', 'month'\n",
    "       ]\n",
    "\n",
    "initial_lag = 1 # 16 = independent from previous predictions\n",
    "lags = 21\n",
    "rolling = [7,14]\n",
    "\n",
    "# Date string\n",
    "date_string_val = \"2017-07-01\"#\"2017-05-01\"\n",
    "date_string_test = \"2016-09-01\"\n",
    "\n",
    "\n",
    "for familyId in [0]:#[6]: #data.family.unique():\n",
    "       # start with only some families\n",
    "       if familyId > 8:\n",
    "          continue\n",
    "\n",
    "       print(familyId)\n",
    "       familyDf = data.loc[data.family==familyId]  \n",
    "\n",
    "       for storeId in [1]:#[41]: #data.store_nbr.unique():\n",
    "              print('store',storeId)\n",
    "              storeDf = familyDf.loc[(familyDf.store_nbr == storeId)] \n",
    "              storeDf = storeDf.loc[(storeDf.dataT == 'train')]# & (storeDf.date > \"2015-07-01\")]\n",
    "\n",
    "              mask = (storeDf.date >= date_string_val)\n",
    "              storeDf.loc[mask,['dataT']] = 'val'\n",
    "\n",
    "              # ln tranformation\n",
    "              storeDf.loc[:,['logSales']] = np.log(storeDf.sales + 1)\n",
    "\n",
    "              relevantSales = storeDf.loc[storeDf.dataT =='train']\n",
    "              dfLen = storeDf.shape[0]\n",
    "              seasonF = []\n",
    "              for period in [7, 14,21,28,52,365]:\n",
    "                     dec = sm.tsa.seasonal_decompose(relevantSales.logSales,period = period, model = 'additive')\n",
    "                     f = 'logSalesSeasonality_'+str(period)\n",
    "                     storeDf.loc[:,[f]] = addSeasonality(period, dec, dfLen)\n",
    "                     seasonF.append(f)\n",
    "\n",
    "              storeDf.loc[:,['ref']] = storeDf['logSales'].shift(initial_lag)\n",
    "              storeDf.loc[:,['target']] = storeDf['logSales'] - storeDf['ref']\n",
    "\n",
    "              seasonF2 = []\n",
    "              for f in seasonF:\n",
    "                     newF = f+'_diff'+str(initial_lag)\n",
    "                     storeDf.loc[:,[newF]] = storeDf[f].diff(initial_lag)\n",
    "                     seasonF2.append(newF)\n",
    "\n",
    "              # lag features / how many past datapoints are we tain\n",
    "              featuresForLag = ['target'] + seasonF + seasonF2\n",
    "              lagF = []#trainF\n",
    "              for i in range(lags):\n",
    "                     lag = i+initial_lag\n",
    "                     newF = [featuresForLag[j] + '_lag' + str(lag) for j in range(len(featuresForLag))]\n",
    "                     lagF = lagF + newF\n",
    "                     storeDf.loc[:,newF] = storeDf[featuresForLag].shift(lag).to_numpy()\n",
    "\n",
    "              # rolling features\n",
    "              rollingF = []\n",
    "              for rol in rolling:\n",
    "                     for i in range(len(lagF)):\n",
    "                            #if 'sales_t-16'  in lagF[i]:\n",
    "                            if 'target'  in lagF[i]:\n",
    "                                   fm = lagF[i]+'_rollingM' + str(rol)\n",
    "                                   fs = lagF[i]+'_rollingS' + str(rol)\n",
    "                                   rollingF.append(fm)\n",
    "                                   rollingF.append(fs)\n",
    "                                   storeDf.loc[:,[fm]] = storeDf[lagF[i]].rolling(rol).mean()#.copy()\n",
    "                                   storeDf.loc[:,[fs]] = storeDf[lagF[i]].rolling(rol).std()#.copy()\n",
    "\n",
    "\n",
    "              allF = lagF + rollingF + timeF + trainF+seasonF\n",
    "\n",
    "              # we get a matrix that predicts only 1 timestamp -> stride it\n",
    "              if len(rolling) == 0:\n",
    "                     storeDf = storeDf.iloc[lags+initial_lag+1:-1]\n",
    "              else:\n",
    "                     storeDf = storeDf.iloc[lags+initial_lag+max(rolling)+1:-1]\n",
    "\n",
    "              train_subDf = storeDf.loc[storeDf.date < date_string_test]\n",
    "              test_subDf  = storeDf.loc[storeDf.date >= date_string_test]\n",
    "              val_subDf   = storeDf.loc[storeDf.dataT =='val']\n",
    "\n",
    "\n",
    "targetF = 'target'\n",
    "baseTrain = train_subDf['ref'].to_numpy()\n",
    "baseTest  = test_subDf['ref'].to_numpy()\n",
    "baseVal   = val_subDf['ref'].to_numpy()\n",
    "\n",
    "X_train = train_subDf[allF].to_numpy()\n",
    "y_train = train_subDf[[targetF]].to_numpy()\n",
    "X_test  =  test_subDf[allF].to_numpy()\n",
    "y_test  =  test_subDf[[targetF]].to_numpy()  \n",
    "X_val   =  val_subDf[allF].to_numpy()\n",
    "y_val   =  val_subDf[[targetF]].to_numpy() \n",
    "\n",
    "np.isnan(X_train).any(),np.isnan(X_test).any(),np.isnan(X_val).any(),np.isnan(y_train).any(),np.isnan(y_test).any(),np.isnan(y_val).any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # compare against base lgbm that just predicts always t+16\n",
    "\n",
    "def calcLossLGBM(pred, y, logTransform, predictDiff, base):\n",
    "    if predictDiff:\n",
    "        pred = np.reshape(pred, (y.shape[0])) +  np.reshape(base, (y.shape[0]))\n",
    "        y = np.reshape(y, (y.shape[0])) + np.reshape(base, (y.shape[0]))\n",
    "\n",
    "    if logTransform:\n",
    "        a = np.exp(pred) -1\n",
    "        y = np.exp(y) -1 \n",
    "    else:\n",
    "        a = (pred)\n",
    "        y = (y)\n",
    "\n",
    "    if (a < 0).any():\n",
    "        a = np.clip(a, 0, 1e20)\n",
    "\n",
    "    rmsleTrain = np.sqrt(mean_squared_log_error(a,y))\n",
    "    return rmsleTrain\n",
    "def plotLGBM(i, logTransform, pred, y, predictDiff, base):\n",
    "    if predictDiff:\n",
    "        pred = np.reshape(pred, (y.shape[0])) +  np.reshape(base, (y.shape[0]))\n",
    "        y = np.reshape(y, (y.shape[0])) + np.reshape(base, (y.shape[0]))\n",
    "    if logTransform:\n",
    "        a = np.exp(pred[i:i+16])\n",
    "        y = np.exp(y[i:i+16])\n",
    "    else:\n",
    "        a = (pred[i:i+16])\n",
    "        y = (y[i:i+16])\n",
    "    x = range(len(a))   \n",
    "\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=1, figsize=(8, 6))\n",
    "    axs.plot(x, y, color='blue',label='Original')\n",
    "    axs.plot(x, a, color='red',label='pred')\n",
    "    axs.set_title('index: '+str(i))\n",
    "    fig.subplots_adjust(hspace=0.5)\n",
    "\n",
    "# Set parameters for LGBM model\n",
    "params = {\n",
    "    'boosting':'gbdt',#'gbdt', #'rf' #'dart'\n",
    "    'objective': 'regression',  # Assuming you're doing regression\n",
    "    'metric': 'mse',  # Mean squared error\n",
    "    'num_leaves': 15,\n",
    "    #'lambda_l1': 0.1,\n",
    "    #'lambda_l2': 0.2,\n",
    "    #'max_depth':10,\n",
    "    #'learning_rate': 0.1,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': -1,\n",
    "    'force_col_wise':True,\n",
    "    'num_iterations':100\n",
    "}   \n",
    "\n",
    "# Train the model\n",
    "gbm = lgb.train(params, lgb.Dataset(X_train, label=y_train), valid_sets=[\n",
    "    lgb.Dataset(X_test, label=y_test)\n",
    "    #,lgb.Dataset(X_val, label=y_val)\n",
    "    ]#,num_boost_round=100\n",
    ",callbacks=[lgb.early_stopping(stopping_rounds=100)]\n",
    ")  \n",
    "predtrainLGBM = gbm.predict(X_train)\n",
    "predtestLGBM = gbm.predict(X_test)\n",
    "predvalLGBM = gbm.predict(X_val)\n",
    "logTransform=True\n",
    "predictDiff=True\n",
    "#baseTrain, baseTest, baseVal = [],[],[]\n",
    "\n",
    "print('errors: ', calcLossLGBM(predtrainLGBM, y_train, logTransform, predictDiff, base=baseTrain), calcLossLGBM(predtestLGBM, y_test, logTransform, predictDiff, baseTest), calcLossLGBM(predvalLGBM, y_val, logTransform, predictDiff, baseVal))\n",
    "\n",
    "for i in range(0):\n",
    "    plotLGBM(i*16, logTransform, predtrainLGBM, y_train, predictDiff, baseTrain)\n",
    "    plotLGBM(i*16, logTransform, predtestLGBM, y_test, predictDiff, baseTest)\n",
    "    plotLGBM(i*16, logTransform, predvalLGBM, y_val, predictDiff, baseVal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = gbm.feature_importance()\n",
    "for name, importance in zip(allF, importances):\n",
    "    print(f'{name}: {importance}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# residual analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = train_subDf[['target','logSalesSeasonality_365']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN / log regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" use a fully connected NN  for sequence with 16\"\"\"\n",
    "from keras import backend as K\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1))\n",
    "\n",
    "tf.keras.utils.set_random_seed(42)\n",
    "\n",
    "n_features = X_train.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(n_features,input_shape=(n_features,)))\n",
    "model.add(Dense(64))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "model.fit(X_train, y_train, epochs=4000, batch_size=3200,validation_data=(X_test, y_test))\n",
    "\n",
    "predtrain = model.predict(X_train, verbose=False)\n",
    "predtest  = model.predict(X_test, verbose=False)\n",
    "logTransform=True\n",
    "predictDiff=True\n",
    "#baseTrain, baseTest, baseVal = [],[],[]\n",
    "\n",
    "print('errors: ', calcLossLGBM(predtrain, y_train, logTransform, predictDiff, baseTrain), calcLossLGBM(predtest, y_test, logTransform, predictDiff, baseTest))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predtrain = model.predict(X_train, verbose=False)\n",
    "predtest  = model.predict(X_test, verbose=False)\n",
    "print('errors: ', calcLossLGBM(predtrain, y_train, logTransform, predictDiff, baseTrain), calcLossLGBM(predtest, y_test, logTransform, predictDiff, baseTest))\n",
    "\n",
    "for i in range(1):\n",
    "    plotLGBM(i*16, logTransform, predtrainLGBM, y_train, predictDiff, baseTrain)\n",
    "    plotLGBM(i*16, logTransform, predtestLGBM, y_test, predictDiff, baseTest)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.13 ('storeSales')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "17f27e0cbed1f29822e509a94c958ed0e80b7d9abe162097755686a614de4732"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
