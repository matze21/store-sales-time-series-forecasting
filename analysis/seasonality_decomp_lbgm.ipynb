{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import plotly.offline as pyo\n",
    "from plotly import subplots\n",
    "import plotly.graph_objects as go\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.tsa.api as smt\n",
    "\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from tensorflow.keras.models import Sequential\n",
    "#from tensorflow.keras.layers import LSTM, Dense,Input,concatenate\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import lightgbm as lgb\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "from baseFunctions import *\n",
    "from data_helpers import processData6, featureEngineering, getSequencesFast, removeOutliers, create_sequences, dataProcessing, addLaggedFutureHolidays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, propDicts, flippedPropDicts = processData6(oneHotEnodeCatFeat=True)\n",
    "data, timeFeatures = featureEngineering(data,splits=[2,2,2,2],oneHotWeekday=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # helps to learn function, but doesn't help with overfitting\n",
    "data['linear_time'] = (data['linear_time'] - data.linear_time.mean()) /data.linear_time.std()\n",
    "data['day_of_year'] = (data['day_of_year'] - data.day_of_year.mean()) /data.day_of_year.std()\n",
    "data['dcoilwtico'] = (data['dcoilwtico'] - data.dcoilwtico.mean()) /data.dcoilwtico.std()\n",
    "data['transactions'] = (data['transactions'] - data.transactions.mean()) /data.transactions.std()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataProcessingLocal(storeDf, date_string_val, date_string_test, predictDiff, seasonalFDiff, seasonalLags, targetLags, rolling, initial_lag, prepPred = False):\n",
    "\n",
    "       \"\"\" create training data based on lagged features not 2 sequences \"\"\"\n",
    "       trainF = [\n",
    "              'store_nbr', 'family', \n",
    "              #'sales', \n",
    "              # 'dataT',\n",
    "              'city', 'state', 'type', 'cluster', \n",
    "              #'dcoilwtico','onpromotion',  # added down the line!\n",
    "              'holidayType',\n",
    "              'description', \n",
    "              'transferred', \n",
    "              #'transactions', \n",
    "              'store_closed',\n",
    "              'weekday_0', 'weekday_1', 'weekday_2',\n",
    "              'weekday_3', 'weekday_4', 'weekday_5', 'weekday_6',\n",
    "              #'type_0','type_1','type_2','type_3','type_4', #store type\n",
    "              'holidayType_0','holidayType_1','holidayType_2','holidayType_3','holidayType_4','holidayType_5','holidayType_6',\n",
    "              'description_0','description_1','description_2','description_3','description_4','description_5','description_6','description_7','description_8','description_9','description_10','description_11','description_12','description_13','description_14','description_15','description_16','description_17','description_18'\n",
    "              ]\n",
    "       timeF = [\n",
    "              'linear_time', 'day_of_year', 'day_of_year_f12_0', 'day_of_year_f104_0','day_of_year_f24_0',  'day_of_year_f52_0',\n",
    "              'day_of_year_f12_180', 'day_of_year_f104_180','day_of_year_f24_180','day_of_year_f52_180', \n",
    "              #'weekday', \n",
    "              'month'\n",
    "              ]\n",
    "       if not prepPred:\n",
    "              storeDf = storeDf.loc[(storeDf.dataT == 'train')]\n",
    "              mask = (storeDf.date >= date_string_val)\n",
    "              storeDf.loc[mask,['dataT']] = 'val'\n",
    "\n",
    "       # ln tranformation\n",
    "       storeDf.loc[:,['logSales']] = np.log(storeDf.sales + 1)\n",
    "       storeDf.loc[:,['transactions']] = storeDf['transactions'].fillna(0)\n",
    "       storeDf['onpromotion'] = (storeDf['onpromotion']- storeDf.onpromotion.mean()) /storeDf.onpromotion.std() # helps against overfitting  # \n",
    "\n",
    "       \n",
    "       relevantSales = storeDf.loc[storeDf.dataT =='train']\n",
    "       dfLen = storeDf.shape[0]\n",
    "\n",
    "\n",
    "       seasonF = []\n",
    "       for period in [7]: #, 14,21,28,52  104, 365\n",
    "              for pf in ['logSales','transactions','dcoilwtico']:\n",
    "                     dec = sm.tsa.seasonal_decompose(relevantSales[pf],period = period, model = 'additive')\n",
    "                     #print(period, max(dec.seasonal))\n",
    "                     f = pf+'Seasonality_'+str(period)\n",
    "                     storeDf.loc[:,[f]] = addSeasonality(period, dec, dfLen)\n",
    "                     seasonF.append(f)\n",
    "\n",
    "       seasonF.append('dcoilwtico')\n",
    "       seasonF.append('onpromotion')   # helps against overfitting\n",
    "       #print('done with seasonal decompose')\n",
    "\n",
    "       if predictDiff:\n",
    "              storeDf.loc[:,['ref']] = storeDf['logSales'].shift(initial_lag)\n",
    "              storeDf.loc[:,['target']] = storeDf['logSales'] - storeDf['ref']\n",
    "       else:\n",
    "              storeDf.loc[:,['target']] = storeDf['logSales']\n",
    "\n",
    "              \n",
    "       seasonDiffF2 = []\n",
    "       for f in seasonF:\n",
    "              newF = f+'_diff'+str(seasonalFDiff)\n",
    "              storeDf.loc[:,[newF]] = storeDf[f].diff(seasonalFDiff)\n",
    "              seasonDiffF2.append(newF)\n",
    "       \n",
    "       # seasonal featuers lags\n",
    "       seasonalFLags = seasonF + seasonDiffF2\n",
    "       featuresForSLag = []#trainF\n",
    "       for i in seasonalLags:\n",
    "              lag = i\n",
    "              newF = [seasonalFLags[j] + '_lag' + str(lag) for j in range(len(seasonalFLags))]\n",
    "              featuresForSLag = featuresForSLag + newF\n",
    "              storeDf.loc[:,newF] = storeDf[seasonalFLags].shift(lag).to_numpy()\n",
    "\n",
    "       seasonalFeatures = seasonF + seasonDiffF2 + featuresForSLag\n",
    "       #print('done with seasonal features')\n",
    "       \n",
    "\n",
    "       \n",
    "       # lag features / how many past datapoints are we tain\n",
    "       featuresForLag = ['target']\n",
    "       targetLagF = []#trainF\n",
    "       for i in targetLags:\n",
    "              lag = i+initial_lag\n",
    "              newF = [featuresForLag[j] + '_lag' + str(lag) for j in range(len(featuresForLag))]\n",
    "              targetLagF = targetLagF + newF\n",
    "              storeDf.loc[:,newF] = storeDf[featuresForLag].shift(lag).to_numpy()\n",
    "       #print('done with target lags')\n",
    "       \n",
    "       #------ also add future holidays!--------\n",
    "       storeDf, transferredF = addLaggedFutureHolidays(storeDf, features=['transferred','holidayType'], lags=4)\n",
    "\n",
    "       # rolling features\n",
    "       createdF = seasonF + targetLagF\n",
    "       rollingF = []\n",
    "       for rol in rolling:\n",
    "              for i in range(len(createdF)):\n",
    "                     #if 'sales_t-16'  in lagF[i]:\n",
    "                     #if createdF[i] in ['target','dcoilwtico']:# or 'dcoilwtico' in lagF[i]:#'target'  in lagF[i]:\n",
    "                            fm = createdF[i]+'_rollingM' + str(rol)\n",
    "                            fs = createdF[i]+'_rollingS' + str(rol)\n",
    "                            rollingF.append(fm)\n",
    "                            rollingF.append(fs)\n",
    "                            storeDf.loc[:,[fm]] = storeDf[createdF[i]].rolling(rol).mean()#.copy()\n",
    "                            storeDf.loc[:,[fs]] = storeDf[createdF[i]].rolling(rol).std()#.copy()\n",
    "              #print('done with rolling:', rol)\n",
    "       #print('done with rolling')\n",
    "\n",
    "\n",
    "       allF = rollingF + timeF + trainF +transferredF +seasonalFeatures + targetLagF\n",
    "       # we get a matrix that predicts only 1 timestamp -> stride it\n",
    "       if len(rolling) == 0:\n",
    "              storeDf = storeDf.iloc[max(max(targetLags), max(seasonalLags))+initial_lag+1:storeDf.shape[0]]\n",
    "       else:\n",
    "              storeDf = storeDf.iloc[max(max(targetLags), max(seasonalLags))+initial_lag+max(rolling)+1:storeDf.shape[0]]\n",
    "\n",
    "       \n",
    "       if prepPred:\n",
    "              train_subDf = storeDf.loc[(storeDf.date < date_string_test) & (storeDf.dataT == 'train')]\n",
    "              test_subDf  = storeDf.loc[(storeDf.date >= date_string_test)& (storeDf.dataT == 'train')]\n",
    "              pred_subDf = storeDf.loc[storeDf.dataT == 'test']\n",
    "       else:\n",
    "              train_subDf = storeDf.loc[storeDf.date < date_string_test]\n",
    "              test_subDf  = storeDf.loc[storeDf.date >= date_string_test]\n",
    "              pred_subDf = storeDf.loc[storeDf.dataT =='val']\n",
    "    \n",
    "       return train_subDf, test_subDf, pred_subDf, allF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: build more features of oil & transactions\n",
    "\n",
    "# Problem pairs \n",
    "# family / store\n",
    "# 12 / 44 -> on promotion has not enough influence!\n",
    "# 13 / 26 -> lawn and garden (13) is bad for multiple stores! -> quite different dynamic from train/test -> recently more promotion, lot more sales\n",
    "# 13 / 21 -> better, store closed for a while\n",
    "# 12 / 37 -> different behavior in past / future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" create training data based on lagged features not 2 sequences \"\"\"\n",
    "\n",
    "initial_lag = 16 # 16 = independent from previous predictions\n",
    "seasonalFDiff = 1\n",
    "seasonalLags=[1,2,3,4,5,6,7,14,21] #[1,2,3,4,5,6,7,14,21,52,104]#range(21)\n",
    "targetLags = [0,1,2,3,4,5,6,7,14]#[0,1,2,3,4,5,6,7,14,21,52,10]#range(21)# [7,14,21,28,35,42,52] #range(52)#[7,14,21]\n",
    "rolling = [7,21] #,21,28,35,42,14,52 ,104\n",
    "predictDiff =False\n",
    "logTransform=True\n",
    "\n",
    "# Date string\n",
    "date_string_val = \"2017-07-01\"#\"2017-05-01\"\n",
    "date_string_test = \"2017-01-01\"\n",
    "date_string_val = \"2017-07-15\"#\"2017-05-01\"\n",
    "date_string_test = \"2016-07-31\"\n",
    "\n",
    "list_tr, list_te, list_val = [],[],[]\n",
    "for familyId in [4]:#[6]: #data.family.unique():\n",
    "       # start with only some families\n",
    "       print(familyId)\n",
    "       familyDf = data.loc[data.family==familyId]  \n",
    "\n",
    "       for storeId in [40,41,42,43]:#[41]: #data.store_nbr.unique():  # when using more data we find more overfitting & more data variiety\n",
    "              print('store',storeId)\n",
    "              storeDf = familyDf.loc[(familyDf.store_nbr == storeId)]# & (familyDf.date > \"2015-07-01\")]\n",
    "\n",
    "              train_subDf, test_subDf, val_subDf, allF = dataProcessingLocal(storeDf, date_string_val, date_string_test, predictDiff, seasonalFDiff, seasonalLags, targetLags, rolling, initial_lag, True)\n",
    "              list_tr.append(train_subDf)\n",
    "              list_te.append(test_subDf)\n",
    "              list_val.append(val_subDf)\n",
    "\n",
    "train_subDf = pd.concat(list_tr)\n",
    "test_subDf = pd.concat(list_te)\n",
    "val_subDf = pd.concat(list_val)\n",
    "\n",
    "targetF = 'target'\n",
    "if predictDiff:\n",
    "       baseTrain = train_subDf['ref'].to_numpy()\n",
    "       baseTest  = test_subDf['ref'].to_numpy()\n",
    "       baseVal   = val_subDf['ref'].to_numpy()\n",
    "else:\n",
    "       baseTrain, baseTest, baseVal = [],[],[]\n",
    "\n",
    "X_train = train_subDf[allF]#.to_numpy()\n",
    "y_train = train_subDf[targetF].to_numpy()\n",
    "X_test  =  test_subDf[allF]#.to_numpy()\n",
    "y_test  =  test_subDf[targetF].to_numpy()  \n",
    "X_val   =  val_subDf[allF]#.to_numpy()\n",
    "y_val   =  val_subDf[targetF].to_numpy() \n",
    "\n",
    "#np.isnan(X_train).any(),np.isnan(X_test).any(),np.isnan(X_val).any(),np.isnan(y_train).any(),np.isnan(y_test).any(),np.isnan(y_val).any()\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subDf[['sales','onpromotion','store_closed','date']].set_index('date').plot(), test_subDf[['sales','onpromotion','store_closed','date']].set_index('date').plot(),val_subDf[['sales','onpromotion','store_closed','date']].set_index('date').plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # compare against base lgbm that just predicts always t+16\n",
    "\n",
    "def calcLossLGBM(pred, y, logTransform, predictDiff, base):\n",
    "    if predictDiff:\n",
    "        pred = np.reshape(pred, (y.shape[0])) +  np.reshape(base, (y.shape[0]))\n",
    "        y = np.reshape(y, (y.shape[0])) + np.reshape(base, (y.shape[0]))\n",
    "\n",
    "    if logTransform:\n",
    "        a = np.exp(pred) -1\n",
    "        y = np.exp(y) -1 \n",
    "    else:\n",
    "        a = (pred)\n",
    "        y = (y)\n",
    "\n",
    "    if (a < 0).any():\n",
    "        a = np.clip(a, 0, 1e20)\n",
    "\n",
    "    rmsleTrain = np.sqrt(mean_squared_log_error(a,y))\n",
    "    return rmsleTrain\n",
    "def plotLGBM(i, logTransform, pred, y, predictDiff, base):\n",
    "    if predictDiff:\n",
    "        pred = np.reshape(pred, (y.shape[0])) +  np.reshape(base, (y.shape[0]))\n",
    "        y = np.reshape(y, (y.shape[0])) + np.reshape(base, (y.shape[0]))\n",
    "    if logTransform:\n",
    "        a = np.exp(pred[i:i+16])\n",
    "        y = np.exp(y[i:i+16])\n",
    "    else:\n",
    "        a = (pred[i:i+16])\n",
    "        y = (y[i:i+16])\n",
    "    x = range(len(a))   \n",
    "\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=1, figsize=(8, 6))\n",
    "    axs.plot(x, y, color='blue',label='Original')\n",
    "    axs.plot(x, a, color='red',label='pred')\n",
    "    axs.set_title('index: '+str(i))\n",
    "    fig.subplots_adjust(hspace=0.5)\n",
    "\n",
    "# Set parameters for LGBM model\n",
    "params = {\n",
    "    'boosting':'gbdt',#'gbdt', #'rf' #'dart'\n",
    "    'objective': 'regression',  # Assuming you're doing regression\n",
    "    'metric': 'mse',  # Mean squared error\n",
    "    'num_leaves': 5,\n",
    "    'lambda_l1': 0.1,\n",
    "    'lambda_l2': 0.2,\n",
    "    #'max_depth':10,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': -1,\n",
    "    'force_col_wise':True,\n",
    "    'num_iterations':500\n",
    "}   \n",
    "\n",
    "sample_weights = np.reshape(\n",
    "    (train_subDf.onpromotion * train_subDf.onpromotion.std()) + train_subDf.onpromotion.mean()\n",
    "    + 1 \n",
    "    #+ train_subDf.linear_time + train_subDf.linear_time.min()\n",
    "    , (-1,1)) # öffset of 1 to not have 0 weight\n",
    "sample_weights = sample_weights/ max(sample_weights)\n",
    "\n",
    "f =rollingF+timeF+trainF+lagsAndDiffs\n",
    "\n",
    "X_train = train_subDf[f]\n",
    "X_test  =  test_subDf[f]\n",
    "X_val   =  val_subDf[f]\n",
    "\n",
    "# Train the model\n",
    "gbm = lgb.train(params, lgb.Dataset(X_train, label=y_train\n",
    "    , weight=sample_weights\n",
    "    ), valid_sets=[\n",
    "    lgb.Dataset(X_test, label=y_test)\n",
    "    #,lgb.Dataset(X_val, label=y_val)\n",
    "    ]#,num_boost_round=100\n",
    "#,callbacks=[lgb.early_stopping(stopping_rounds=100)]\n",
    "\n",
    ")  \n",
    "predtrainLGBM = gbm.predict(X_train)\n",
    "predtestLGBM = gbm.predict(X_test)\n",
    "predvalLGBM = gbm.predict(X_val)\n",
    "\n",
    "print('errors: ', calcLossLGBM(predtrainLGBM, y_train, logTransform, predictDiff, base=baseTrain), calcLossLGBM(predtestLGBM, y_test, logTransform, predictDiff, baseTest))\n",
    "#, calcLossLGBM(predvalLGBM, y_val, logTransform, predictDiff, baseVal))\n",
    "\n",
    "for i in range(1):\n",
    "    plotLGBM(i*16, logTransform, predtrainLGBM, y_train, predictDiff, baseTrain)\n",
    "    plotLGBM(i*16, logTransform, predtestLGBM, y_test, predictDiff, baseTest)\n",
    "    plotLGBM(i*16, logTransform, predvalLGBM, y_val, predictDiff, baseVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# featuer study with lgbm\n",
    "\n",
    "rollingF = ['logSalesSeasonality_7_rollingM7',\n",
    " 'logSalesSeasonality_7_rollingS7',\n",
    " 'transactionsSeasonality_7_rollingM7',\n",
    " 'transactionsSeasonality_7_rollingS7',\n",
    " 'dcoilwticoSeasonality_7_rollingM7',\n",
    " 'dcoilwticoSeasonality_7_rollingS7',\n",
    " 'logSalesSeasonality_7_rollingM21',\n",
    " 'logSalesSeasonality_7_rollingS21',\n",
    " 'transactionsSeasonality_7_rollingM21',\n",
    " 'transactionsSeasonality_7_rollingS21',\n",
    " 'dcoilwticoSeasonality_7_rollingM21',\n",
    " 'dcoilwticoSeasonality_7_rollingS21',\n",
    "\n",
    " 'dcoilwtico_rollingM7',\n",
    " 'dcoilwtico_rollingS7',\n",
    " 'onpromotion_rollingM7',\n",
    " 'onpromotion_rollingS7',\n",
    " 'target_lag16_rollingM7',\n",
    " 'target_lag16_rollingS7',\n",
    " 'target_lag17_rollingM7',\n",
    " 'target_lag17_rollingS7',\n",
    " 'target_lag18_rollingM7',\n",
    " 'target_lag18_rollingS7',\n",
    " 'target_lag19_rollingM7',\n",
    " 'target_lag19_rollingS7',\n",
    " 'target_lag20_rollingM7',\n",
    " 'target_lag20_rollingS7',\n",
    " 'target_lag21_rollingM7',\n",
    " 'target_lag21_rollingS7',\n",
    " 'target_lag22_rollingM7',\n",
    " 'target_lag22_rollingS7',\n",
    " 'target_lag23_rollingM7',\n",
    " 'target_lag23_rollingS7',\n",
    " 'target_lag30_rollingM7',\n",
    " 'target_lag30_rollingS7',\n",
    "\n",
    " 'dcoilwtico_rollingM21',\n",
    " 'dcoilwtico_rollingS21',\n",
    " 'onpromotion_rollingM21',\n",
    " 'onpromotion_rollingS21',\n",
    " 'target_lag16_rollingM21',\n",
    " 'target_lag16_rollingS21',\n",
    " 'target_lag17_rollingM21',\n",
    " 'target_lag17_rollingS21',\n",
    " 'target_lag18_rollingM21',\n",
    " 'target_lag18_rollingS21',\n",
    " 'target_lag19_rollingM21',\n",
    " 'target_lag19_rollingS21',\n",
    " 'target_lag20_rollingM21',\n",
    " 'target_lag20_rollingS21',\n",
    " 'target_lag21_rollingM21',\n",
    " 'target_lag21_rollingS21',\n",
    " 'target_lag22_rollingM21',\n",
    " 'target_lag22_rollingS21',\n",
    " 'target_lag23_rollingM21',\n",
    " 'target_lag23_rollingS21',\n",
    " 'target_lag30_rollingM21',\n",
    " 'target_lag30_rollingS21']\n",
    "timeF=[\n",
    " 'linear_time',\n",
    " 'day_of_year',\n",
    " 'day_of_year_f12_0',\n",
    " 'day_of_year_f104_0',\n",
    " 'day_of_year_f24_0',\n",
    " 'day_of_year_f52_0',\n",
    " 'day_of_year_f12_180',\n",
    " 'day_of_year_f104_180',\n",
    " 'day_of_year_f24_180',\n",
    " 'day_of_year_f52_180',\n",
    " 'month']\n",
    "trainF=[\n",
    " 'dcoilwtico',\n",
    " 'onpromotion',\n",
    " 'holidayType',\n",
    " 'description',\n",
    " 'transferred',\n",
    " 'store_closed',\n",
    " 'weekday_0',\n",
    " 'weekday_1',\n",
    " 'weekday_2',\n",
    " 'weekday_3',\n",
    " 'weekday_4',\n",
    " 'weekday_5',\n",
    " 'weekday_6',\n",
    " 'holidayType_0',\n",
    " 'holidayType_1',\n",
    " 'holidayType_2',\n",
    " 'holidayType_3',\n",
    " 'holidayType_4',\n",
    " 'holidayType_5',\n",
    " 'holidayType_6',\n",
    " 'description_0',\n",
    " 'description_1',\n",
    " 'description_2',\n",
    " 'description_3',\n",
    " 'description_4',\n",
    " 'description_5',\n",
    " 'description_6',\n",
    " 'description_7',\n",
    " 'description_8',\n",
    " 'description_9',\n",
    " 'description_10',\n",
    " 'description_11',\n",
    " 'description_12',\n",
    " 'description_13',\n",
    " 'description_14',\n",
    " 'description_15',\n",
    " 'description_16',\n",
    " 'description_17',\n",
    " 'description_18']\n",
    "targetLags=[\n",
    "  'target_lag16',\n",
    " 'target_lag17',\n",
    " 'target_lag18',\n",
    " 'target_lag19',\n",
    " 'target_lag20',\n",
    " 'target_lag21',\n",
    " 'target_lag22',\n",
    " 'target_lag23',\n",
    " 'target_lag30']\n",
    "transferredF = [\n",
    " 'transferred_lag1',\n",
    " 'transferred_lag-1',\n",
    " 'holidayType_lag1',\n",
    " 'holidayType_lag-1',\n",
    " 'transferred_lag2',\n",
    " 'transferred_lag-2',\n",
    " 'holidayType_lag2',\n",
    " 'holidayType_lag-2',\n",
    " 'transferred_lag3',\n",
    " 'transferred_lag-3',\n",
    " 'holidayType_lag3',\n",
    " 'holidayType_lag-3',\n",
    " 'transferred_lag4',\n",
    " 'transferred_lag-4',\n",
    " 'holidayType_lag4',\n",
    " 'holidayType_lag-4',\n",
    "#  'transferred_lag5',\n",
    "#  'transferred_lag-5',\n",
    "#  'holidayType_lag5',\n",
    "#  'holidayType_lag-5',\n",
    "#  'transferred_lag6',\n",
    "#  'transferred_lag-6',\n",
    "#  'holidayType_lag6',\n",
    "#  'holidayType_lag-6',\n",
    "#  'transferred_lag7',\n",
    "#  'transferred_lag-7',\n",
    "#  'holidayType_lag7',\n",
    "#  'holidayType_lag-7',\n",
    "#  'transferred_lag8',\n",
    "#  'transferred_lag-8',\n",
    "#  'holidayType_lag8',\n",
    "#  'holidayType_lag-8',\n",
    "#  'transferred_lag9',\n",
    "#  'transferred_lag-9',\n",
    "#  'holidayType_lag9',\n",
    "#  'holidayType_lag-9',\n",
    "#  'transferred_lag10',\n",
    "#  'transferred_lag-10',\n",
    "#  'holidayType_lag10',\n",
    "#  'holidayType_lag-10',\n",
    " ]\n",
    "\n",
    "lagsAndDiffs = [\n",
    " 'logSalesSeasonality_7',\n",
    " 'transactionsSeasonality_7',\n",
    " 'dcoilwticoSeasonality_7',\n",
    " \n",
    " 'logSalesSeasonality_7_diff1',\n",
    " 'transactionsSeasonality_7_diff1',\n",
    " 'dcoilwticoSeasonality_7_diff1',\n",
    " 'dcoilwtico_diff1',\n",
    " 'onpromotion_diff1',\n",
    " 'logSalesSeasonality_7_lag1',\n",
    " 'transactionsSeasonality_7_lag1',\n",
    " 'dcoilwticoSeasonality_7_lag1',\n",
    " 'dcoilwtico_lag1',\n",
    " 'onpromotion_lag1',\n",
    " 'logSalesSeasonality_7_diff1_lag1',\n",
    " 'transactionsSeasonality_7_diff1_lag1',\n",
    " 'dcoilwticoSeasonality_7_diff1_lag1',\n",
    " 'dcoilwtico_diff1_lag1',\n",
    " 'onpromotion_diff1_lag1',\n",
    " 'logSalesSeasonality_7_lag2',\n",
    " 'transactionsSeasonality_7_lag2',\n",
    " 'dcoilwticoSeasonality_7_lag2',\n",
    " 'dcoilwtico_lag2',\n",
    " 'onpromotion_lag2',\n",
    " 'logSalesSeasonality_7_diff1_lag2',\n",
    " 'transactionsSeasonality_7_diff1_lag2',\n",
    " 'dcoilwticoSeasonality_7_diff1_lag2',\n",
    " 'dcoilwtico_diff1_lag2',\n",
    " 'onpromotion_diff1_lag2',\n",
    " 'logSalesSeasonality_7_lag3',\n",
    " 'transactionsSeasonality_7_lag3',\n",
    " 'dcoilwticoSeasonality_7_lag3',\n",
    " 'dcoilwtico_lag3',\n",
    " 'onpromotion_lag3',\n",
    " 'logSalesSeasonality_7_diff1_lag3',\n",
    " 'transactionsSeasonality_7_diff1_lag3',\n",
    " 'dcoilwticoSeasonality_7_diff1_lag3',\n",
    " 'dcoilwtico_diff1_lag3',\n",
    " 'onpromotion_diff1_lag3',\n",
    " 'logSalesSeasonality_7_lag4',\n",
    " 'transactionsSeasonality_7_lag4',\n",
    " 'dcoilwticoSeasonality_7_lag4',\n",
    " 'dcoilwtico_lag4',\n",
    " 'onpromotion_lag4',\n",
    " 'logSalesSeasonality_7_diff1_lag4',\n",
    " 'transactionsSeasonality_7_diff1_lag4',\n",
    " 'dcoilwticoSeasonality_7_diff1_lag4',\n",
    " 'dcoilwtico_diff1_lag4',\n",
    " 'onpromotion_diff1_lag4',\n",
    " 'logSalesSeasonality_7_lag5',\n",
    " 'transactionsSeasonality_7_lag5',\n",
    " 'dcoilwticoSeasonality_7_lag5',\n",
    " 'dcoilwtico_lag5',\n",
    " 'onpromotion_lag5',\n",
    " 'logSalesSeasonality_7_diff1_lag5',\n",
    " 'transactionsSeasonality_7_diff1_lag5',\n",
    " 'dcoilwticoSeasonality_7_diff1_lag5',\n",
    " 'dcoilwtico_diff1_lag5',\n",
    " 'onpromotion_diff1_lag5',\n",
    " 'logSalesSeasonality_7_lag6',\n",
    " 'transactionsSeasonality_7_lag6',\n",
    " 'dcoilwticoSeasonality_7_lag6',\n",
    " 'dcoilwtico_lag6',\n",
    " 'onpromotion_lag6',\n",
    " 'logSalesSeasonality_7_diff1_lag6',\n",
    " 'transactionsSeasonality_7_diff1_lag6',\n",
    " 'dcoilwticoSeasonality_7_diff1_lag6',\n",
    " 'dcoilwtico_diff1_lag6',\n",
    " 'onpromotion_diff1_lag6',\n",
    " 'logSalesSeasonality_7_lag7',\n",
    " 'transactionsSeasonality_7_lag7',\n",
    " 'dcoilwticoSeasonality_7_lag7',\n",
    " 'dcoilwtico_lag7',\n",
    " 'onpromotion_lag7',\n",
    " 'logSalesSeasonality_7_diff1_lag7',\n",
    " 'transactionsSeasonality_7_diff1_lag7',\n",
    " 'dcoilwticoSeasonality_7_diff1_lag7',\n",
    " 'dcoilwtico_diff1_lag7',\n",
    " 'onpromotion_diff1_lag7',\n",
    " 'logSalesSeasonality_7_lag14',\n",
    " 'transactionsSeasonality_7_lag14',\n",
    " 'dcoilwticoSeasonality_7_lag14',\n",
    " 'dcoilwtico_lag14',\n",
    " 'onpromotion_lag14',\n",
    " 'logSalesSeasonality_7_diff1_lag14',\n",
    " 'transactionsSeasonality_7_diff1_lag14',\n",
    " 'dcoilwticoSeasonality_7_diff1_lag14',\n",
    " 'dcoilwtico_diff1_lag14',\n",
    " 'onpromotion_diff1_lag14',\n",
    " 'logSalesSeasonality_7_lag21',\n",
    " 'transactionsSeasonality_7_lag21',\n",
    " 'dcoilwticoSeasonality_7_lag21',\n",
    " 'dcoilwtico_lag21',\n",
    " 'onpromotion_lag21',\n",
    " 'logSalesSeasonality_7_diff1_lag21',\n",
    " 'transactionsSeasonality_7_diff1_lag21',\n",
    " 'dcoilwticoSeasonality_7_diff1_lag21',\n",
    " 'dcoilwtico_diff1_lag21',\n",
    " 'onpromotion_diff1_lag21',\n",
    "]\n",
    "feats = [rollingF,timeF,trainF,targetLags,transferredF,lagsAndDiffs]\n",
    "\n",
    "# Set parameters for LGBM model\n",
    "params = {\n",
    "    'boosting':'gbdt',#'gbdt', #'rf' #'dart'\n",
    "    'objective': 'regression',  # Assuming you're doing regression\n",
    "    'metric': 'mse',  # Mean squared error\n",
    "    'num_leaves': 5,\n",
    "    'lambda_l1': 0.1,\n",
    "    'lambda_l2': 0.2,\n",
    "    #'max_depth':10,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': -1,\n",
    "    'force_col_wise':True\n",
    "}   \n",
    "\n",
    "for f in feats: \n",
    "  X_train = train_subDf[f]\n",
    "  X_test  =  test_subDf[f]\n",
    "  X_val   =  val_subDf[f]\n",
    "\n",
    "  # Train the model\n",
    "  gbm = lgb.train(params, lgb.Dataset(X_train, label=y_train), 500, valid_sets=[lgb.Dataset(X_test, label=y_test)]\n",
    "  ,callbacks=[lgb.early_stopping(stopping_rounds=500)]\n",
    "  )\n",
    "  predtrainLGBM = gbm.predict(X_train)\n",
    "  predtestLGBM = gbm.predict(X_test)\n",
    "  print('errors: ', calcLossLGBM(predtrainLGBM, y_train, logTransform, predictDiff, base=baseTrain), calcLossLGBM(predtestLGBM, y_test, logTransform, predictDiff, baseTest))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsplits = 5\n",
    "num_iter = 500\n",
    "\n",
    "params = {\n",
    "    'boosting':'gbdt',#'gbdt', #'rf' #'dart'\n",
    "    'objective': 'regression',  # Assuming you're doing regression\n",
    "    'metric': 'mse',  # Mean squared error\n",
    "    'num_leaves': 5,\n",
    "    'lambda_l1': 0.1,\n",
    "    'lambda_l2': 0.2,\n",
    "    #'max_depth':10,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': -1,\n",
    "    'force_col_wise':True,\n",
    "    'num_iterations':500\n",
    "}   \n",
    "\n",
    "sample_weights = np.reshape(\n",
    "    (train_subDf.onpromotion * train_subDf.onpromotion.std()) + train_subDf.onpromotion.mean()+ 1 \n",
    "    #+ train_subDf.linear_time + train_subDf.linear_time.min() #makes it a lot worse\n",
    "    , (-1,1)) # öffset of 1 to not have 0 weight\n",
    "sample_weights = sample_weights/ max(sample_weights)\n",
    "\n",
    "kf = KFold(n_splits=nsplits, shuffle=False)# , random_state=42) #random doesn't help\n",
    "splits = kf.split(X_train,y_train)\n",
    "\n",
    "cv_results = lgb.cv(\n",
    "    params,\n",
    "    lgb.Dataset(X_train, label=y_train\n",
    "    #, weight=sample_weights\n",
    "    ),\n",
    "    num_boost_round=num_iter,\n",
    "    folds=splits,\n",
    "    stratified=False,  # Set to True for stratified sampling in classification\n",
    "    #early_stopping_rounds=50,  # Stop if score doesn't improve for 50 rounds\n",
    "    metrics=['mse'],  # Evaluation metrics to track\n",
    "    seed=42,  # Set a seed for reproducibility\n",
    "    return_cvbooster=True\n",
    "    ,callbacks=[lgb.early_stopping(stopping_rounds=400)]\n",
    ")\n",
    "\n",
    "lenIter = len(cv_results['valid l2-mean'])\n",
    "print(cv_results['valid l2-mean'][lenIter-1], cv_results['valid l2-stdv'][lenIter-1])\n",
    "for i in range(nsplits):\n",
    "    predtrainLGBM = cv_results['cvbooster'].boosters[i].predict(X_train)\n",
    "    predtestLGBM = cv_results['cvbooster'].boosters[i].predict(X_test)\n",
    "\n",
    "    print('errors: ', round(calcLossLGBM(predtrainLGBM, y_train, logTransform, predictDiff, base=baseTrain),3), round(calcLossLGBM(predtestLGBM, y_test, logTransform, predictDiff, baseTest),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_df = (\n",
    "    pd.DataFrame({\n",
    "        'feature_name': gbm.feature_name(),\n",
    "        'importance_gain': gbm.feature_importance(importance_type='gain'),\n",
    "        'importance_split': gbm.feature_importance(importance_type='split'),\n",
    "    })\n",
    "    .sort_values('importance_gain', ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('Display.max_rows', None)\n",
    "importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict everything with LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask.distributed import Client, progress,LocalCluster\n",
    "from dask import delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@delayed\n",
    "def fitLGBM(storeId, family, date_string_val, date_string_test, predictDiff, seasonalFDiff, seasonalLags, targetLags, rolling, initial_lag):\n",
    "       train_subDf, test_subDf, pred_subDf, allF = dataProcessing(storeId, family, date_string_val, date_string_test, predictDiff, seasonalFDiff, seasonalLags, targetLags, rolling, initial_lag, prepPred = True)\n",
    "       \n",
    "       targetF = 'target'\n",
    "       if predictDiff:\n",
    "              baseTrain = train_subDf['ref'].to_numpy()\n",
    "              baseTest  = test_subDf['ref'].to_numpy()\n",
    "              baseVal   = pred_subDf['ref'].to_numpy()\n",
    "       else:\n",
    "              baseTrain, baseTest, baseVal = [],[],[]\n",
    "\n",
    "       X_train = train_subDf[allF].to_numpy()\n",
    "       y_train = train_subDf[targetF].to_numpy()\n",
    "       X_test  =  test_subDf[allF].to_numpy()\n",
    "       y_test  =  test_subDf[targetF].to_numpy()  \n",
    "       X_pred   =  pred_subDf[allF].to_numpy()\n",
    "\n",
    "       params = {\n",
    "           'boosting':'gbdt',#'gbdt', #'rf' #'dart'\n",
    "           'objective': 'regression',  # Assuming you're doing regression\n",
    "           'metric': 'mse',  # Mean squared error\n",
    "           'num_leaves': 5,\n",
    "           'feature_fraction': 0.9,\n",
    "           'bagging_fraction': 0.8,\n",
    "           'bagging_freq': 5,\n",
    "           'verbose': -1,\n",
    "           'force_col_wise':True,\n",
    "           #'num_iterations':500\n",
    "       }   \n",
    "\n",
    "       # Train the model\n",
    "       gbm = lgb.train(params, lgb.Dataset(X_train, label=y_train),500, valid_sets=[lgb.Dataset(X_test, label=y_test)],callbacks=[lgb.early_stopping(stopping_rounds=100)])\n",
    "       predtrainLGBM = gbm.predict(X_train)\n",
    "       predtestLGBM = gbm.predict(X_test)\n",
    "       predLGBM = gbm.predict(X_pred)\n",
    "\n",
    "       trainLoss = calcLossLGBM(predtrainLGBM, y_train, logTransform, predictDiff, base=baseTrain)\n",
    "       testLoss  = calcLossLGBM(predtestLGBM, y_test, logTransform, predictDiff, baseTest)\n",
    "       storeId = storeDf.store_nbr.unique()[0]\n",
    "       family = storeDf.family.unique()[0]\n",
    "       print('store',storeId,'family',family,'errors: ', trainLoss, testLoss,gbm.best_iteration)\n",
    "\n",
    "       subDf = pred_subDf[['id','sales']]\n",
    "       subDf.loc[:,['sales']] = np.exp(predLGBM)-1\n",
    "       subDf.loc[:,['trainE']] = trainLoss\n",
    "       subDf.loc[:,['testE']] = testLoss\n",
    "       subDf.loc[:,['bestIt']] = gbm.best_iteration\n",
    "\n",
    "       return subDf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" create training data based on lagged features not 2 sequences \"\"\"\n",
    "trainF = [\n",
    "       #'store_nbr', 'family', \n",
    "       #'sales', \n",
    "       'onpromotion',# 'dataT',\n",
    "       #'city', 'state', 'type', 'cluster', \n",
    "       'dcoilwtico', \n",
    "       'holidayType',\n",
    "       'description', \n",
    "       'transferred', \n",
    "       'transactions', \n",
    "       'store_closed',\n",
    "       'weekday_0', 'weekday_1', 'weekday_2',\n",
    "       'weekday_3', 'weekday_4', 'weekday_5', 'weekday_6',\n",
    "       #'type_0','type_1','type_2','type_3','type_4', #store type\n",
    "       'holidayType_0','holidayType_1','holidayType_2','holidayType_3','holidayType_4','holidayType_5','holidayType_6',\n",
    "       'description_0','description_1','description_2','description_3','description_4','description_5','description_6','description_7','description_8','description_9','description_10','description_11','description_12','description_13','description_14','description_15','description_16','description_17','description_18'\n",
    "\n",
    "       ]\n",
    "timeF = [\n",
    "       'linear_time', 'day_of_year', 'day_of_year_f12_0', 'day_of_year_f104_0','day_of_year_f24_0',  'day_of_year_f52_0',\n",
    "       'day_of_year_f12_180', 'day_of_year_f104_180','day_of_year_f24_180','day_of_year_f52_180', \n",
    "       #'weekday', \n",
    "       'month'\n",
    "       ]\n",
    "\n",
    "initial_lag = 16 # 16 = independent from previous predictions\n",
    "seasonalFDiff = 1\n",
    "seasonalLags=[1,2,3,4,5,6,7,14,21,52,104]#range(21)\n",
    "targetLags = [0,1,2,3,4,5,6,7,14,21,52,10]#range(21)# [7,14,21,28,35,42,52] #range(52)#[7,14,21]\n",
    "rolling = [7,14,21,28,35,42,52,104]\n",
    "predictDiff =False\n",
    "logTransform=True\n",
    "\n",
    "# Date string\n",
    "date_string_val = \"2017-07-15\"#\"2017-05-01\"\n",
    "date_string_test = \"2017-07-15\"\n",
    "\n",
    "cluster = LocalCluster(n_workers=4)\n",
    "client = Client(cluster)\n",
    "#cluster.scale(threads=1, memory=\"4GB\", GPUs=-1)\n",
    "\n",
    "results_list = []\n",
    "for familyId in data.family.unique():\n",
    "       # start with only some families\n",
    "       print('processing',familyId)\n",
    "       familyDf = data.loc[data.family==familyId]  \n",
    "\n",
    "       for storeId in data.store_nbr.unique():\n",
    "              #storeDf = familyDf.loc[(familyDf.store_nbr == storeId)] \n",
    "\n",
    "              result = dask.delayed(fitLGBM)(storeId, familyId, date_string_val, date_string_test, predictDiff, seasonalFDiff, seasonalLags, targetLags, rolling, initial_lag)\n",
    "              results_list.append(result)\n",
    "df_list = dask.compute(*results_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check multiprocess output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionsParallel = pd.read_csv(\"multiprocess_sarima313_1117.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionsParallel.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionsParallel['testE'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionsParallel.set_index('id')['sales'].to_csv('sub_multiprocess_sarima.csv') # = 0.52 (vs.  0.41 sarima)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: find difference to sarima & investigate big errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# correlation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_subDf[corrF].isna().sum()).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrF = allF + ['target']\n",
    "print(len(corrF))\n",
    "corr = train_subDf[corrF].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr['target_lag16']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = corr['target'].dropna().sort_values()\n",
    "a[0:60], a[a.shape[0]-50:a.shape[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[a.shape[0]-150:a.shape[0]-100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN / log regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" use a fully connected NN  for sequence with 16\"\"\"\n",
    "from keras import backend as K\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1))\n",
    "\n",
    "tf.keras.utils.set_random_seed(42)\n",
    "\n",
    "n_features = X_train.shape[1]\n",
    "\n",
    "#model = Sequential()\n",
    "#model.add(Dense(1,input_shape=(n_features,), activation='linear'))\n",
    "#model.add(Dense(64))\n",
    "#model.add(Dense(1))\n",
    "input_layer = Input(shape=(n_features,))\n",
    "x = BatchNormalization()(input_layer) # helps to learn function, but doesn't help with overfitting\n",
    "# Define the layers and connect them\n",
    "#idden_layer = Dense(16, activation='relu')(input_layer)\n",
    "\n",
    "output_layer = Dense(1, activation='linear')(x)\n",
    "\n",
    "# Define the model\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=100)\n",
    "model.fit(X_train, y_train, epochs=2000, batch_size=6400,validation_data=(X_test, y_test), callbacks=[early_stopping])\n",
    "\n",
    "predtrain = model.predict(X_train, verbose=False)\n",
    "predtest  = model.predict(X_test, verbose=False)\n",
    "predvalLGBM = model.predict(X_val, verbose=False)\n",
    "#baseTrain, baseTest, baseVal = [],[],[]\n",
    "\n",
    "print('errors: ', calcLossLGBM(predtrain, y_train, logTransform, predictDiff, baseTrain), calcLossLGBM(predtest, y_test, logTransform, predictDiff, baseTest))#, calcLossLGBM(predvalLGBM, y_val, logTransform, predictDiff, baseVal))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rrors:  0.5459896492508169 0.6072244010093094 0.5857688678529015     0.3687   lags 7,14,21\n",
    "#errors:  0.5412146789623853 0.6114486464169528 0.5711310694066851    0.3739\n",
    "a = {}\n",
    "for i,weight in enumerate(model.layers[2].get_weights()[0]):\n",
    "    a[allF[i]] = weight\n",
    "b = pd.DataFrame(a)\n",
    "c = b.iloc[0].sort_values()\n",
    "c[0:30], c[b.shape[1]-30:b.shape[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predtrain = model.predict(X_train, verbose=False)\n",
    "predtest  = model.predict(X_test, verbose=False)\n",
    "print('errors: ', calcLossLGBM(predtrain, y_train, logTransform, predictDiff, baseTrain), calcLossLGBM(predtest, y_test, logTransform, predictDiff, baseTest))\n",
    "\n",
    "for i in range(1):\n",
    "    plotLGBM(i*16, logTransform, predtrainLGBM, y_train, predictDiff, baseTrain)\n",
    "    plotLGBM(i*16, logTransform, predtestLGBM, y_test, predictDiff, baseTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# some more feature analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in allF:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = ['sales', 'weekday',\n",
    " #'month',\n",
    " 'onpromotion',\n",
    " #'dcoilwtico',\n",
    " 'holidayType',\n",
    " 'description',\n",
    " 'transferred',\n",
    " 'store_closed',\n",
    " #'logSalesSeasonality_7',\n",
    " #'logSalesSeasonality_14',\n",
    " #'logSalesSeasonality_21',\n",
    " #'logSalesSeasonality_28',\n",
    " #'logSalesSeasonality_52',\n",
    " #'logSalesSeasonality_365'\n",
    " ]\n",
    "\n",
    "train_subDf[f].iloc[0:50].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = ['sales', #'weekday',\n",
    " #'month',\n",
    " #'onpromotion',\n",
    " 'dcoilwtico',\n",
    " 'dcoilwtico_diff1',\n",
    " #'holidayType',\n",
    " #'description',\n",
    " #'transferred',\n",
    " #'store_closed',\n",
    " 'logSalesSeasonality_7',\n",
    " 'logSalesSeasonality_14',\n",
    " #'logSalesSeasonality_21',\n",
    " #'logSalesSeasonality_28',\n",
    " #'logSalesSeasonality_52',\n",
    " #'logSalesSeasonality_365'\n",
    " ]\n",
    "\n",
    "train_subDf[f].iloc[0:50].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.13 ('storeSales')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "17f27e0cbed1f29822e509a94c958ed0e80b7d9abe162097755686a614de4732"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
