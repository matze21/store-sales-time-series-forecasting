{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goals\n",
    "1. predict for every store individually\n",
    "- make stationary target by diff, yes/no?\n",
    "- z score normalization on train data\n",
    "- predict next 16 values directly vs recursively?\n",
    "2. predict store individually but with every pair/family as parameter\n",
    "- needs zscore normalization\n",
    "- stationary target yes/no?\n",
    "3. predict all store/family pairs simultaneously\n",
    "- zscore? maybe not needed\n",
    "- stationary?\n",
    "\n",
    "features:\n",
    "1. time features:\n",
    "- linear timestamp\n",
    "- sin/cos of year, check for (week/month) if pattern present\n",
    "- encoding of weekday, maybe also month\n",
    "2. oil/holidays/location should be ok\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import plotly.offline as pyo\n",
    "from plotly import subplots\n",
    "import plotly.graph_objects as go\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.tsa.api as smt\n",
    "\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from tensorflow.keras.models import Sequential\n",
    "#from tensorflow.keras.layers import LSTM, Dense,Input,concatenate\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "from baseFunctions import *\n",
    "from data_helpers import processData6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, propDicts, flippedPropDicts = processData6()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature engineering\n",
    "\n",
    "aggregated data\n",
    "- there is some linear trend "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dailyData = data.groupby('date')['sales'].sum()\n",
    "dec = sm.tsa.seasonal_decompose(dailyData,period = 12, model = 'additive').plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_periodogram(dailyData, 365, n_domFreq=30)\n",
    "\n",
    "# strong frequencies     => TimePeriod \n",
    "# 52 (weekly) 365/52     = 7 days\n",
    "# 24 (biweekly) 365/24   = 15 days (half-month)\n",
    "# 104 (halfweek) 365/104 = 3.5 = 3.5 days \n",
    "# 12 (monthly)  365/12   = 30 days\n",
    "# 6 (bimonthly)          = 60 days\n",
    "# 4 (quarters)           = 90 days\n",
    "# 3 (thirds)             = 120 days\n",
    "# 2 (half-year)          = 182\n",
    "# 1 (yearly)             = 365 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = data.copy()\n",
    "\n",
    "# add linear time\n",
    "data1['linear_time'] = (data1['date'] - data1['date'].iloc[0]).dt.days +1\n",
    "data1['day_of_year'] = data1['date'].dt.day_of_year\n",
    "\n",
    "data1, periodicfeat = addFourierFeature(data1, n_splits = 6, frequency=1, feature='day_of_year', referenceTimespan = 365)\n",
    "data1, periodicfeat = addFourierFeature(data1, n_splits = 6, frequency=2, feature='day_of_year', referenceTimespan = 365)\n",
    "data1, periodicfeat = addFourierFeature(data1, n_splits = 6, frequency=3, feature='day_of_year', referenceTimespan = 365)\n",
    "data1, periodicfeat = addFourierFeature(data1, n_splits = 6, frequency=4, feature='day_of_year', referenceTimespan = 365)\n",
    "data1, periodicfeat = addFourierFeature(data1, n_splits = 6, frequency=6, feature='day_of_year', referenceTimespan = 365)\n",
    "data1, periodicfeat = addFourierFeature(data1, n_splits = 6, frequency=12, feature='day_of_year', referenceTimespan = 365)\n",
    "data1, periodicfeat = addFourierFeature(data1, n_splits = 6, frequency=104, feature='day_of_year', referenceTimespan = 365)\n",
    "data1, periodicfeat = addFourierFeature(data1, n_splits = 6, frequency=24, feature='day_of_year', referenceTimespan = 365)\n",
    "data1, periodicfeat = addFourierFeature(data1, n_splits = 6, frequency=52, feature='day_of_year', referenceTimespan = 365)\n",
    "\n",
    "data1['weekday'] = data1['date'].dt.weekday\n",
    "data1['month'] = data1['date'].dt.month\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# individual prediction\n",
    "\n",
    "some approaches work for large values but not for small and vice versa\n",
    "predicting large & small values at the same time is hard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flippedPropDicts['family']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data1.loc[(data1.dataT == 'train') & (data1.store_nbr == 1) & (data1.family == 3)]\n",
    "fig = subplots.make_subplots(rows=2, cols=1, shared_xaxes='all')\n",
    "fig.add_trace(go.Scattergl(x=train.date, y=train.sales), col=1, row = 1)\n",
    "fig.add_trace(go.Scattergl(x=train.date, y=train.weekday), col=1, row = 2)\n",
    "fig.add_trace(go.Scattergl(x=train.date, y=train.holidayType), col=1, row = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores = cross_val_score(model, X, y, cv=5)\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "\n",
    "print(\"Mean CV score:\", cv_scores.mean())\n",
    "print(\"Standard deviation of CV scores:\", cv_scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainF = [\n",
    "       #'store_nbr', 'family', \n",
    "       #'sales',\n",
    "       'onpromotion', \n",
    "       #'dataT',\n",
    "       #'city', 'state', 'type', 'cluster',\n",
    "       'dcoilwtico', \n",
    "       'holidayType',\n",
    "       'description', 'transferred', \n",
    "       'linear_time', \n",
    "       'day_of_year',\n",
    "       'weekday',\n",
    "       'month',\n",
    "\n",
    "       #'day_of_year_f1_0', \n",
    "       # 'day_of_year_f1_60', 'day_of_year_f1_120',\n",
    "       #'day_of_year_f1_180', 'day_of_year_f1_240', 'day_of_year_f1_300',\n",
    "       #'day_of_year_f2_0', \n",
    "       # 'day_of_year_f2_60', 'day_of_year_f2_120',\n",
    "       #'day_of_year_f2_180', 'day_of_year_f2_240', 'day_of_year_f2_300',\n",
    "       #'day_of_year_f3_0',\n",
    "       #  'day_of_year_f3_60', 'day_of_year_f3_120',\n",
    "       #'day_of_year_f3_180', 'day_of_year_f3_240', 'day_of_year_f3_300',\n",
    "       #'day_of_year_f4_0',# 'day_of_year_f4_60', 'day_of_year_f4_120',\n",
    "       #'day_of_year_f4_180', 'day_of_year_f4_240', 'day_of_year_f4_300',\n",
    "       #'day_of_year_f6_0', #'day_of_year_f6_60', 'day_of_year_f6_120',\n",
    "       #'day_of_year_f6_180', 'day_of_year_f6_240', 'day_of_year_f6_300',\n",
    "       'day_of_year_f12_0',\n",
    "          #'day_of_year_f12_60',\n",
    "       #'day_of_year_f12_120',\n",
    "       #'day_of_year_f12_180',# \n",
    "       #'day_of_year_f12_240', #'day_of_year_f12_300',\n",
    "       'day_of_year_f104_0',\n",
    "        # 'day_of_year_f104_60',\n",
    "       #'day_of_year_f104_120',\n",
    "       #'day_of_year_f104_180',# \n",
    "       #'day_of_year_f104_240',# 'day_of_year_f104_300',\n",
    "       'day_of_year_f24_0', \n",
    "       #'day_of_year_f24_60', \n",
    "       #'day_of_year_f24_120',\n",
    "       #'day_of_year_f24_180', \n",
    "       #'day_of_year_f24_240',# 'day_of_year_f24_300',\n",
    "       'day_of_year_f52_0',\n",
    "       # 'day_of_year_f52_60', \n",
    "       #'day_of_year_f52_120',\n",
    "       #'day_of_year_f52_180'#, \n",
    "       #'day_of_year_f52_240'#, 'day_of_year_f52_300'\n",
    "       \n",
    "       ]\n",
    "\n",
    "train0 = trainF + ['sales']\n",
    "\n",
    "train = data1.loc[(data1.dataT == 'train') & (data1.store_nbr == 1) & (data1.family == 18)] # family 18\n",
    "\n",
    "n_predictedValues = 16\n",
    "look_back = 100\n",
    "zScoreNorm = True\n",
    "\n",
    "sequence0 = []\n",
    "sequence1 = []\n",
    "labels = []\n",
    "\n",
    "# zscore over all values -> not ideal bc test data\n",
    "if zScoreNorm:\n",
    "    mean = train.sales.mean()\n",
    "    mean = 0 # modified zScore, not in mean = 0\n",
    "    std = train.sales.std()\n",
    "    train['sales'] = (train.sales - mean) / std\n",
    "\n",
    "for i in range(train.shape[0]-look_back-n_predictedValues):\n",
    "    startS0 = i\n",
    "    endS0 = startS0 + look_back\n",
    "    endS1 = endS0 + n_predictedValues\n",
    "    sequence0.append(train[train0].iloc[startS0:endS0].to_numpy().flatten())\n",
    "    sequence1.append(train[trainF].iloc[endS0:endS1].to_numpy().flatten())\n",
    "    labels.append(train['sales'].iloc[endS0:endS1])\n",
    "sequence0 = np.stack(sequence0, axis = 0)\n",
    "sequence1 = np.stack(sequence1, axis=0)\n",
    "labels    = np.stack(labels, axis = 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate((sequence0, sequence1), axis=1)\n",
    "y = labels\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "test_data = lgb.Dataset(X_test, label=y_test)\n",
    "\n",
    "# Set parameters for LGBM model\n",
    "params = {\n",
    "    'objective': 'regression',  # Assuming you're doing regression\n",
    "    'metric': 'msle',  # Mean squared error\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': 1\n",
    "}\n",
    "\n",
    "\n",
    "# Train the model\n",
    "num_round = 100\n",
    "bst = lgb.train(params, train_data, num_round, valid_sets=[test_data], early_stopping_rounds=10)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = bst.predict(X_test, num_iteration=bst.best_iteration)\n",
    "\n",
    "\n",
    "\n",
    "#forecast = model.predict(X_train, verbose=False)\n",
    "#if zScoreNorm:\n",
    "#    forecast = forecast *std  + mean\n",
    "#    y_train = y_train *std + mean\n",
    "#rmsleTrain = np.sqrt(mean_squared_log_error(forecast, y_train))\n",
    "#forecast = model.predict(X_test, verbose=False)\n",
    "#if zScoreNorm:\n",
    "#    forecast = forecast *std  + mean\n",
    "#    y_test = y_test*std + mean\n",
    "#rmsleTest = np.sqrt(mean_squared_log_error(forecast, y_test))\n",
    "#print('errors:  ', round(rmsleTrain,3), round(rmsleTest,3), y_train.shape[0], y_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM approach\n",
    "only works either for small OR for large values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainF = [\n",
    "       #'store_nbr', 'family', \n",
    "       #'sales',\n",
    "       'onpromotion', \n",
    "       #'dataT',\n",
    "       #'city', 'state', 'type', 'cluster',\n",
    "       'dcoilwtico', \n",
    "       'holidayType',\n",
    "       'description', 'transferred', \n",
    "       'linear_time', \n",
    "       'day_of_year',\n",
    "       'weekday',\n",
    "       'month',\n",
    "\n",
    "       #'day_of_year_f1_0', \n",
    "       # 'day_of_year_f1_60', 'day_of_year_f1_120',\n",
    "       #'day_of_year_f1_180', 'day_of_year_f1_240', 'day_of_year_f1_300',\n",
    "       #'day_of_year_f2_0', \n",
    "       # 'day_of_year_f2_60', 'day_of_year_f2_120',\n",
    "       #'day_of_year_f2_180', 'day_of_year_f2_240', 'day_of_year_f2_300',\n",
    "       #'day_of_year_f3_0',\n",
    "       #  'day_of_year_f3_60', 'day_of_year_f3_120',\n",
    "       #'day_of_year_f3_180', 'day_of_year_f3_240', 'day_of_year_f3_300',\n",
    "       #'day_of_year_f4_0',# 'day_of_year_f4_60', 'day_of_year_f4_120',\n",
    "       #'day_of_year_f4_180', 'day_of_year_f4_240', 'day_of_year_f4_300',\n",
    "       #'day_of_year_f6_0', #'day_of_year_f6_60', 'day_of_year_f6_120',\n",
    "       #'day_of_year_f6_180', 'day_of_year_f6_240', 'day_of_year_f6_300',\n",
    "       'day_of_year_f12_0',\n",
    "          #'day_of_year_f12_60',\n",
    "       #'day_of_year_f12_120',\n",
    "       #'day_of_year_f12_180',# \n",
    "       #'day_of_year_f12_240', #'day_of_year_f12_300',\n",
    "       'day_of_year_f104_0',\n",
    "        # 'day_of_year_f104_60',\n",
    "       #'day_of_year_f104_120',\n",
    "       #'day_of_year_f104_180',# \n",
    "       #'day_of_year_f104_240',# 'day_of_year_f104_300',\n",
    "       'day_of_year_f24_0', \n",
    "       #'day_of_year_f24_60', \n",
    "       #'day_of_year_f24_120',\n",
    "       #'day_of_year_f24_180', \n",
    "       #'day_of_year_f24_240',# 'day_of_year_f24_300',\n",
    "       'day_of_year_f52_0',\n",
    "       # 'day_of_year_f52_60', \n",
    "       #'day_of_year_f52_120',\n",
    "       #'day_of_year_f52_180'#, \n",
    "       #'day_of_year_f52_240'#, 'day_of_year_f52_300'\n",
    "       \n",
    "       ]\n",
    "\n",
    "train0 = trainF + ['sales']\n",
    "\n",
    "train = data1.loc[(data1.dataT == 'train') & (data1.store_nbr == 1) & (data1.family == 18)] # family 18\n",
    "\n",
    "n_predictedValues = 16\n",
    "look_back = 100\n",
    "zScoreNorm = True\n",
    "\n",
    "sequence0 = []\n",
    "sequence1 = []\n",
    "labels = []\n",
    "\n",
    "# zscore over all values -> not ideal bc test data\n",
    "if zScoreNorm:\n",
    "    mean = train.sales.mean()\n",
    "    mean = 0 # modified zScore, not in mean = 0\n",
    "    std = train.sales.std()\n",
    "    train['sales'] = (train.sales - mean) / std\n",
    "\n",
    "for i in range(train.shape[0]-look_back-n_predictedValues):\n",
    "    startS0 = i\n",
    "    endS0 = startS0 + look_back\n",
    "    endS1 = endS0 + n_predictedValues\n",
    "    sequence0.append(train[train0].iloc[startS0:endS0])\n",
    "    sequence1.append(train[trainF].iloc[endS0:endS1])\n",
    "    labels.append(train['sales'].iloc[endS0:endS1])\n",
    "sequence0 = np.stack(sequence0, axis = 0)\n",
    "sequence1 = np.stack(sequence1, axis=0)\n",
    "labels    = np.stack(labels, axis = 0)\n",
    "\n",
    "\n",
    "try:\n",
    "    tf.keras.utils.set_random_seed(42)\n",
    "except:\n",
    "    print('using new tf')\n",
    "\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "n_features = len(train0)\n",
    "\n",
    "input1 = Input(shape=(look_back, n_features))\n",
    "input2 = Input(shape=(n_predictedValues, n_features-1))\n",
    "\n",
    "lstm1 = LSTM(64, activation='relu', return_sequences=False)(input1)\n",
    "lstm2 = LSTM(64, activation='relu', return_sequences=False)(input2)\n",
    "\n",
    "#lstm1 = LSTM(64, activation='relu', return_sequences=False, kernel_regularizer=regularizers.l2(0.001))(lstm1)\n",
    "#lstm2 = LSTM(64, activation='relu', return_sequences=False, kernel_regularizer=regularizers.l2(0.001))(lstm2)\n",
    "\n",
    "#lstm2 = Dense(n_predictedValues, activation='relu')(lstm2)\n",
    "x = tf.keras.layers.concatenate([lstm1, lstm2])\n",
    "x = Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "if zScoreNorm:\n",
    "    output = Dense(n_predictedValues, activation='relu')(x)\n",
    "else:\n",
    "    output = Dense(n_predictedValues, activation='relu')(x)\n",
    "\n",
    "# Define the model\n",
    "model = Model(inputs=[input1, input2], outputs=output)\n",
    "\n",
    "optimizer = Adam(learning_rate=0.00001)\n",
    "model.compile(optimizer='adam', loss='mae', metrics=[tf.keras.losses.MSLE]) \n",
    "#model.compile(optimizer='adam', loss=tf.keras.losses.MSLE, metrics=['mae']) \n",
    "\n",
    "n_splits = 5\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "X = [sequence0,sequence1]\n",
    "y = labels\n",
    "\n",
    "for train_index, test_index in tscv.split(sequence0):\n",
    "    X_train = [sequence0[train_index],sequence1[train_index]]\n",
    "    X_test  = [sequence0[test_index], sequence1[test_index]]\n",
    "    y_train, y_test = labels[train_index], labels[test_index]\n",
    "\n",
    "    model.fit(X_train, y_train, epochs=20, batch_size=32,validation_data=(X_test, y_test))\n",
    "\n",
    "\n",
    "    forecast = model.predict(X_train, verbose=False)\n",
    "    if zScoreNorm:\n",
    "        forecast = forecast *std  + mean\n",
    "        y_train = y_train *std + mean\n",
    "    rmsleTrain = np.sqrt(mean_squared_log_error(forecast, y_train))\n",
    "    forecast = model.predict(X_test, verbose=False)\n",
    "    if zScoreNorm:\n",
    "        forecast = forecast *std  + mean\n",
    "        y_test = y_test*std + mean\n",
    "    rmsleTest = np.sqrt(mean_squared_log_error(forecast, y_test))\n",
    "    print('errors:  ', round(rmsleTrain,3), round(rmsleTest,3), y_train.shape[0], y_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test, forecast\n",
    "# --------------seed = 0 -------------------------------------------------------------------------------------------------------------\n",
    "# --------------familyId = 3 (Beverages), store id = 1 -----------(train 1307 test 261)---------- 7.219 7.652 == all 0 ----------------\n",
    "# --------------5 splits, 10 epochs per split, 32 batch size---------------------------------------------------------------------------\n",
    "# errors:   0.856 0.740     without all the time featuers: 'dcoilwtico', 'holidayType','description', 'transferred', 'linear_time', 'day_of_year','weekday', 'month',\n",
    "# errors:   7.219 7.652     with all the time featuers\n",
    "# errors:   2.885 4.493     without oil a lot worse :o     'holidayType','description', 'transferred', 'linear_time', 'day_of_year','weekday', 'month',\n",
    "# errors:   1.533 2.071     without descrip.transf         'dcoilwtico', 'holidayType', 'linear_time', 'day_of_year','weekday', 'month',\n",
    "# errors:   1.262 1.156     without time features          'dcoilwtico', 'holidayType','description', 'transferred',\n",
    "# errors:   7.219 7.653     without day of year & month    'dcoilwtico', 'holidayType','description', 'transferred', 'linear_time','weekday',\n",
    "# errors:   7.219 7.653     without month                  'dcoilwtico', 'holidayType','description', 'transferred', 'linear_time', 'day_of_year','weekday',\n",
    "# errors:   1.247 1.128     without lin time & day of y    'dcoilwtico', 'holidayType','description', 'transferred','weekday', 'month',\n",
    "# errors:   7.219 7.653     without day of year            'dcoilwtico', 'holidayType','description', 'transferred', 'linear_time','weekday', 'month',\n",
    "\n",
    "# testing fourier features, lock those features (always use) 'dcoilwtico', 'holidayType','description', 'transferred', 'linear_time', 'day_of_year','weekday', 'month',\n",
    "# errors:   0.624 0.694     frequency: 12, 24, 52, 104 only 0 phase diff:       day_of_year_f12_0, day_of_year_f104_0, day_of_year_f24_0, day_of_year_f52_0\n",
    "# errors:   3.162 3.661     frequency: 12, 24, 52, 104  6x 60° phase diff:\n",
    "# errors ->nans  frequency: 12, 24, 52, 104 only 0 & 180°:\n",
    "# errors:   0.705 0.731     frequency: 12, 24, 52, 104 only 0, 120, 240° phase diff:\n",
    "# errors:   6.92  7.155     frequency: 1,2,3,4,6,12, 24, 52, 104 only 0 phase diff:\n",
    "# errors:   7.211 7.595     frequency: 1,2,6,12, 24, 52, 104 only 0 phase diff:\n",
    "# errors:   4.929 5.107     frequency: 6,12, 24, 52, 104 only 0 phase diff:\n",
    "# errors ->nans  frequency: 4, 12, 24, 52, 104 only 0:\n",
    "# errors:   2.155 1.544     frequency: 3,12, 24, 52, 104 only 0 phase diff:\n",
    "# errors ->nans  frequency: 1, 12, 24, 52, 104 only 0:\n",
    "# errors ->nans  frequency: 2, 12, 24, 52, 104 only 0:\n",
    "\n",
    "# --------------familyId = 3 (Beverages), store id = 18 ----------(train 1307 test 261)---------- 3.873 5.123 == all 0 ----------------\n",
    "# --------------5 splits, 10 epochs per split, 32 batch size---------------------------------------------------------------------------\n",
    "#errors:   3.873 5.123      'dcoilwtico', 'holidayType','description', 'transferred', 'linear_time', 'day_of_year','weekday', 'month', frequency: 12, 24, 52, 104 only 0 phase diff\n",
    "# -> zscaling doesn't help, nothing really works for this one\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(forecast != 0).any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict in one big dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = (data1.loc[data1.dataT == 'train'].pivot(index='date', columns=['store_nbr', 'family']))#.transpose#()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "60f2b4b2b39245c89a47c8dbe671288aea181e96fbe781c7f5f13eb9eb69cf46"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
