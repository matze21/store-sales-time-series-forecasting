{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goals\n",
    "1. predict for every store individually\n",
    "- make stationary target by diff, yes/no?\n",
    "- z score normalization on train data\n",
    "- predict next 16 values directly vs recursively?\n",
    "2. predict store individually but with every pair/family as parameter\n",
    "- needs zscore normalization\n",
    "- stationary target yes/no?\n",
    "3. predict all store/family pairs simultaneously\n",
    "- zscore? maybe not needed\n",
    "- stationary?\n",
    "\n",
    "features:\n",
    "1. time features:\n",
    "- linear timestamp\n",
    "- sin/cos of year, check for (week/month) if pattern present\n",
    "- encoding of weekday, maybe also month\n",
    "2. oil/holidays/location should be ok\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import plotly.offline as pyo\n",
    "from plotly import subplots\n",
    "import plotly.graph_objects as go\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.tsa.api as smt\n",
    "\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "\n",
    "\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from tensorflow.keras.models import Sequential\n",
    "#from tensorflow.keras.layers import LSTM, Dense,Input,concatenate\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "\n",
    "from baseFunctions import *\n",
    "from data_helpers import processData6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, propDicts, flippedPropDicts = processData6()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature engineering\n",
    "\n",
    "aggregated data\n",
    "- there is some linear trend "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dailyData = data.groupby('date')['sales'].sum()\n",
    "dec = sm.tsa.seasonal_decompose(dailyData,period = 12, model = 'additive').plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_periodogram(dailyData, 365, n_domFreq=30)\n",
    "\n",
    "# strong frequencies     => TimePeriod \n",
    "# 52 (weekly) 365/52     = 7 days\n",
    "# 24 (biweekly) 365/24   = 15 days (half-month)\n",
    "# 104 (halfweek) 365/104 = 3.5 = 3.5 days \n",
    "# 12 (monthly)  365/12   = 30 days\n",
    "# 6 (bimonthly)          = 60 days\n",
    "# 4 (quarters)           = 90 days\n",
    "# 3 (thirds)             = 120 days\n",
    "# 2 (half-year)          = 182\n",
    "# 1 (yearly)             = 365 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = data.copy()\n",
    "\n",
    "# add linear time\n",
    "data1['linear_time'] = (data1['date'] - data1['date'].iloc[0]).dt.days +1\n",
    "data1['day_of_year'] = data1['date'].dt.day_of_year\n",
    "\n",
    "data1, periodicfeat = addFourierFeature(data1, n_splits = 6, frequency=1, feature='day_of_year', referenceTimespan = 365)\n",
    "data1, periodicfeat = addFourierFeature(data1, n_splits = 6, frequency=2, feature='day_of_year', referenceTimespan = 365)\n",
    "data1, periodicfeat = addFourierFeature(data1, n_splits = 6, frequency=3, feature='day_of_year', referenceTimespan = 365)\n",
    "data1, periodicfeat = addFourierFeature(data1, n_splits = 6, frequency=4, feature='day_of_year', referenceTimespan = 365)\n",
    "data1, periodicfeat = addFourierFeature(data1, n_splits = 6, frequency=6, feature='day_of_year', referenceTimespan = 365)\n",
    "data1, periodicfeat = addFourierFeature(data1, n_splits = 6, frequency=12, feature='day_of_year', referenceTimespan = 365)\n",
    "data1, periodicfeat = addFourierFeature(data1, n_splits = 6, frequency=104, feature='day_of_year', referenceTimespan = 365)\n",
    "data1, periodicfeat = addFourierFeature(data1, n_splits = 6, frequency=24, feature='day_of_year', referenceTimespan = 365)\n",
    "data1, periodicfeat = addFourierFeature(data1, n_splits = 6, frequency=52, feature='day_of_year', referenceTimespan = 365)\n",
    "\n",
    "data1['weekday'] = data1['date'].dt.weekday\n",
    "data1['month'] = data1['date'].dt.month\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# individual prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainF = [\n",
    "       #'store_nbr', 'family', \n",
    "       #'sales',\n",
    "       'onpromotion', \n",
    "       #'dataT',\n",
    "       #'city', 'state', 'type', 'cluster',\n",
    "       'dcoilwtico', 'holidayType',\n",
    "       'description', 'transferred', \n",
    "       'linear_time', 'day_of_year',\n",
    "       'day_of_year_f1_0', 'day_of_year_f1_60', 'day_of_year_f1_120',\n",
    "       'day_of_year_f1_180', 'day_of_year_f1_240', 'day_of_year_f1_300',\n",
    "       'day_of_year_f2_0', 'day_of_year_f2_60', 'day_of_year_f2_120',\n",
    "       'day_of_year_f2_180', 'day_of_year_f2_240', 'day_of_year_f2_300',\n",
    "       'day_of_year_f3_0', 'day_of_year_f3_60', 'day_of_year_f3_120',\n",
    "       'day_of_year_f3_180', 'day_of_year_f3_240', 'day_of_year_f3_300',\n",
    "       'day_of_year_f4_0', 'day_of_year_f4_60', 'day_of_year_f4_120',\n",
    "       'day_of_year_f4_180', 'day_of_year_f4_240', 'day_of_year_f4_300',\n",
    "       'day_of_year_f6_0', 'day_of_year_f6_60', 'day_of_year_f6_120',\n",
    "       'day_of_year_f6_180', 'day_of_year_f6_240', 'day_of_year_f6_300',\n",
    "       'day_of_year_f12_0', 'day_of_year_f12_60', 'day_of_year_f12_120',\n",
    "       'day_of_year_f12_180', 'day_of_year_f12_240', 'day_of_year_f12_300',\n",
    "       'day_of_year_f104_0', 'day_of_year_f104_60', 'day_of_year_f104_120',\n",
    "       'day_of_year_f104_180', 'day_of_year_f104_240', 'day_of_year_f104_300',\n",
    "       'day_of_year_f24_0', 'day_of_year_f24_60', 'day_of_year_f24_120',\n",
    "       'day_of_year_f24_180', 'day_of_year_f24_240', 'day_of_year_f24_300',\n",
    "       'day_of_year_f52_0', 'day_of_year_f52_60', 'day_of_year_f52_120',\n",
    "       'day_of_year_f52_180', 'day_of_year_f52_240', 'day_of_year_f52_300',\n",
    "       'weekday', 'month'\n",
    "       ]\n",
    "\n",
    "train0 = trainF + ['sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data1.loc[(data1.dataT == 'train') & (data1.store_nbr == 1) & (data1.family == 3)] # family 18\n",
    "\n",
    "n_predictedValues = 16\n",
    "look_back = 100\n",
    "\n",
    "sequence0 = []\n",
    "sequence1 = []\n",
    "labels = []\n",
    "\n",
    "for i in range(train.shape[0]-look_back-n_predictedValues):\n",
    "    startS0 = i\n",
    "    endS0 = startS0 + look_back\n",
    "    endS1 = endS0 + n_predictedValues\n",
    "    sequence0.append(train[train0].iloc[startS0:endS0])\n",
    "    sequence1.append(train[trainF].iloc[endS0:endS1])\n",
    "    labels.append(train['sales'].iloc[endS0:endS1])\n",
    "sequence0, sequence1, labels = np.array(sequence0), np.array(sequence1), np.array(labels)\n",
    "\n",
    "\n",
    "\n",
    "tf.keras.utils.set_random_seed(42)\n",
    "\n",
    "n_features = len(train0)\n",
    "\n",
    "input1 = Input(shape=(look_back, n_features))\n",
    "input2 = Input(shape=(n_predictedValues, n_features-1))\n",
    "\n",
    "lstm1 = LSTM(64, activation='relu', return_sequences=True, kernel_regularizer=regularizers.l2(0.001))(input1)\n",
    "lstm2 = LSTM(64, activation='relu', return_sequences=True, kernel_regularizer=regularizers.l2(0.001))(input2)\n",
    "\n",
    "lstm1 = LSTM(64, activation='relu', return_sequences=False, kernel_regularizer=regularizers.l2(0.001))(lstm1)\n",
    "lstm2 = LSTM(64, activation='relu', return_sequences=False, kernel_regularizer=regularizers.l2(0.001))(lstm2)\n",
    "\n",
    "#lstm2 = Dense(n_predictedValues, activation='relu')(lstm2)\n",
    "x = tf.keras.layers.concatenate([lstm1, lstm2])\n",
    "x = Dense(128, activation='relu')(x)\n",
    "output = Dense(n_predictedValues, activation='relu')(x)\n",
    "\n",
    "# Define the model\n",
    "model = Model(inputs=[input1, input2], outputs=output)\n",
    "\n",
    "# Compile and train the model\n",
    "model.compile(optimizer='adam', loss=tf.keras.losses.MSLE, metrics=['mae']) \n",
    "\n",
    "n_splits = 10\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "for train_index, test_index in tscv.split(sequence0):\n",
    "    X_train = [sequence0[train_index],sequence1[train_index]]\n",
    "    X_test  = [sequence0[test_index], sequence1[test_index]]\n",
    "    y_train, y_test = labels[train_index], labels[test_index]\n",
    "\n",
    "    model.fit(X_train, y_train, epochs=5, batch_size=32,validation_data=(X_test, y_test))\n",
    "\n",
    "forecast = model.predict(X_train, verbose=False)\n",
    "rmsleTrain = np.sqrt(mean_squared_log_error(forecast, y_train))\n",
    "forecast = model.predict(X_test, verbose=False)\n",
    "rmsleTest = np.sqrt(mean_squared_log_error(forecast, y_test))\n",
    "print('errors:  ', rmsleTrain, rmsleTest,X_train[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast = model.predict(X_train, verbose=False)\n",
    "rmsleTrain = np.sqrt(mean_squared_log_error(forecast, y_train))\n",
    "forecast = model.predict(X_test, verbose=False)\n",
    "rmsleTest = np.sqrt(mean_squared_log_error(forecast, y_test))\n",
    "print('errors:  ', rmsleTrain, rmsleTest, X_train[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict in one big dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = (data1.loc[data1.dataT == 'train'].pivot(index='date', columns=['store_nbr', 'family']))#.transpose#()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "60f2b4b2b39245c89a47c8dbe671288aea181e96fbe781c7f5f13eb9eb69cf46"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
