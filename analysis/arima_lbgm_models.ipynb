{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import plotly.offline as pyo\n",
    "from plotly import subplots\n",
    "import plotly.graph_objects as go\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.tsa.api as smt\n",
    "\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from tensorflow.keras.models import Sequential\n",
    "#from tensorflow.keras.layers import LSTM, Dense,Input,concatenate\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import lightgbm as lgb\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "from baseFunctions import *\n",
    "from data_helpers import processData6, featureEngineering, getSequencesFast, removeOutliers, create_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, propDicts, flippedPropDicts = processData6()\n",
    "data, timeFeatures = featureEngineering(data,splits=[2,2,2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addSeasonality(dt, dec, outputLength):\n",
    "    first365Vals = dec.seasonal[0:dt] \n",
    "    first365Vals = first365Vals / max(first365Vals) \n",
    "\n",
    "    folds = int(outputLength/ dt)\n",
    "    rest = (outputLength % dt) \n",
    "\n",
    "    seasonalVals = np.ones((outputLength)) * np.nan\n",
    "    for i in range(folds):\n",
    "        firstId = i*dt\n",
    "        second = firstId + dt\n",
    "        seasonalVals[firstId:second] = first365Vals\n",
    "    seasonalVals[second:second+rest] = first365Vals[0:rest]\n",
    "\n",
    "    return seasonalVals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcLossArima(pred, y, logTransform):\n",
    "    if logTransform:\n",
    "        a = np.exp(pred) -1\n",
    "        y = np.exp(y) -1 \n",
    "    else:\n",
    "        a = (pred)\n",
    "        y = (y)\n",
    "\n",
    "    if (a < 0).any():\n",
    "        a = np.clip(a, 0, 1e20)\n",
    "\n",
    "    rmsleTrain = np.sqrt(mean_squared_log_error(a,y))\n",
    "    return rmsleTrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_sarimax(group):\n",
    "    \"\"\"\n",
    "    Fit a SARIMAX model to each group (time series) and make predictions.\n",
    "    \"\"\"\n",
    "    train = group[group['dataT'] != 'test']\n",
    "    test = group[group['dataT'] =='test']\n",
    "    \n",
    "    model = SARIMAX(train['logSales'], order=(3, 1, 3), seasonal_order=(1, 1, 1, 7))\n",
    "    model_fit = model.fit()\n",
    "    \n",
    "    predictions = model_fit.forecast(len(test))\n",
    "    \n",
    "    group.loc[train.index,'salesArima'] = model_fit.fittedvalues\n",
    "    group.loc[test.index, 'salesArima'] = np.reshape(predictions, (len(test),1))\n",
    "    \n",
    "    return group\n",
    "\n",
    "\"\"\" calc error or arima prediction \"\"\"\n",
    "def calcArimaErrorFullDf(data2):\n",
    "    data2['target'] = data2['logSales'] - data2['salesArima']\n",
    "    data2['targetSquared'] = data2['target']**2\n",
    "    \n",
    "    a = data2.groupby(['store_nbr','family'])['targetSquared'].mean()\n",
    "    print(np.sqrt(a.mean()))\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes 190min!! -> use multicore in the future\n",
    "data1 = data1.groupby(['store_nbr','family']).apply(fit_sarimax)\n",
    "data1.to_csv('data_enriched_with_sarima313_1117.csv')\n",
    "data2 = data1.set_index('id').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processAllData(data1, targetLags, featureLags, rolling, initial_lag):\n",
    "       grouped = data1.groupby(['store_nbr','family'])\n",
    "\n",
    "       data1['transactions'] = (data1.transactions - grouped.transactions.transform('mean')) / grouped.transactions.transform('std')\n",
    "       data1['linear_time'] = (data1['linear_time'] - grouped.linear_time.transform('mean')) / grouped.linear_time.transform('std')\n",
    "       data1['day_of_year'] = (data1['day_of_year'] - grouped.day_of_year.transform('mean')) / grouped.day_of_year.transform('std')\n",
    "       data1['dcoilwtico'] = (data1['dcoilwtico'] - grouped.dcoilwtico.transform('mean')) / grouped.dcoilwtico.transform('std')\n",
    "\n",
    "       #mask = (data1.date >= date_string_val) & (data1.dataT == 'train')\n",
    "       #data1.loc[mask,['dataT']] = 'val'\n",
    "\n",
    "       data1.loc[:,['logSales']] = np.log(data1.sales + 1)\n",
    "\n",
    "       arimaPred = pd.read_csv('sarima_313_117_and_id.csv')\n",
    "       data1 = pd.merge(data1, arimaPred[['id','sales','salesArima']], on=['id','sales'], how='left')\n",
    "       print('loaded arima data')\n",
    "\n",
    "       data1['ref'] = data1['salesArima']\n",
    "       data1['target'] = data1['logSales'] - data1['ref']\n",
    "       \n",
    "\n",
    "       featuresForLag = ['target']\n",
    "       lagF_target = []\n",
    "       for l in targetLags:\n",
    "              lag = l + initial_lag\n",
    "              newF = [featuresForLag[j] + '_lag' + str(lag) for j in range(len(featuresForLag))]\n",
    "              lagF_target = lagF_target + newF\n",
    "              data1[newF] = data1.groupby(['store_nbr','family'])[featuresForLag].shift(lag)\n",
    "\n",
    "       featuresForLag2 = ['salesArima','onpromotion','dcoilwtico']\n",
    "       lagF_features = []\n",
    "       for i in featureLags:\n",
    "              lag = i\n",
    "              newF = [featuresForLag2[j] + '_lag' + str(lag) for j in range(len(featuresForLag2))]\n",
    "              lagF_features = lagF_features + newF\n",
    "              data1[newF] = data1.groupby(['store_nbr','family'])[featuresForLag2].shift(lag)\n",
    "\n",
    "       lagF = lagF_target + lagF_features\n",
    "\n",
    "       rollingF = []\n",
    "       for rol in rolling:\n",
    "              for i in range(len(lagF)):\n",
    "                     #if 'sales_t-16'  in lagF[i]:\n",
    "                     if 'target'  in lagF[i] or 'dcoilwtico'  in lagF[i] or 'onpromotion'  in lagF[i]:\n",
    "                            fm = lagF[i]+'_rollingM' + str(rol)\n",
    "                            fs = lagF[i]+'_rollingS' + str(rol)\n",
    "                            rollingF.append(fm)\n",
    "                            rollingF.append(fs)\n",
    "                            data1[fm] = data1.groupby(['store_nbr','family'])[lagF[i]].rolling(rol, min_periods=1).mean().reset_index(drop=True)#.set_index('id')#.reset_index() #transform('mean') #lambda x: x.rolling(rol).mean()).to_numpy()\n",
    "                            data1[fs] = data1.groupby(['store_nbr','family'])[lagF[i]].rolling(rol, min_periods=1).std().reset_index(drop=True)\n",
    "\n",
    "       data3 = data1.groupby(['store_nbr','family']).apply(lambda x: x.iloc[max(max(targetLags),max(featureLags))+initial_lag+1:])\n",
    "       data3 = data3.set_index('id').reset_index()\n",
    "\n",
    "       return data3, lagF + rollingF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_lag = 16 # 16 = independent from previous predictions\n",
    "targetLags = [1,2,3,4,5,6,7,8,9,10]\n",
    "featureLags = [1,2,3,4,5,6,7,8,9,10]\n",
    "rolling = [7,21]\n",
    "\n",
    "allData, addedF = processAllData(data, targetLags, featureLags, rolling, initial_lag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allData[['target','zscaleTaget']].hist(bins = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allData['meanTarget'] = allData.groupby(['store_nbr','family'])['target'].transform('mean')\n",
    "allData['stdTarget'] = allData.groupby(['store_nbr','family'])['target'].transform('std')\n",
    "allData['zscaleTaget'] = (allData['target']-allData['meanTarget']) / allData['stdTarget']  # hard to normalize over std! creating lots of nans\n",
    "allData.zscaleTaget.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allData.stdTarget.hist(bins =100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = calcArimaErrorFullDf(allData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date string\n",
    "date_string_test = \"2017-08-01\"\n",
    "\n",
    "train_bigDf = allData.loc[(allData.date < date_string_test) & (allData.dataT =='train')]\n",
    "test_bigDf  = allData.loc[(allData.date >= date_string_test) & (allData.dataT !='test')]\n",
    "val_bigDf   = allData.loc[ allData.dataT =='test']\n",
    "\n",
    "trainF = [\n",
    "       'store_nbr', 'family', \n",
    "       #'sales', \n",
    "       'onpromotion',# 'dataT',\n",
    "       'city', 'state', 'type', 'cluster', \n",
    "       'dcoilwtico', \n",
    "       'holidayType',\n",
    "       'description', \n",
    "       'transferred', \n",
    "       #'transactions', \n",
    "       'store_closed']\n",
    "timeF = [\n",
    "       'linear_time', 'day_of_year', 'day_of_year_f12_0', 'day_of_year_f104_0','day_of_year_f24_0',  'day_of_year_f52_0',\n",
    "       'day_of_year_f12_180', 'day_of_year_f104_180','day_of_year_f24_180','day_of_year_f52_180', \n",
    "       'weekday', 'month'\n",
    "       ]\n",
    "\n",
    "allF = addedF + trainF + timeF\n",
    "\n",
    "targetF, refF = 'zscaleTaget', 'ref'\n",
    "logTransform = True\n",
    "predictDiff = True\n",
    "X_train, X_test, X_pred = train_bigDf[allF],test_bigDf[allF],val_bigDf[allF]\n",
    "y_train, y_test = train_bigDf[targetF], test_bigDf[targetF]\n",
    "baseTrain, baseTest, basePred = train_bigDf[refF], test_bigDf[refF],val_bigDf[refF]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### per group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processDataArima(storeDf, date_string_val, date_string_test, targetLags, featureLags):\n",
    "              trainF = [\n",
    "                     #'store_nbr', 'family', \n",
    "                     #'sales', \n",
    "                     'onpromotion',# 'dataT',\n",
    "                     #'city', 'state', 'type', 'cluster', \n",
    "                     'dcoilwtico', \n",
    "                     'holidayType',\n",
    "                     'description', \n",
    "                     'transferred', \n",
    "                     #'transactions', \n",
    "                     'store_closed']\n",
    "              timeF = [\n",
    "                     'linear_time', 'day_of_year', 'day_of_year_f12_0', 'day_of_year_f104_0','day_of_year_f24_0',  'day_of_year_f52_0',\n",
    "                     'day_of_year_f12_180', 'day_of_year_f104_180','day_of_year_f24_180','day_of_year_f52_180', \n",
    "                     'weekday', 'month'\n",
    "                     ]\n",
    "\n",
    "              storeDf['linear_time'] = (storeDf['linear_time'] - storeDf.linear_time.mean()) /storeDf.linear_time.std()\n",
    "              storeDf['day_of_year'] = (storeDf['day_of_year'] - storeDf.day_of_year.mean()) /storeDf.day_of_year.std()\n",
    "              storeDf['dcoilwtico'] = (storeDf['dcoilwtico'] - storeDf.dcoilwtico.mean()) /storeDf.dcoilwtico.std()\n",
    "              storeDf['transactions'] = (storeDf['transactions'] - storeDf.transactions.mean()) /storeDf.transactions.std()\n",
    "\n",
    "              mask = (storeDf.date >= date_string_val)\n",
    "              storeDf.loc[mask,['dataT']] = 'val'\n",
    "\n",
    "              # ln tranformation\n",
    "              storeDf.loc[:,['logSales']] = np.log(storeDf.sales + 1)\n",
    "\n",
    "              f = 'logSales'#'transactions'# 'salesOrig' #target\n",
    "              y_trainArima = storeDf.loc[storeDf.dataT == 'train'][[f,'date']]\n",
    "              y_trainArima = y_trainArima.set_index('date')\n",
    "              y_trainArima.index = pd.DatetimeIndex(y_trainArima.index).to_period('D')\n",
    "\n",
    "              model = SARIMAX(y_trainArima, order=(3, 1, 3), seasonal_order=(1,1,1,7))\n",
    "\n",
    "              model_fit = model.fit()\n",
    "              storeDf = storeDf.reset_index()\n",
    "              \n",
    "              arimaSalesLogT = np.array(model_fit.fittedvalues)\n",
    "              print(storeDf.shape[0]- len(model_fit.fittedvalues), 'arima predicted')\n",
    "              arimaSalesLogT = np.concatenate((arimaSalesLogT, model_fit.forecast(steps=(storeDf.shape[0]- len(model_fit.fittedvalues)))), axis=0)\n",
    "\n",
    "              storeDf.loc[:,['salesArima']] = arimaSalesLogT\n",
    "\n",
    "              storeDf.loc[:,['ref']] = storeDf['salesArima']\n",
    "              storeDf.loc[:,['target']] = storeDf['logSales'] - storeDf['ref']\n",
    "\n",
    "              # lag features / how many past datapoints are we tain\n",
    "              featuresForLag = ['target']\n",
    "              lagF_target = []#trainF\n",
    "              for i in targetLags:\n",
    "                     lag = i+initial_lag\n",
    "                     newF = [featuresForLag[j] + '_lag' + str(lag) for j in range(len(featuresForLag))]\n",
    "                     lagF_target = lagF_target + newF\n",
    "                     storeDf.loc[:,newF] = storeDf[featuresForLag].shift(lag).to_numpy()\n",
    "\n",
    "              featuresForLag2 = ['salesArima','onpromotion','dcoilwtico']\n",
    "              lagF_features = []#trainF\n",
    "              for i in featureLags:\n",
    "                     lag = i\n",
    "                     newF = [featuresForLag2[j] + '_lag' + str(lag) for j in range(len(featuresForLag2))]\n",
    "                     lagF_features = lagF_features + newF\n",
    "                     storeDf.loc[:,newF] = storeDf[featuresForLag2].shift(lag).to_numpy()\n",
    "\n",
    "              lagF = lagF_target + lagF_features\n",
    "\n",
    "              # rolling features\n",
    "              rollingF = []\n",
    "              for rol in rolling:\n",
    "                     for i in range(len(lagF)):\n",
    "                            #if 'sales_t-16'  in lagF[i]:\n",
    "                            if 'target'  in lagF[i] or 'dcoilwtico'  in lagF[i] or 'onpromotion'  in lagF[i]:\n",
    "                                   fm = lagF[i]+'_rollingM' + str(rol)\n",
    "                                   fs = lagF[i]+'_rollingS' + str(rol)\n",
    "                                   rollingF.append(fm)\n",
    "                                   rollingF.append(fs)\n",
    "                                   storeDf.loc[:,[fm]] = storeDf[lagF[i]].rolling(rol).mean()#.copy()\n",
    "                                   storeDf.loc[:,[fs]] = storeDf[lagF[i]].rolling(rol).std()#.copy()\n",
    "\n",
    "\n",
    "              allF = lagF + rollingF + timeF + trainF\n",
    "\n",
    "              # we get a matrix that predicts only 1 timestamp -> stride it\n",
    "              storeDf = storeDf.iloc[max(max(targetLags),max(featureLags))+initial_lag+max(rolling)+1:storeDf.shape[0]]\n",
    "\n",
    "              train_subDf = storeDf.loc[storeDf.date < date_string_test]\n",
    "              test_subDf  = storeDf.loc[storeDf.date >= date_string_test]\n",
    "              val_subDf   = storeDf.loc[storeDf.dataT =='val']\n",
    "\n",
    "              return train_subDf, test_subDf, val_subDf, allF, storeDf\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_lag = 16 # 16 = independent from previous predictions\n",
    "targetLags = [1,2,3,4,5,6,7,8,9,10]\n",
    "featureLags = [1,2,3,4,5,6,7,8,9,10]\n",
    "rolling = [7,21]\n",
    "\n",
    "# Date string\n",
    "date_string_val = \"2017-07-31\"#\"2017-05-01\" # no arima fit on this one\n",
    "date_string_test = \"2016-09-01\"\n",
    "\n",
    "logTransform = True\n",
    "predictDiff = False # predict diff between arima & sales tho\n",
    "\n",
    "\n",
    "for familyId in [0]:#[6]: #data.family.unique():\n",
    "       # start with only some families\n",
    "       if familyId > 8:\n",
    "          continue\n",
    "\n",
    "       print(familyId)\n",
    "       familyDf = data.loc[data.family==familyId]  \n",
    "\n",
    "       for storeId in [1]:#[41]: #data.store_nbr.unique():\n",
    "              print('store',storeId)\n",
    "              storeDf = familyDf.loc[(familyDf.store_nbr == storeId)] \n",
    "              storeDf = storeDf.loc[(storeDf.dataT == 'train')]# & (storeDf.date > \"2015-07-01\")]\n",
    "              train_subDf, test_subDf, val_subDf, allF, modifiedDf = processDataArima(storeDf, date_string_val, date_string_test, targetLags, featureLags)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "targetF = 'target'\n",
    "baseTrain = train_subDf[['ref']].to_numpy()\n",
    "baseTest  = test_subDf[['ref']].to_numpy()\n",
    "baseVal   = val_subDf[['ref']].to_numpy()\n",
    "    \n",
    "\n",
    "X_train = train_subDf[allF]\n",
    "y_train = train_subDf[targetF].to_numpy()\n",
    "X_test  =  test_subDf[allF]\n",
    "y_test  =  test_subDf[targetF].to_numpy()  \n",
    "X_val   =  val_subDf[allF]\n",
    "y_val   =  val_subDf[targetF].to_numpy() \n",
    "\n",
    "#np.isnan(X_train).any(),np.isnan(X_test).any(),np.isnan(X_val).any(),np.isnan(y_train).any(),np.isnan(y_test).any(),np.isnan(y_val).any()\n",
    "print('arima errors: ', calcLossArima(train_subDf.ref, train_subDf.logSales, logTransform, base=baseTrain), calcLossArima(test_subDf.ref, test_subDf.logSales, logTransform, baseTest), calcLossArima(val_subDf.ref, val_subDf.logSales, logTransform, baseTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subDf[['sales','onpromotion','store_closed','date']].set_index('date').plot(), test_subDf[['sales','onpromotion','store_closed','date']].set_index('date').plot(),val_subDf[['sales','onpromotion','store_closed','date']].set_index('date').plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analyze residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bigDf['salesPred'] = np.exp(train_bigDf['LGBMpred'] + train_bigDf['salesArima'])-1\n",
    "test_bigDf['salesPred'] = np.exp(test_bigDf['LGBMpred'] + test_bigDf['salesArima'])-1\n",
    "val_bigDf['salesPred'] = np.exp(val_bigDf['LGBMpred'] + val_bigDf['salesArima'])-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bigDf[['sales','salesPred']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bigDf['err'] = (np.log(train_bigDf.sales+1) - np.log(train_bigDf.salesPred + 1))**2\n",
    "test_bigDf['err'] = (np.log(test_bigDf.sales+1) - np.log(test_bigDf.salesPred + 1))**2\n",
    "val_bigDf['err'] = (np.log(val_bigDf.sales+1) - np.log(val_bigDf.salesPred + 1))**2\n",
    "np.sqrt(train_bigDf['err'].mean()),np.sqrt(test_bigDf['err'].mean()),np.sqrt(val_bigDf['err'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_bigDf.loc[(val_bigDf.store_nbr == 32) & (val_bigDf.family==11)][['sales','salesPred']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bigDf.loc[(test_bigDf.store_nbr == 32) & (test_bigDf.family==11)][['sales','salesPred']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bigDf.loc[(train_bigDf.store_nbr == 32) & (train_bigDf.family==11)][['sales','salesPred']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_bigDf['sales'] = val_bigDf['salesPred']\n",
    "val_bigDf['sales'] = np.clip(val_bigDf.sales, 0, 1e30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_bigDf[['sales','id']].set_index('id').to_csv('allData_arima_lgbm_pred_nLeaves20.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value = test_stationarity(train_subDf.target, 7, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value = test_stationarity(train_subDf.target.diff(21).dropna(), 14, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_periodogram(train_subDf.target.diff(21).fillna(0), 365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsplot(train_subDf.target.diff(16).fillna(0), lags=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec = sm.tsa.seasonal_decompose(train_subDf.logSales,period = 365, model = 'additive')\n",
    "dec.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subDf['seasonal'] = dec.seasonal\n",
    "train_subDf.seasonal = train_subDf.seasonal / max(train_subDf.seasonal)\n",
    "train_subDf['res_seasonalAnalysis'] = train_subDf.logSales - dec.seasonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subDf.seasonal.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_periodogram(train_subDf.res_seasonalAnalysis, 365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value = test_stationarity(train_subDf.res_seasonalAnalysis, 14, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LGBM fit\n",
    "- doesn't really matter if t-1 or t-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcLossLGBM(pred, y):\n",
    "    logPred = np.reshape(pred, (pred.shape[0],1))\n",
    "    a = np.exp(logPred) -1\n",
    "\n",
    "    logy = np.reshape(y, (pred.shape[0],1))\n",
    "    y = np.exp(logy) -1\n",
    "\n",
    "    rmsleTrain = np.sqrt(np.mean((a-y)**2))\n",
    "    return rmsleTrain\n",
    "def mse(pred,y):\n",
    "    return np.sqrt(np.mean((pred-y)**2))\n",
    "def calcLossLGBMArima1(pred, sales, arima, predictDiff):\n",
    "    if predictDiff:\n",
    "        pred = np.reshape(pred, (pred.shape[0],1)) + np.reshape(arima, (pred.shape[0],1))\n",
    "    a = np.exp(pred) -1\n",
    "\n",
    "    if (a < 0).any():\n",
    "        a = np.clip(a, 0, 1e20)\n",
    "    rmsleTrain = np.sqrt(mean_squared_log_error(a,sales))\n",
    "    return rmsleTrain\n",
    "def calcLossLGBMArima2(pred, y, arima, predictDiff):\n",
    "    if predictDiff:\n",
    "        pred = np.reshape(pred, (pred.shape[0],1)) + np.reshape(arima, (pred.shape[0],1))\n",
    "        y = np.reshape(y, (pred.shape[0],1)) + np.reshape(arima, (pred.shape[0],1))\n",
    "    a = np.exp(pred) -1\n",
    "    y = np.exp(y) -1\n",
    "\n",
    "    if (a < 0).any():\n",
    "        a = np.clip(a, 0, 1e20)\n",
    "    rmsleTrain = np.sqrt(mean_squared_log_error(a,y))\n",
    "    return rmsleTrain\n",
    "def plotLGBM(i, logTransform, salesDomain, pred, y, base):\n",
    "    if salesDomain:\n",
    "        pred = np.reshape(pred, (pred.shape[0],1)) + np.reshape(base, (pred.shape[0],1))\n",
    "        y = np.reshape(y, (pred.shape[0],1)) + np.reshape(base, (pred.shape[0],1))\n",
    "    if logTransform:\n",
    "        a = np.exp(pred[i:i+16])-1\n",
    "        y = np.exp(y[i:i+16]) -1\n",
    "        arima = np.exp(base[i:i+16]) - 1\n",
    "    else:\n",
    "        a = (pred[i:i+16])\n",
    "        y = (y[i:i+16])\n",
    "        arima = base[i:i+16]\n",
    "    x = range(len(a))   \n",
    "\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=1, figsize=(8, 6))\n",
    "    axs.plot(x, y, color='blue',label='Original')\n",
    "    axs.plot(x, a, color='red',label='pred')\n",
    "    axs.plot(x, arima, color='orange',label='arima')\n",
    "    axs.set_title('index: '+str(i))\n",
    "    fig.subplots_adjust(hspace=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # compare against base lgbm that just predicts always t+16\n",
    "\n",
    "# Set parameters for LGBM model\n",
    "params = {\n",
    "    'boosting':'gbdt',#'gbdt', #'rf' #'dart'\n",
    "    'objective': 'regression',  # Assuming you're doing regression\n",
    "    'metric': 'mse',  # Mean squared error\n",
    "    'num_leaves': 20,\n",
    "    #'lambda_l1': 0.1,\n",
    "    #'lambda_l2': 0.2,\n",
    "    #'max_depth':10,\n",
    "    #'learning_rate': 0.1,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': -1,\n",
    "    'force_col_wise':True,\n",
    "}   \n",
    "\n",
    "# Train the model\n",
    "gbm = lgb.train(params, lgb.Dataset(X_train, label=y_train), 2000,valid_sets=[\n",
    "    lgb.Dataset(X_test, label=y_test)\n",
    "    #,lgb.Dataset(X_val, label=y_val)\n",
    "    ]#,num_boost_round=100\n",
    ",callbacks=[lgb.early_stopping(stopping_rounds=500)]\n",
    ")  \n",
    "predtrainLGBM = gbm.predict(X_train)\n",
    "predtestLGBM = gbm.predict(X_test)\n",
    "predvalLGBM = gbm.predict(X_pred)\n",
    "\n",
    "#de-zscale\n",
    "predtrainLGBM = (predtrainLGBM * train_bigDf.stdTarget) + train_bigDf.meanTarget\n",
    "predtestLGBM = (predtestLGBM * test_bigDf.stdTarget) + test_bigDf.meanTarget\n",
    "predvalLGBM = (predvalLGBM * val_bigDf.stdTarget) + val_bigDf.meanTarget\n",
    "\n",
    "y_train = (y_train * train_bigDf.stdTarget) + train_bigDf.meanTarget\n",
    "y_test = (y_test * test_bigDf.stdTarget) + test_bigDf.meanTarget\n",
    "\n",
    "#print('arima errors: ', calcLossArima(train_subDf.ref, train_subDf.logSales, logTransform), calcLossArima(test_subDf.ref, test_subDf.logSales, logTransform))#, calcLossArima(val_subDf.ref, val_subDf.logSales, logTransform, baseTest))\n",
    "print('arima errors: ', calcLossArima(train_bigDf.ref, train_bigDf.logSales, logTransform), calcLossArima(test_bigDf.ref, test_bigDf.logSales, logTransform))\n",
    "#print('lgbm errors: ', calcLossLGBM(predtrainLGBM, y_train), calcLossLGBM(predtestLGBM, y_test))#, calcLossLGBM(predvalLGBM, y_val))\n",
    "\n",
    "print('sales errors: ', mse(predtrainLGBM, y_train), mse(predtestLGBM, y_test))\n",
    "print('sales errors: ', calcLossLGBMArima2(predtrainLGBM, y_train, baseTrain, predictDiff), calcLossLGBMArima2(predtestLGBM, y_test, baseTest,predictDiff))#, calcLossLGBMArima2(predvalLGBM, y_val,baseVal))\n",
    "#print('sales errors: ', calcLossLGBMArima1(predtrainLGBM, train_subDf.sales, baseTrain), calcLossLGBMArima1(predtestLGBM, test_subDf.sales, baseTest), calcLossLGBMArima1(predvalLGBM, val_subDf.sales,baseVal))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.isna().sum(),y_test.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predtrainLGBM = (predtrainLGBM * train_bigDf.stdTarget) + train_bigDf.meanTarget\n",
    "predtestLGBM = (predtestLGBM * test_bigDf.stdTarget) + test_bigDf.meanTarget\n",
    "predvalLGBM = (predvalLGBM * val_bigDf.stdTarget) + val_bigDf.meanTarget\n",
    "\n",
    "y_train = (y_train * train_bigDf.stdTarget) + train_bigDf.meanTarget\n",
    "y_test = (y_test * test_bigDf.stdTarget) + test_bigDf.meanTarget\n",
    "\n",
    "#print('arima errors: ', calcLossArima(train_subDf.ref, train_subDf.logSales, logTransform), calcLossArima(test_subDf.ref, test_subDf.logSales, logTransform))#, calcLossArima(val_subDf.ref, val_subDf.logSales, logTransform, baseTest))\n",
    "print('arima errors: ', calcLossArima(train_bigDf.ref, train_bigDf.logSales, logTransform), calcLossArima(test_bigDf.ref, test_bigDf.logSales, logTransform))\n",
    "#print('lgbm errors: ', calcLossLGBM(predtrainLGBM, y_train), calcLossLGBM(predtestLGBM, y_test))#, calcLossLGBM(predvalLGBM, y_val))\n",
    "\n",
    "print('sales errors: ', mse(predtrainLGBM, y_train), mse(predtestLGBM, y_test))\n",
    "print('sales errors: ', calcLossLGBMArima2(predtrainLGBM, y_train, baseTrain, predictDiff), calcLossLGBMArima2(predtestLGBM, y_test, baseTest,predictDiff))#, calcLossLGBMArima2(predvalLGBM, y_val,baseVal))\n",
    "#print('sales errors: ', calcLossLGBMArima1(predtrainLGBM, train_subDf.sales, baseTrain), calcLossLGBMArima1(predtestLGBM, test_subDf.sales, baseTest), calcLossLGBMArima1(predvalLGBM, val_subDf.sales,baseVal))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sales errors:  0.37130709804425716 0.3891904372498804    2000/10\n",
    "#sales errors:  0.40056093868328896 0.3921714758330452    2000/5\n",
    "#sales errors:  0.3770976302581994 0.3904403544628816\n",
    "#sales errors:  0.35511432092326384 0.387162551419242     2000/20  (1993)\n",
    "#sales errors:  0.3618022418342807 0.3882029763030535     2000/30 (783) -> 0.42\n",
    "\n",
    "print('sales errors: ', mse(predtrainLGBM, y_train), mse(predtestLGBM, y_test))#, calcLossLGBMArima2(predvalLGBM, y_val,baseVal))\n",
    "#sales errors:  0.43143281396561584 0.41506468712898    500   / 5 max leaves\n",
    "#sales errors:  0.41479865940305194 0.4113797188055782  1000\n",
    "#sales errors:  0.400785079081174 0.4055445445983433    2000(1979)\n",
    "#sales errors:  0.38301045799101135 0.4009700741045326  2000/10 (1299) -> 0.458 in pred set\n",
    "#sales errors:  0.38265366510147164 0.4025353319230588  2000/15 (795)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sales errors:  0.37130709804425716 0.3891904372498804    2000/10\n",
    "#sales errors:  0.40056093868328896 0.3921714758330452    2000/5\n",
    "sales errors:  0.3770976302581994 0.3904403544628816\n",
    "\n",
    "print('sales errors: ', mse(predtrainLGBM, y_train), mse(predtestLGBM, y_test))#, calcLossLGBMArima2(predvalLGBM, y_val,baseVal))\n",
    "#sales errors:  0.43143281396561584 0.41506468712898    500   / 5 max leaves\n",
    "#sales errors:  0.41479865940305194 0.4113797188055782  1000\n",
    "#sales errors:  0.400785079081174 0.4055445445983433    2000(1979)\n",
    "#sales errors:  0.38301045799101135 0.4009700741045326  2000/10 (1299) -> 0.458 in pred set\n",
    "#sales errors:  0.38265366510147164 0.4025353319230588  2000/15 (795)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((np.exp(predtrainLGBM)-1)<0).any(), ((np.exp(y_train)-1)<0).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(y_train)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predtrainLGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bigDf['LGBMpred'] = predtrainLGBM\n",
    "test_bigDf['LGBMpred'] = predtestLGBM\n",
    "val_bigDf['LGBMpred'] = predvalLGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_bigDf['sales'] = np.exp(val_bigDf.LGBMpred+ val_bigDf.salesArima)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sales errors:  0.3978436189821838 0.4313888853367467\n",
    "print('sales errors: ', calcLossLGBMArima2(predtrainLGBM, y_train, baseTrain), calcLossLGBMArima2(predtestLGBM, y_test, baseTest))#, calcLossLGBMArima2(predvalLGBM, y_val,baseVal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    salesDomaine = True\n",
    "    logTransformLoc = True\n",
    "    plotLGBM(i*16, logTransform, salesDomaine,predtrainLGBM, y_train, baseTrain)\n",
    "    plotLGBM(i*16, logTransform, salesDomaine,predtestLGBM, y_test, baseTest)\n",
    "    plotLGBM(i*16, logTransform, salesDomaine,predvalLGBM, y_val, baseVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_df = (\n",
    "    pd.DataFrame({\n",
    "        'feature_name': gbm.feature_name(),\n",
    "        'importance_gain': gbm.feature_importance(importance_type='gain'),\n",
    "        'importance_split': gbm.feature_importance(importance_type='split'),\n",
    "    })\n",
    "    .sort_values('importance_gain', ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "pd.set_option('Display.max_rows', None)\n",
    "importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample_weights = np.reshape((train_subDf.onpromotion * train_subDf.onpromotion.std()) + train_subDf.onpromotion.mean()+ 1, (-1,1)) # öffset of 1 to not have 0 weight\n",
    "sample_weights = np.reshape((train_bigDf.onpromotion * train_bigDf.onpromotion.std()) + train_bigDf.onpromotion.mean()+ 1, (-1,1))\n",
    "sample_weights = sample_weights/ max(sample_weights)\n",
    "\n",
    "nsplits=5\n",
    "num_iter=500\n",
    "\n",
    "kf = KFold(n_splits=nsplits, shuffle=False)# , random_state=42) #random doesn't help\n",
    "splits = kf.split(X_train,y_train)\n",
    "\n",
    "cv_results = lgb.cv(\n",
    "    params,\n",
    "    lgb.Dataset(X_train, label=y_train\n",
    "    , weight=sample_weights\n",
    "    ),\n",
    "    num_boost_round=num_iter,\n",
    "    folds=splits,\n",
    "    stratified=False,  # Set to True for stratified sampling in classification\n",
    "    #early_stopping_rounds=50,  # Stop if score doesn't improve for 50 rounds\n",
    "    metrics=['mse'],  # Evaluation metrics to track\n",
    "    seed=42,  # Set a seed for reproducibility\n",
    "    return_cvbooster=True\n",
    "    ,callbacks=[lgb.early_stopping(stopping_rounds=400)]\n",
    ")\n",
    "\n",
    "lenIter = len(cv_results['valid l2-mean'])\n",
    "print(cv_results['valid l2-mean'][lenIter-1], cv_results['valid l2-stdv'][lenIter-1])\n",
    "for i in range(nsplits):\n",
    "    predtrainLGBM = cv_results['cvbooster'].boosters[i].predict(X_train)\n",
    "    predtestLGBM = cv_results['cvbooster'].boosters[i].predict(X_test)\n",
    "    #predvalLGBM = cv_results['cvbooster'].boosters[i].predict(X_val)\n",
    "\n",
    "    print('sales errors: ', calcLossLGBMArima2(predtrainLGBM, y_train, baseTrain), calcLossLGBMArima2(predtestLGBM, y_test, baseTest))#, calcLossLGBMArima2(predvalLGBM, y_val,baseVal))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(nsplits):\n",
    "    predtrainLGBM = cv_results['cvbooster'].boosters[i].predict(X_train)\n",
    "    predtestLGBM = cv_results['cvbooster'].boosters[i].predict(X_test)\n",
    "\n",
    "    print('sales errors: ', calcLossLGBMArima2(predtrainLGBM, y_train, baseTrain), calcLossLGBMArima2(predtestLGBM, y_test, baseTest))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multisetp pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = modifiedDf[allF]\n",
    "y = modifiedDf['target']\n",
    "ref = modifiedDf['ref']\n",
    "n_sequence = 16\n",
    "\n",
    "\n",
    "X0 = np.lib.stride_tricks.sliding_window_view(X, (n_sequence, len(allF)))[:,0,:,:]\n",
    "y0 = np.lib.stride_tricks.sliding_window_view(y, (n_sequence))#[:,0,:,:]\n",
    "ref0 = np.lib.stride_tricks.sliding_window_view(ref, (n_sequence))#[:,0,:,:]\n",
    "\n",
    "trainEnd = X_train.shape[0]\n",
    "testEnd = X0.shape[0]-1\n",
    "predEnd = X0.shape[0]\n",
    "\n",
    "X_trainM= X0[0:trainEnd,:,:]\n",
    "X_testM = X0[trainEnd:testEnd,:,:]\n",
    "X_valM =  X0[testEnd:predEnd,:,:]\n",
    "y_trainM= y0[0:trainEnd,:]\n",
    "y_testM = y0[trainEnd:testEnd,:]\n",
    "y_valM =  y0[testEnd:predEnd,:]\n",
    "base_trainM= ref0[0:trainEnd,:]\n",
    "base_testM = ref0[trainEnd:testEnd,:]\n",
    "base_valM =  ref0[testEnd:predEnd,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arima                   arima errors:  0.6413825470701051  0.5897267038139773  0.5635773590686141\n",
    "# t-16 single prediction: sales errors:  0.5693091354085361  0.5790932252858579  0.5789745678057521\n",
    "# model for every timestamp:            (0.4886254287678627, 0.5553120246259722, 0.5473653335392659)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcMultiLossLGBMArima2(pred, y, arima):\n",
    "    logPred = np.reshape(pred, (pred.shape)) + np.reshape(arima, (pred.shape))\n",
    "    a = np.exp(logPred) -1\n",
    "\n",
    "    logy = np.reshape(y, (pred.shape)) + np.reshape(arima, (pred.shape))\n",
    "    y = np.exp(logy) -1\n",
    "\n",
    "    if (a < 0).any():\n",
    "        a = np.clip(a, 0, 1e20)\n",
    "    rmsleTrain = np.sqrt(mean_squared_log_error(a,y))\n",
    "    return rmsleTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'objective': 'regression',  # Assuming you're doing regression\n",
    "    'metric': 'mse',  # Mean squared error\n",
    "    'num_leaves': 5,\n",
    "    'learning_rate': 0.04,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': -1\n",
    "}   \n",
    "\n",
    "\n",
    "X_train0 = np.reshape(X_trainM, (X_trainM.shape[0], -1))\n",
    "X_test0 = np.reshape(X_testM, (X_testM.shape[0], -1)) \n",
    "X_val0 = np.reshape(X_valM, (X_valM.shape[0], -1)) \n",
    "\n",
    "# Train the model\n",
    "num_round = 300  \n",
    "\n",
    "gbms = [lgb.train(params, lgb.Dataset(X_train0, label=y_trainM[:,i]),num_round, valid_sets=[lgb.Dataset(X_test0, label=y_testM[:,i])]\n",
    "    ,callbacks=[lgb.early_stopping(stopping_rounds=100)]\n",
    "    ) for i in range(n_sequence)]\n",
    "\n",
    "predtrainLGBM = [gbm.predict(X_train0, num_iteration=gbm.best_iteration) for i,gbm in enumerate(gbms)]\n",
    "predtestLGBM = [gbm.predict(X_test0, num_iteration=gbm.best_iteration) for i,gbm in enumerate(gbms)]\n",
    "predvalLGBM = [gbm.predict(X_val0, num_iteration=gbm.best_iteration) for i,gbm in enumerate(gbms)]\n",
    "for i in range(16):\n",
    "    print(i,'lgbm errors: ', calcLossLGBM(predtrainLGBM[i], y_trainM[:,i]), calcLossLGBM(predtestLGBM[i], y_testM[:,i]), calcLossLGBM(predvalLGBM[i], y_valM[:,i]))\n",
    "for i in range(16):\n",
    "    print(i,'sales errors: ', calcLossLGBMArima2(predtrainLGBM[i], y_trainM[:,i], base_trainM[:,i]), calcLossLGBMArima2(predtestLGBM[i], y_testM[:,i], base_testM[:,i]), calcLossLGBMArima2(predvalLGBM[i], y_valM[:,i],base_valM[:,i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calcMultiLossLGBMArima2(np.column_stack(predtrainLGBM), y_trainM, base_trainM),calcMultiLossLGBMArima2(np.column_stack(predtestLGBM), y_testM, base_testM),calcMultiLossLGBMArima2(np.column_stack(predvalLGBM), y_valM, base_valM) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" create training data based on lagged features not 2 sequences \"\"\"\n",
    "trainF = [\n",
    "       #'store_nbr', 'family', \n",
    "       #'sales', \n",
    "       'onpromotion',# 'dataT',\n",
    "       #'city', 'state', 'type', 'cluster', \n",
    "       'dcoilwtico', \n",
    "       'holidayType',\n",
    "       'description', \n",
    "       'transferred', \n",
    "       #'transactions', \n",
    "       'store_closed']\n",
    "timeF = [\n",
    "       'linear_time', 'day_of_year', 'day_of_year_f12_0', 'day_of_year_f104_0','day_of_year_f24_0',  'day_of_year_f52_0',\n",
    "       'day_of_year_f12_180', 'day_of_year_f104_180','day_of_year_f24_180','day_of_year_f52_180', \n",
    "       'weekday', 'month'\n",
    "       ]\n",
    "\n",
    "initial_lag = 16 # 16 = independent from previous predictions\n",
    "lags = 10 #21\n",
    "rolling = [7]\n",
    "\n",
    "# Date string\n",
    "date_string_test = \"2017-07-01\"\n",
    "\n",
    "data0 = data#.loc[(data.date > \"2015-07-01\")]\n",
    "\n",
    "predictions = []\n",
    "log={}\n",
    "\n",
    "\n",
    "for familyId in [0]:#[6]: #data.family.unique():\n",
    "       # start with only some families\n",
    "\n",
    "       print(familyId)\n",
    "       familyDf = data0.loc[data0.family==familyId]  \n",
    "       log1={}\n",
    "\n",
    "       for storeId in data.store_nbr.unique():\n",
    "            #print('store',storeId)\n",
    "            storeDf = familyDf.loc[(familyDf.store_nbr == storeId)] \n",
    "\n",
    "            # ln tranformation\n",
    "            storeDf.loc[:,['logSales']] = np.log(storeDf.sales + 1)\n",
    "\n",
    "            f = 'logSales'#'transactions'# 'salesOrig' #target\n",
    "            y_trainArima = storeDf.loc[storeDf.dataT == 'train'][[f,'date']]\n",
    "            y_trainArima = y_trainArima.set_index('date')\n",
    "            y_trainArima.index = pd.DatetimeIndex(y_trainArima.index).to_period('D')\n",
    "            model = ARIMA(y_trainArima, order=(5, 1, 5))\n",
    "            model_fit = model.fit()\n",
    "            \n",
    "            arimaSalesLogT = np.array(model_fit.fittedvalues)\n",
    "            arimaSalesLogT = np.concatenate((arimaSalesLogT, model_fit.forecast(steps=(16))), axis=0)\n",
    "            storeDf.loc[:,['salesArima']] = arimaSalesLogT\n",
    "            storeDf.loc[:,['ref']] = storeDf['salesArima']\n",
    "            storeDf.loc[:,['target']] = storeDf['logSales'] - storeDf['ref']\n",
    "\n",
    "            # lag features / how many past datapoints are we tain\n",
    "            featuresForLag = ['target']\n",
    "            lagF = []#trainF\n",
    "            for i in range(lags):\n",
    "                   lag = i+1+initial_lag\n",
    "                   newF = [featuresForLag[j] + '_lag' + str(lag) for j in range(len(featuresForLag))]\n",
    "                   lagF = lagF + newF\n",
    "                   storeDf.loc[:,newF] = storeDf[featuresForLag].shift(lag).to_numpy()\n",
    "            \n",
    "            # rolling features\n",
    "            rollingF = []\n",
    "            for rol in rolling:\n",
    "                   for i in range(len(lagF)):\n",
    "                          #if 'sales_t-16'  in lagF[i]:\n",
    "                          if 'target'  in lagF[i]:\n",
    "                                 fm = lagF[i]+'_rollingM' + str(rol)\n",
    "                                 fs = lagF[i]+'_rollingS' + str(rol)\n",
    "                                 rollingF.append(fm)\n",
    "                                 rollingF.append(fs)\n",
    "                                 storeDf.loc[:,[fm]] = storeDf[lagF[i]].rolling(rol).mean()#.copy()\n",
    "                                 storeDf.loc[:,[fs]] = storeDf[lagF[i]].rolling(rol).std()#.copy()\n",
    "\n",
    "            allF = lagF + rollingF + timeF + trainF\n",
    "\n",
    "            # we get a matrix that predicts only 1 timestamp -> stride it\n",
    "            storeDf = storeDf.iloc[lags+initial_lag+max(rolling)+1:storeDf.shape[0]]\n",
    "            train_subDf = storeDf.loc[storeDf.date < date_string_test]\n",
    "            test_subDf  = storeDf.loc[(storeDf.date >= date_string_test) & (storeDf.dataT =='train')]\n",
    "            pred_subDf   = storeDf.loc[storeDf.dataT =='test']\n",
    "\n",
    "\n",
    "            targetF = 'target'\n",
    "            baseTrain = train_subDf[['ref']].to_numpy()\n",
    "            baseTest  = test_subDf[['ref']].to_numpy()\n",
    "            \n",
    "            X_train = train_subDf[allF].to_numpy()\n",
    "            y_train = train_subDf[[targetF]].to_numpy()\n",
    "            X_test  =  test_subDf[allF].to_numpy()\n",
    "            y_test  =  test_subDf[[targetF]].to_numpy()  \n",
    "\n",
    "\n",
    "            baseSub = pred_subDf[['ref']].to_numpy()\n",
    "            X_sub   = pred_subDf[allF].to_numpy()\n",
    "\n",
    "            # Set parameters for LGBM model\n",
    "            params = {\n",
    "                'boosting':'gbdt',#'gbdt', #'rf' #'dart'\n",
    "                'objective': 'regression',  # Assuming you're doing regression\n",
    "                'metric': 'mse',  # Mean squared error\n",
    "                'num_leaves': 15,\n",
    "                'verbose': -1,\n",
    "                'force_col_wise':True,\n",
    "                'num_iterations':200\n",
    "            }   \n",
    "            \n",
    "            # Train the model\n",
    "            gbm = lgb.train(params, lgb.Dataset(X_train, label=y_train), valid_sets=[\n",
    "                lgb.Dataset(X_test, label=y_test)\n",
    "                #,lgb.Dataset(X_val, label=y_val)\n",
    "                ]\n",
    "            ,callbacks=[lgb.early_stopping(stopping_rounds=30)]\n",
    "            )  \n",
    "            predtrainLGBM = gbm.predict(X_train)\n",
    "            predtestLGBM = gbm.predict(X_test)\n",
    "            logTransform=True\n",
    "            predictDiff=True\n",
    "            \n",
    "            lossTrain = calcLossLGBM(predtrainLGBM, y_train, logTransform, predictDiff, base=baseTrain)\n",
    "            lossTest = calcLossLGBM(predtestLGBM, y_test, logTransform, predictDiff, baseTest)\n",
    "            print('errors: ', lossTrain, lossTest)\n",
    "\n",
    "            log2 = {}\n",
    "            log2['testL'] = lossTest\n",
    "            log2['trainL'] = lossTrain\n",
    "            log1[storeId] = log2\n",
    "\n",
    "            predvalLGBM = gbm.predict(X_sub)\n",
    "            pred = np.reshape(predvalLGBM, baseSub.shape) + baseSub\n",
    "            a = np.exp(pred)-1\n",
    "\n",
    "            pred_subDf.loc[:,['sales']] = a\n",
    "            predictions.append(pred_subDf[['id','sales']])\n",
    "\n",
    "       print(log1)\n",
    "       log[familyId] = log1\n",
    "\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predDf2 = pd.concat(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.read_csv('simpleArima_logT.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predDf = predDf2.set_index('id')\n",
    "b = a.set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = pd.merge(predDf, b, on='id', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c['sales'] = c['sales_x'].fillna(c['sales_y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = c.drop(['sales_x','sales_y'], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.to_csv('arima_prediction_and_updatedFam0WithLGBM_useFullData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.13 ('storeSales')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "17f27e0cbed1f29822e509a94c958ed0e80b7d9abe162097755686a614de4732"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
