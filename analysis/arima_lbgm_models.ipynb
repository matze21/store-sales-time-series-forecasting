{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import plotly.offline as pyo\n",
    "from plotly import subplots\n",
    "import plotly.graph_objects as go\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.tsa.api as smt\n",
    "\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from tensorflow.keras.models import Sequential\n",
    "#from tensorflow.keras.layers import LSTM, Dense,Input,concatenate\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import lightgbm as lgb\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "from baseFunctions import *\n",
    "from data_helpers import processData6, featureEngineering, getSequencesFast, removeOutliers, create_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, propDicts, flippedPropDicts = processData6()\n",
    "data, timeFeatures = featureEngineering(data,splits=[2,2,2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addSeasonality(dt, dec, outputLength):\n",
    "    first365Vals = dec.seasonal[0:dt] \n",
    "    first365Vals = first365Vals / max(first365Vals) \n",
    "\n",
    "    folds = int(outputLength/ dt)\n",
    "    rest = (outputLength % dt) \n",
    "\n",
    "    seasonalVals = np.ones((outputLength)) * np.nan\n",
    "    for i in range(folds):\n",
    "        firstId = i*dt\n",
    "        second = firstId + dt\n",
    "        seasonalVals[firstId:second] = first365Vals\n",
    "    seasonalVals[second:second+rest] = first365Vals[0:rest]\n",
    "\n",
    "    return seasonalVals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" create training data based on lagged features not 2 sequences \"\"\"\n",
    "trainF = [\n",
    "       #'store_nbr', 'family', \n",
    "       #'sales', \n",
    "       'onpromotion',# 'dataT',\n",
    "       #'city', 'state', 'type', 'cluster', \n",
    "       'dcoilwtico', \n",
    "       'holidayType',\n",
    "       'description', \n",
    "       'transferred', \n",
    "       #'transactions', \n",
    "       'store_closed']\n",
    "timeF = [\n",
    "       'linear_time', 'day_of_year', 'day_of_year_f12_0', 'day_of_year_f104_0','day_of_year_f24_0',  'day_of_year_f52_0',\n",
    "       'day_of_year_f12_180', 'day_of_year_f104_180','day_of_year_f24_180','day_of_year_f52_180', \n",
    "       'weekday', 'month'\n",
    "       ]\n",
    "\n",
    "initial_lag = 16 # 16 = independent from previous predictions\n",
    "lags = 21\n",
    "rolling = [7,14]\n",
    "\n",
    "# Date string\n",
    "date_string_val = \"2017-07-01\"#\"2017-05-01\"\n",
    "date_string_test = \"2016-09-01\"\n",
    "\n",
    "\n",
    "for familyId in [0]:#[6]: #data.family.unique():\n",
    "       # start with only some families\n",
    "       if familyId > 8:\n",
    "          continue\n",
    "\n",
    "       print(familyId)\n",
    "       familyDf = data.loc[data.family==familyId]  \n",
    "\n",
    "       for storeId in [1]:#[41]: #data.store_nbr.unique():\n",
    "              print('store',storeId)\n",
    "              storeDf = familyDf.loc[(familyDf.store_nbr == storeId)] \n",
    "              storeDf = storeDf.loc[(storeDf.dataT == 'train')]# & (storeDf.date > \"2015-07-01\")]\n",
    "\n",
    "              mask = (storeDf.date >= date_string_val)\n",
    "              storeDf.loc[mask,['dataT']] = 'val'\n",
    "\n",
    "              # ln tranformation\n",
    "              storeDf.loc[:,['logSales']] = np.log(storeDf.sales + 1)\n",
    "\n",
    "              f = 'logSales'#'transactions'# 'salesOrig' #target\n",
    "              y_trainArima = storeDf.loc[storeDf.dataT == 'train'][[f,'date']]\n",
    "              y_trainArima = y_trainArima.set_index('date')\n",
    "              y_trainArima.index = pd.DatetimeIndex(y_trainArima.index).to_period('D')\n",
    "\n",
    "              model = SARIMAX(y_trainArima, order=(3, 1, 3), seasonal_order=(1,1,1,7))\n",
    "              model_fit = model.fit()\n",
    "              storeDf = storeDf.reset_index()\n",
    "              \n",
    "              arimaSalesLogT = np.array(model_fit.fittedvalues)\n",
    "              arimaSalesLogT = np.concatenate((arimaSalesLogT, model_fit.forecast(steps=(storeDf.shape[0]- len(model_fit.fittedvalues)))), axis=0)\n",
    "\n",
    "              storeDf.loc[:,['salesArima']] = arimaSalesLogT\n",
    "\n",
    "              storeDf.loc[:,['ref']] = storeDf['salesArima']\n",
    "              storeDf.loc[:,['target']] = storeDf['logSales'] - storeDf['ref']\n",
    "\n",
    "              # lag features / how many past datapoints are we tain\n",
    "              featuresForLag = ['target']\n",
    "              lagF = []#trainF\n",
    "              for i in range(lags):\n",
    "                     lag = i+1+initial_lag\n",
    "                     newF = [featuresForLag[j] + '_lag' + str(lag) for j in range(len(featuresForLag))]\n",
    "                     lagF = lagF + newF\n",
    "                     storeDf.loc[:,newF] = storeDf[featuresForLag].shift(lag).to_numpy()\n",
    "\n",
    "              featuresForLag2 = ['salesArima']\n",
    "              lagF2 = []#trainF\n",
    "              for i in range(lags):\n",
    "                     lag = i+1\n",
    "                     newF = [featuresForLag2[j] + '_lag' + str(lag) for j in range(len(featuresForLag2))]\n",
    "                     lagF2 = lagF2 + newF\n",
    "                     storeDf.loc[:,newF] = storeDf[featuresForLag2].shift(lag).to_numpy()\n",
    "\n",
    "              lagF = lagF + lagF2\n",
    "\n",
    "              # rolling features\n",
    "              rollingF = []\n",
    "              for rol in rolling:\n",
    "                     for i in range(len(lagF)):\n",
    "                            #if 'sales_t-16'  in lagF[i]:\n",
    "                            if 'target'  in lagF[i]:\n",
    "                                   fm = lagF[i]+'_rollingM' + str(rol)\n",
    "                                   fs = lagF[i]+'_rollingS' + str(rol)\n",
    "                                   rollingF.append(fm)\n",
    "                                   rollingF.append(fs)\n",
    "                                   storeDf.loc[:,[fm]] = storeDf[lagF[i]].rolling(rol).mean()#.copy()\n",
    "                                   storeDf.loc[:,[fs]] = storeDf[lagF[i]].rolling(rol).std()#.copy()\n",
    "\n",
    "\n",
    "              allF = lagF + rollingF + timeF + trainF\n",
    "\n",
    "              # we get a matrix that predicts only 1 timestamp -> stride it\n",
    "              storeDf = storeDf.iloc[lags+initial_lag+max(rolling)+1:-1]\n",
    "\n",
    "              train_subDf = storeDf.loc[storeDf.date < date_string_test]\n",
    "              test_subDf  = storeDf.loc[storeDf.date >= date_string_test]\n",
    "              val_subDf   = storeDf.loc[storeDf.dataT =='val']\n",
    "\n",
    "\n",
    "targetF = 'target'\n",
    "baseTrain = train_subDf[['ref']].to_numpy()\n",
    "baseTest  = test_subDf[['ref']].to_numpy()\n",
    "baseVal   = val_subDf[['ref']].to_numpy()\n",
    "    \n",
    "\n",
    "X_train = train_subDf[allF].to_numpy()\n",
    "y_train = train_subDf[[targetF]].to_numpy()\n",
    "X_test  =  test_subDf[allF].to_numpy()\n",
    "y_test  =  test_subDf[[targetF]].to_numpy()  \n",
    "X_val   =  val_subDf[allF].to_numpy()\n",
    "y_val   =  val_subDf[[targetF]].to_numpy() \n",
    "\n",
    "np.isnan(X_train).any(),np.isnan(X_test).any(),np.isnan(X_val).any(),np.isnan(y_train).any(),np.isnan(y_test).any(),np.isnan(y_val).any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analyze residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value = test_stationarity(train_subDf.target, 7, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value = test_stationarity(train_subDf.target.diff(21).dropna(), 14, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_periodogram(train_subDf.target.diff(21).fillna(0), 365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsplot(train_subDf.target.diff(16).fillna(0), lags=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec = sm.tsa.seasonal_decompose(train_subDf.logSales,period = 365, model = 'additive')\n",
    "dec.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subDf['seasonal'] = dec.seasonal\n",
    "train_subDf.seasonal = train_subDf.seasonal / max(train_subDf.seasonal)\n",
    "train_subDf['res_seasonalAnalysis'] = train_subDf.logSales - dec.seasonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addSeasonality(dt, dec, outputLength):\n",
    "    first365Vals = dec.seasonal[0:dt] \n",
    "    first365Vals = first365Vals / max(first365Vals) \n",
    "\n",
    "    folds = int(outputLength/ dt)\n",
    "    rest = (outputLength % dt) \n",
    "\n",
    "    seasonalVals = np.ones((outputLength)) * np.nan\n",
    "    for i in range(folds):\n",
    "        firstId = i*dt\n",
    "        second = firstId + dt\n",
    "        seasonalVals[firstId:second] = first365Vals\n",
    "    seasonalVals[second:second+rest] = first365Vals[0:rest]\n",
    "\n",
    "    return seasonalVals\n",
    "\n",
    "train_subDf['seasonal2'] = addSeasonality(365, dec, train_subDf.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subDf.seasonal.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_periodogram(train_subDf.res_seasonalAnalysis, 365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value = test_stationarity(train_subDf.res_seasonalAnalysis, 14, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LGBM fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # compare against base lgbm that just predicts always t+16\n",
    "\n",
    "def calcLossLGBM(pred, y, logTransform, predictDiff, base):\n",
    "    if predictDiff:\n",
    "        pred = np.reshape(pred, (pred.shape[0],1)) + base\n",
    "        y = y + base\n",
    "\n",
    "    if logTransform:\n",
    "        a = np.exp(pred) -1\n",
    "        y = np.exp(y) -1 \n",
    "    else:\n",
    "        a = (pred)\n",
    "        y = (y)\n",
    "\n",
    "    if (a < 0).any():\n",
    "        a = np.clip(a, 0, 1e20)\n",
    "    rmsleTrain = np.sqrt(mean_squared_log_error(a,y))\n",
    "    return rmsleTrain\n",
    "def plotLGBM(i, logTransform, pred, y, predictDiff, base):\n",
    "    if predictDiff:\n",
    "        pred = np.reshape(pred, (pred.shape[0],1)) + base\n",
    "        y = y + base\n",
    "    if logTransform:\n",
    "        a = np.exp(pred[i:i+16])\n",
    "        y = np.exp(y[i:i+16])\n",
    "    else:\n",
    "        a = (pred[i:i+16])\n",
    "        y = (y[i:i+16])\n",
    "    x = range(len(a))   \n",
    "\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=1, figsize=(8, 6))\n",
    "    axs.plot(x, y, color='blue',label='Original')\n",
    "    axs.plot(x, a, color='red',label='pred')\n",
    "    axs.set_title('index: '+str(i))\n",
    "    fig.subplots_adjust(hspace=0.5)\n",
    "\n",
    "# Set parameters for LGBM model\n",
    "params = {\n",
    "    'boosting':'gbdt',#'gbdt', #'rf' #'dart'\n",
    "    'objective': 'regression',  # Assuming you're doing regression\n",
    "    'metric': 'mse',  # Mean squared error\n",
    "    'num_leaves': 15,\n",
    "    #'lambda_l1': 0.1,\n",
    "    #'lambda_l2': 0.2,\n",
    "    #'max_depth':10,\n",
    "    #'learning_rate': 0.1,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': -1,\n",
    "    'force_col_wise':True,\n",
    "    'num_iterations':100\n",
    "}   \n",
    "\n",
    "# Train the model\n",
    "gbm = lgb.train(params, lgb.Dataset(X_train, label=y_train), valid_sets=[\n",
    "    lgb.Dataset(X_test, label=y_test)\n",
    "    #,lgb.Dataset(X_val, label=y_val)\n",
    "    ]#,num_boost_round=100\n",
    ",callbacks=[lgb.early_stopping(stopping_rounds=100)]\n",
    ")  \n",
    "predtrainLGBM = gbm.predict(X_train)\n",
    "predtestLGBM = gbm.predict(X_test)\n",
    "predvalLGBM = gbm.predict(X_val)\n",
    "logTransform=True\n",
    "predictDiff=True\n",
    "\n",
    "print('errors: ', calcLossLGBM(predtrainLGBM, y_train, logTransform, predictDiff, base=baseTrain), calcLossLGBM(predtestLGBM, y_test, logTransform, predictDiff, baseTest), calcLossLGBM(predvalLGBM, y_val, logTransform, predictDiff, baseVal))\n",
    "\n",
    "for i in range(1):\n",
    "    plotLGBM(i*16, logTransform, predtrainLGBM, y_train, predictDiff, baseTrain)\n",
    "    plotLGBM(i*16, logTransform, predtestLGBM, y_test, predictDiff, baseTest)\n",
    "    plotLGBM(i*16, logTransform, predvalLGBM, y_val, predictDiff, baseVal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# errors:  0.39217121993164555 0.4369127610627825 0.46647451624311603 initial lag = 0 -> \n",
    "# errors:  0.39779196439513415 0.44181336636642143 0.44775333790913957 initial lag 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = gbm.feature_importance()\n",
    "for name, importance in zip(allF, importances):\n",
    "    print(f'{name}: {importance}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" create training data based on lagged features not 2 sequences \"\"\"\n",
    "trainF = [\n",
    "       #'store_nbr', 'family', \n",
    "       #'sales', \n",
    "       'onpromotion',# 'dataT',\n",
    "       #'city', 'state', 'type', 'cluster', \n",
    "       'dcoilwtico', \n",
    "       'holidayType',\n",
    "       'description', \n",
    "       'transferred', \n",
    "       #'transactions', \n",
    "       'store_closed']\n",
    "timeF = [\n",
    "       'linear_time', 'day_of_year', 'day_of_year_f12_0', 'day_of_year_f104_0','day_of_year_f24_0',  'day_of_year_f52_0',\n",
    "       'day_of_year_f12_180', 'day_of_year_f104_180','day_of_year_f24_180','day_of_year_f52_180', \n",
    "       'weekday', 'month'\n",
    "       ]\n",
    "\n",
    "initial_lag = 16 # 16 = independent from previous predictions\n",
    "lags = 10 #21\n",
    "rolling = [7]\n",
    "\n",
    "# Date string\n",
    "date_string_test = \"2017-07-01\"\n",
    "\n",
    "data0 = data#.loc[(data.date > \"2015-07-01\")]\n",
    "\n",
    "predictions = []\n",
    "log={}\n",
    "\n",
    "\n",
    "for familyId in [0]:#[6]: #data.family.unique():\n",
    "       # start with only some families\n",
    "\n",
    "       print(familyId)\n",
    "       familyDf = data0.loc[data0.family==familyId]  \n",
    "       log1={}\n",
    "\n",
    "       for storeId in data.store_nbr.unique():\n",
    "            #print('store',storeId)\n",
    "            storeDf = familyDf.loc[(familyDf.store_nbr == storeId)] \n",
    "\n",
    "            # ln tranformation\n",
    "            storeDf.loc[:,['logSales']] = np.log(storeDf.sales + 1)\n",
    "\n",
    "            f = 'logSales'#'transactions'# 'salesOrig' #target\n",
    "            y_trainArima = storeDf.loc[storeDf.dataT == 'train'][[f,'date']]\n",
    "            y_trainArima = y_trainArima.set_index('date')\n",
    "            y_trainArima.index = pd.DatetimeIndex(y_trainArima.index).to_period('D')\n",
    "            model = ARIMA(y_trainArima, order=(5, 1, 5))\n",
    "            model_fit = model.fit()\n",
    "            \n",
    "            arimaSalesLogT = np.array(model_fit.fittedvalues)\n",
    "            arimaSalesLogT = np.concatenate((arimaSalesLogT, model_fit.forecast(steps=(16))), axis=0)\n",
    "            storeDf.loc[:,['salesArima']] = arimaSalesLogT\n",
    "            storeDf.loc[:,['ref']] = storeDf['salesArima']\n",
    "            storeDf.loc[:,['target']] = storeDf['logSales'] - storeDf['ref']\n",
    "\n",
    "            # lag features / how many past datapoints are we tain\n",
    "            featuresForLag = ['target']\n",
    "            lagF = []#trainF\n",
    "            for i in range(lags):\n",
    "                   lag = i+1+initial_lag\n",
    "                   newF = [featuresForLag[j] + '_lag' + str(lag) for j in range(len(featuresForLag))]\n",
    "                   lagF = lagF + newF\n",
    "                   storeDf.loc[:,newF] = storeDf[featuresForLag].shift(lag).to_numpy()\n",
    "            \n",
    "            # rolling features\n",
    "            rollingF = []\n",
    "            for rol in rolling:\n",
    "                   for i in range(len(lagF)):\n",
    "                          #if 'sales_t-16'  in lagF[i]:\n",
    "                          if 'target'  in lagF[i]:\n",
    "                                 fm = lagF[i]+'_rollingM' + str(rol)\n",
    "                                 fs = lagF[i]+'_rollingS' + str(rol)\n",
    "                                 rollingF.append(fm)\n",
    "                                 rollingF.append(fs)\n",
    "                                 storeDf.loc[:,[fm]] = storeDf[lagF[i]].rolling(rol).mean()#.copy()\n",
    "                                 storeDf.loc[:,[fs]] = storeDf[lagF[i]].rolling(rol).std()#.copy()\n",
    "\n",
    "            allF = lagF + rollingF + timeF + trainF\n",
    "\n",
    "            # we get a matrix that predicts only 1 timestamp -> stride it\n",
    "            storeDf = storeDf.iloc[lags+initial_lag+max(rolling)+1:storeDf.shape[0]]\n",
    "            train_subDf = storeDf.loc[storeDf.date < date_string_test]\n",
    "            test_subDf  = storeDf.loc[(storeDf.date >= date_string_test) & (storeDf.dataT =='train')]\n",
    "            pred_subDf   = storeDf.loc[storeDf.dataT =='test']\n",
    "\n",
    "\n",
    "            targetF = 'target'\n",
    "            baseTrain = train_subDf[['ref']].to_numpy()\n",
    "            baseTest  = test_subDf[['ref']].to_numpy()\n",
    "            \n",
    "            X_train = train_subDf[allF].to_numpy()\n",
    "            y_train = train_subDf[[targetF]].to_numpy()\n",
    "            X_test  =  test_subDf[allF].to_numpy()\n",
    "            y_test  =  test_subDf[[targetF]].to_numpy()  \n",
    "\n",
    "\n",
    "            baseSub = pred_subDf[['ref']].to_numpy()\n",
    "            X_sub   = pred_subDf[allF].to_numpy()\n",
    "\n",
    "            # Set parameters for LGBM model\n",
    "            params = {\n",
    "                'boosting':'gbdt',#'gbdt', #'rf' #'dart'\n",
    "                'objective': 'regression',  # Assuming you're doing regression\n",
    "                'metric': 'mse',  # Mean squared error\n",
    "                'num_leaves': 15,\n",
    "                'verbose': -1,\n",
    "                'force_col_wise':True,\n",
    "                'num_iterations':200\n",
    "            }   \n",
    "            \n",
    "            # Train the model\n",
    "            gbm = lgb.train(params, lgb.Dataset(X_train, label=y_train), valid_sets=[\n",
    "                lgb.Dataset(X_test, label=y_test)\n",
    "                #,lgb.Dataset(X_val, label=y_val)\n",
    "                ]\n",
    "            ,callbacks=[lgb.early_stopping(stopping_rounds=30)]\n",
    "            )  \n",
    "            predtrainLGBM = gbm.predict(X_train)\n",
    "            predtestLGBM = gbm.predict(X_test)\n",
    "            logTransform=True\n",
    "            predictDiff=True\n",
    "            \n",
    "            lossTrain = calcLossLGBM(predtrainLGBM, y_train, logTransform, predictDiff, base=baseTrain)\n",
    "            lossTest = calcLossLGBM(predtestLGBM, y_test, logTransform, predictDiff, baseTest)\n",
    "            print('errors: ', lossTrain, lossTest)\n",
    "\n",
    "            log2 = {}\n",
    "            log2['testL'] = lossTest\n",
    "            log2['trainL'] = lossTrain\n",
    "            log1[storeId] = log2\n",
    "\n",
    "            predvalLGBM = gbm.predict(X_sub)\n",
    "            pred = np.reshape(predvalLGBM, baseSub.shape) + baseSub\n",
    "            a = np.exp(pred)-1\n",
    "\n",
    "            pred_subDf.loc[:,['sales']] = a\n",
    "            predictions.append(pred_subDf[['id','sales']])\n",
    "\n",
    "       print(log1)\n",
    "       log[familyId] = log1\n",
    "\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predDf2 = pd.concat(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.read_csv('simpleArima_logT.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predDf = predDf2.set_index('id')\n",
    "b = a.set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = pd.merge(predDf, b, on='id', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c['sales'] = c['sales_x'].fillna(c['sales_y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = c.drop(['sales_x','sales_y'], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.to_csv('arima_prediction_and_updatedFam0WithLGBM_useFullData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.13 ('storeSales')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "17f27e0cbed1f29822e509a94c958ed0e80b7d9abe162097755686a614de4732"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
