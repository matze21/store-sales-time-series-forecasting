{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import plotly.offline as pyo\n",
    "from plotly import subplots\n",
    "import plotly.graph_objects as go\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.tsa.api as smt\n",
    "\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from tensorflow.keras.models import Sequential\n",
    "#from tensorflow.keras.layers import LSTM, Dense,Input,concatenate\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import lightgbm as lgb\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from baseFunctions import *\n",
    "from data_helpers import processData6, featureEngineering, getSequencesFast, removeOutliers, create_sequences, processAllData\n",
    "\n",
    "#import dask.dataframe as dd\n",
    "import plotly.express as px\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, propDicts, flippedPropDicts = processData6()\n",
    "data, timeFeatures = featureEngineering(data,splits=[2,2,2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_lag = 16 # 16 = independent from previous predictions\n",
    "targetLags = [1,2,3,4,5,6,7]\n",
    "featureLags = [1,2,3,4,5,6,7]\n",
    "rolling = [7,21]\n",
    "\n",
    "data, addedF = processAllData(data, targetLags, featureLags, rolling, initial_lag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addedF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" define features and test/train partition \"\"\"\n",
    "date_string_test = \"2017-08-01\"\n",
    "\n",
    "train_bigDf = data.loc[(data.date < date_string_test) & (data.date > \"2015-05-01\") & (data.dataT =='train')]\n",
    "test_bigDf  = data.loc[(data.date >= date_string_test) & (data.dataT !='test')]\n",
    "val_bigDf   = data.loc[ data.dataT =='test']\n",
    "\n",
    "trainF = [\n",
    "       'store_nbr', 'family', \n",
    "       #'sales', \n",
    "       'onpromotion',# 'dataT',\n",
    "       'city', 'state', 'type', 'cluster', \n",
    "       'dcoilwtico', \n",
    "       'holidayType',\n",
    "       'description', \n",
    "       'transferred', \n",
    "       #'transactions', \n",
    "       'store_closed']\n",
    "timeF = [\n",
    "       'linear_time', 'day_of_year', 'day_of_year_f12_0', 'day_of_year_f104_0','day_of_year_f24_0',  'day_of_year_f52_0',\n",
    "       'day_of_year_f12_180', 'day_of_year_f104_180','day_of_year_f24_180','day_of_year_f52_180', \n",
    "       'weekday', 'month'\n",
    "       ]\n",
    "\n",
    "allF = addedF + trainF + timeF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" create lag 16 dataframe \"\"\"\n",
    "predictDiff16 = True\n",
    "targetF, refF = 'target', 'ref'\n",
    "logTransform16 = True\n",
    "X_train16, X_test16, X_pred16 = train_bigDf[allF],test_bigDf[allF],val_bigDf[allF]\n",
    "y_train16, y_test16 = train_bigDf[targetF], test_bigDf[targetF]\n",
    "baseTrain16, baseTest16, basePred16 = train_bigDf[refF], test_bigDf[refF],val_bigDf[refF]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSequencePerGroup(group):\n",
    "    X = group[allF]\n",
    "    y = group['target']\n",
    "    ref = group['ref']\n",
    "    n_sequence = 16\n",
    "\n",
    "    X0 = np.lib.stride_tricks.sliding_window_view(X, (n_sequence, len(allF)))[:,0,:,:]\n",
    "    y0 = np.lib.stride_tricks.sliding_window_view(y, (n_sequence))#[:,0,:,:]\n",
    "    ref0 = np.lib.stride_tricks.sliding_window_view(ref, (n_sequence))#[:,0,:,:]\n",
    "\n",
    "    trainEnd = X0.shape[0] - 17\n",
    "    testEnd = X0.shape[0]-1\n",
    "    predEnd = X0.shape[0]\n",
    "\n",
    "    X_trainM= X0[0:trainEnd,:,:]\n",
    "    X_testM = X0[trainEnd:testEnd,:,:]\n",
    "    X_valM =  X0[testEnd:predEnd,:,:]\n",
    "    y_trainM= y0[0:trainEnd,:]\n",
    "    y_testM = y0[trainEnd:testEnd,:]\n",
    "    y_valM =  y0[testEnd:predEnd,:]\n",
    "    base_trainM= ref0[0:trainEnd,:]\n",
    "    base_testM = ref0[trainEnd:testEnd,:]\n",
    "    base_valM =  ref0[testEnd:predEnd,:]\n",
    "\n",
    "    return X_trainM, X_testM, X_valM, y_trainM, y_testM, y_valM, base_trainM, base_testM, base_valM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dd.from_pandas(data, npartitions=4)  # Adjust the number of partitions as needed\n",
    "\n",
    "# Step 2: Group the Dask DataFrame by the 'time_series_id' column\n",
    "grouped = ddf.groupby('store_nbr','family')\n",
    "\n",
    "# Step 3: Create a custom function to generate sequences\n",
    "def create_sequences(group, seq_length=16):\n",
    "    group = group.sort_values('date')  # Ensure the group is sorted by timestamp\n",
    "    sequences = [group.iloc[i:i+seq_length] for i in range(0, len(group)-seq_length)]\n",
    "    return dd.concat(sequences)\n",
    "\n",
    "# Step 4: Apply the create_sequences function to each group\n",
    "#sequences = grouped.apply(create_sequences, meta=ddf, seq_length=16)\n",
    "sequences = ddf.loc[(ddf.store_nbr==1) & (ddf.family<=2)].groupby('store_nbr','family').apply(create_sequences, meta=ddf.loc[(ddf.store_nbr==1) & (ddf.family<=2)],seq_length=16)\n",
    "\n",
    "# Step 5: Compute the result to get the sequences as a Dask DataFrame\n",
    "sequences = sequences.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dd.from_pandas(data, npartitions=4)  \n",
    "ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.array.lib.stride_tricks import sliding_window_view\n",
    "import dask.array as da\n",
    "\n",
    "#dask_array = da.from_array(ddf.values)\n",
    "# Assuming you want sequences of length 10\n",
    "seq_length = 16\n",
    "sequences = sliding_window_view(ddf.values, (seq_length, data.shape[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainM, X_testM, X_valM, y_trainM, y_testM, y_valM, base_trainM, base_testM, base_valM = [],[],[],[],[],[],[],[],[]\n",
    "init=False\n",
    "for family in data.family.unique():\n",
    "    for store in data.store_nbr.unique():\n",
    "        group = data.loc[(data.family==family) & (data.store_nbr == store)]\n",
    "        X_trainM0, X_testM0, X_valM0, y_trainM0, y_testM0, y_valM0, base_trainM0, base_testM0, base_valM0 = getSequencePerGroup(group)\n",
    " \n",
    "        if init:\n",
    "            X_trainM = np.concatenate((X_trainM, X_trainM0), axis=0)\n",
    "            X_testM  = np.concatenate((X_testM,  X_testM0), axis=0)\n",
    "            X_valM  = np.concatenate((X_valM,  X_valM0), axis=0)\n",
    "            y_trainM = np.concatenate((y_trainM, y_trainM0), axis=0)\n",
    "            y_testM  = np.concatenate((y_testM,  y_testM0), axis=0)\n",
    "            y_valM  = np.concatenate((y_valM,  y_valM0), axis=0)\n",
    "            base_trainM = np.concatenate((base_trainM, base_trainM0), axis=0)\n",
    "            base_testM  = np.concatenate((base_testM,  base_testM0), axis=0)\n",
    "            base_valM  = np.concatenate((base_valM,  base_valM0), axis=0)\n",
    "        else:\n",
    "            X_trainM, X_testM, X_valM, y_trainM,y_testM, y_valM = X_trainM0, X_testM0, X_valM0,y_trainM0, y_testM0, y_valM0\n",
    "            base_trainM, base_testM, base_valM = base_trainM0, base_testM0, base_valM0\n",
    "            init=True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainM.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainM.shape, y_trainM.shape, base_trainM.shape,X_testM.shape, y_testM.shape, base_testM.shape,X_valM.shape, y_valM.shape, base_valM.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LGBM t-16 fits\n",
    "- seems that families are too different, we optimize only a couple families but others are declining -> lgbm per family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcLossArima(pred, y, logTransform):\n",
    "    if logTransform:\n",
    "        a = np.exp(pred) -1\n",
    "        y = np.exp(y) -1 \n",
    "    else:\n",
    "        a = (pred)\n",
    "        y = (y)\n",
    "\n",
    "    if (a < 0).any():\n",
    "        a = np.clip(a, 0, 1e20)\n",
    "\n",
    "    rmsleTrain = np.sqrt(mean_squared_log_error(a,y))\n",
    "    return rmsleTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcLossLGBM(pred, y):\n",
    "    logPred = np.reshape(pred, (pred.shape[0],1))\n",
    "    a = np.exp(logPred) -1\n",
    "\n",
    "    logy = np.reshape(y, (pred.shape[0],1))\n",
    "    y = np.exp(logy) -1\n",
    "\n",
    "    rmsleTrain = np.sqrt(np.mean((a-y)**2))\n",
    "    return rmsleTrain\n",
    "def mse(pred,y):\n",
    "    return np.sqrt(np.mean((pred-y)**2))\n",
    "def calcLossLGBMArima1(pred, sales, arima, predictDiff):\n",
    "    if predictDiff:\n",
    "        pred = np.reshape(pred, (pred.shape[0],1)) + np.reshape(arima, (pred.shape[0],1))\n",
    "    a = np.exp(pred) -1\n",
    "\n",
    "    if (a < 0).any():\n",
    "        a = np.clip(a, 0, 1e20)\n",
    "    rmsleTrain = np.sqrt(mean_squared_log_error(a,sales))\n",
    "    return rmsleTrain\n",
    "def calcLossLGBMArima2(pred, y, arima, predictDiff):\n",
    "    if predictDiff:\n",
    "        pred = np.reshape(pred, (pred.shape[0],1)) + np.reshape(arima, (pred.shape[0],1))\n",
    "        y = np.reshape(y, (pred.shape[0],1)) + np.reshape(arima, (pred.shape[0],1))\n",
    "    a = np.exp(pred) -1\n",
    "    y = np.exp(y) -1\n",
    "\n",
    "    if (a < 0).any():\n",
    "        a = np.clip(a, 0, 1e20)\n",
    "    rmsleTrain = np.sqrt(mean_squared_log_error(a,y))\n",
    "    return rmsleTrain\n",
    "def plotLGBM(i, logTransform, salesDomain, pred, y, base):\n",
    "    if salesDomain:\n",
    "        pred = np.reshape(pred, (pred.shape[0],1)) + np.reshape(base, (pred.shape[0],1))\n",
    "        y = np.reshape(y, (pred.shape[0],1)) + np.reshape(base, (pred.shape[0],1))\n",
    "    if logTransform:\n",
    "        a = np.exp(pred[i:i+16])-1\n",
    "        y = np.exp(y[i:i+16]) -1\n",
    "        arima = np.exp(base[i:i+16]) - 1\n",
    "    else:\n",
    "        a = (pred[i:i+16])\n",
    "        y = (y[i:i+16])\n",
    "        arima = base[i:i+16]\n",
    "    x = range(len(a))   \n",
    "\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=1, figsize=(8, 6))\n",
    "    axs.plot(x, y, color='blue',label='Original')\n",
    "    axs.plot(x, a, color='red',label='pred')\n",
    "    axs.plot(x, arima, color='orange',label='arima')\n",
    "    axs.set_title('index: '+str(i))\n",
    "    fig.subplots_adjust(hspace=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # compare against base lgbm that just predicts always t+16\n",
    "sample_weights = ((train_bigDf.month > 7) & (train_bigDf.month < 9)) + 1# (train_bigDf.date > \"2017-06-01\") + 1 #train_bigDf.logSales+1 #np.reshape(( * train_bigDf.onpromotion.std()) + train_bigDf.onpromotion.mean()+ 1, (-1,1))\n",
    "sample_weights = sample_weights/ max(sample_weights)\n",
    "\n",
    "\n",
    "\n",
    "# Set parameters for LGBM model\n",
    "params = {\n",
    "    'boosting':'gbdt',#'gbdt', #'rf' #'dart'\n",
    "    'objective': 'regression',  # Assuming you're doing regression\n",
    "    'metric': 'mse',  # Mean squared error\n",
    "    'num_leaves': 10,\n",
    "    #'lambda_l1': 0.1,\n",
    "    #'lambda_l2': 0.2,\n",
    "    #'max_depth':10,\n",
    "    #'learning_rate': 0.1,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': -1,\n",
    "    'force_col_wise':True,\n",
    "}   \n",
    "\n",
    "# Train the model\n",
    "gbm = lgb.train(params, lgb.Dataset(X_train16, label=y_train16\n",
    "    #, weight=sample_weights\n",
    "    ), 2000,valid_sets=[lgb.Dataset(X_test16, label=y_test16)]#,num_boost_round=100\n",
    ",callbacks=[lgb.early_stopping(stopping_rounds=500)]\n",
    ")  \n",
    "predtrainLGBM = gbm.predict(X_train16)\n",
    "predtestLGBM = gbm.predict(X_test16)\n",
    "predvalLGBM = gbm.predict(X_pred16)\n",
    "\n",
    "#print('arima errors: ', calcLossArima(train_subDf.ref, train_subDf.logSales, logTransform), calcLossArima(test_subDf.ref, test_subDf.logSales, logTransform))#, calcLossArima(val_subDf.ref, val_subDf.logSales, logTransform, baseTest))\n",
    "print('arima errors: ', calcLossArima(train_bigDf.ref, train_bigDf.logSales, logTransform16), calcLossArima(test_bigDf.ref, test_bigDf.logSales, logTransform16))\n",
    "#print('lgbm errors: ', calcLossLGBM(predtrainLGBM, y_train), calcLossLGBM(predtestLGBM, y_test))#, calcLossLGBM(predvalLGBM, y_val))\n",
    "\n",
    "print('sales errors: ', mse(predtrainLGBM, y_train16), mse(predtestLGBM, y_test16))\n",
    "print('sales errors: ', calcLossLGBMArima2(predtrainLGBM, y_train16, baseTrain16, predictDiff16), calcLossLGBMArima2(predtestLGBM, y_test16, baseTest16,predictDiff16))#, calcLossLGBMArima2(predvalLGBM, y_val,baseVal))\n",
    "#print('sales errors: ', calcLossLGBMArima1(predtrainLGBM, train_subDf.sales, baseTrain), calcLossLGBMArima1(predtestLGBM, test_subDf.sales, baseTest), calcLossLGBMArima1(predvalLGBM, val_subDf.sales,baseVal))\n",
    "\n",
    "#sales errors:  0.3925699715971475 0.38953956730324635 with weight on date & groupedF\n",
    "#sales errors:  0.389990522698507 0.387005544750438    without weight on date\n",
    "\n",
    "# sales errors:  0.38269330293604925 0.38686356026792235 without weight & more features\n",
    "# sales errors:  0.3838912161743427  0.3874510405251872  with weight on recent\n",
    "# sales errors:  0.406618874649369   0.3908985813082843  with weight on juli, august, september\n",
    "# sales errors:  0.38151648760876566 0.3880183808501404  with weight on august\n",
    "# sales errors:  0.3809698888298268  0.3878809225435719  less lags / feature lags no weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('arima errors: ', calcLossArima(train_bigDf.ref, train_bigDf.logSales, logTransform16), calcLossArima(test_bigDf.ref, test_bigDf.logSales, logTransform16))\n",
    "#print('lgbm errors: ', calcLossLGBM(predtrainLGBM, y_train), calcLossLGBM(predtestLGBM, y_test))#, calcLossLGBM(predvalLGBM, y_val))\n",
    "\n",
    "print('sales errors: ', mse(predtrainLGBM, y_train16), mse(predtestLGBM, y_test16))\n",
    "print('sales errors: ', calcLossLGBMArima2(predtrainLGBM, y_train16, baseTrain16, predictDiff16), calcLossLGBMArima2(predtestLGBM, y_test16, baseTest16,predictDiff16))#, calcLossLGBMArima2(predvalLGBM, y_val,baseVal))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### analyze residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotDistOfTestTrain(feat, test, train,labelA ='test',labelB='train'):\n",
    "    a = test[[feat]].value_counts()\n",
    "    b = train[[feat]].value_counts()\n",
    "\n",
    "    indexa, indexb = [],[]\n",
    "    for i, val in enumerate(a.index):\n",
    "        indexa.append(val[0])\n",
    "    for i, val in enumerate(b.index):\n",
    "        indexb.append(val[0])\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.bar(indexa, a.values/np.sum(a), alpha=0.5, label=labelA)\n",
    "    ax.bar(indexb, b.values/np.sum(b), alpha=0.5, label=labelB)\n",
    "    ax.set_title('Distribution of' + feat)\n",
    "    ax.set_xlabel('Category')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    plt.xticks(rotation=90)\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "def display2Ddifferences(feata, featb, dfa, dfb):\n",
    "    a = dfa[[feata,featb]].value_counts().sort_index()\n",
    "    b = dfb[[feata,featb]].value_counts().sort_index()\n",
    "    xa, xb = [],[]\n",
    "    ya, yb = [],[]\n",
    "    for i, val in enumerate(a.index):\n",
    "        xa.append(val[0])\n",
    "        ya.append(val[1])\n",
    "    for i, val in enumerate(b.index):\n",
    "        xb.append(val[0])\n",
    "        yb.append(val[1])\n",
    "\n",
    "    df1 = pd.DataFrame()\n",
    "    df1[feata] = xa\n",
    "    df1[featb]=ya\n",
    "    df1['f'] = a.values/np.sum(a)*100\n",
    "\n",
    "    df2 = pd.DataFrame()\n",
    "    df2[feata] = xb\n",
    "    df2[featb]=yb\n",
    "    df2['f'] = b.values/np.sum(b)*100\n",
    "\n",
    "\n",
    "    fig = px.scatter(df1,\n",
    "        y=feata,\n",
    "    x=featb,\n",
    "        color='f',title='bad'\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "    fig1 = px.scatter(df2,\n",
    "        y=feata,\n",
    "        x=featb,\n",
    "        color='f',title='good'\n",
    "    )\n",
    "    fig1.show()\n",
    "\n",
    "    return df1, df2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bigDf['predTarget'] = predtrainLGBM\n",
    "test_bigDf['predTarget'] = predtestLGBM\n",
    "train_bigDf['err_predTarget'] = np.abs(train_bigDf['predTarget'] - train_bigDf['target'])\n",
    "test_bigDf['err_predTarget'] = np.abs(test_bigDf['predTarget'] - test_bigDf['target'])\n",
    "\n",
    "\n",
    "\n",
    "train_bigDf['predLogSales'] = train_bigDf['predTarget']+train_bigDf['salesArima']\n",
    "test_bigDf['predLogSales'] = test_bigDf['predTarget']+test_bigDf['salesArima']\n",
    "train_bigDf['err_LogSales'] = (train_bigDf['predLogSales'] - train_bigDf['logSales'])**2\n",
    "test_bigDf['err_LogSales'] =(test_bigDf['predLogSales'] - test_bigDf['logSales'])**2\n",
    "train_bigDf['err_arimaLogSales'] = (train_bigDf['salesArima'] - train_bigDf['logSales'])**2\n",
    "test_bigDf['err_arimaLogSales'] = (test_bigDf['salesArima'] - test_bigDf['logSales'])**2\n",
    "\n",
    "# arimaLogError > errLogSales = positive (= reduction of errLogSales)\n",
    "train_bigDf['improvedScoreByLGBM'] = train_bigDf['err_arimaLogSales'] - train_bigDf['err_LogSales']\n",
    "test_bigDf['improvedScoreByLGBM'] = test_bigDf['err_arimaLogSales'] - test_bigDf['err_LogSales']\n",
    "\n",
    "train_bigDf['predSales'] = np.exp(train_bigDf.predLogSales) -1\n",
    "test_bigDf['predSales'] = np.exp(test_bigDf.predLogSales) -1#\n",
    "train_bigDf['err_sales'] = np.abs(train_bigDf['predSales'] - train_bigDf['sales'])\n",
    "test_bigDf['err_sales'] = np.abs(test_bigDf['predSales'] - test_bigDf['sales'])\n",
    "\n",
    "train_bigDf['predArimaSales'] = np.exp(train_bigDf.salesArima) -1\n",
    "test_bigDf['predArimaSales'] = np.exp(test_bigDf.salesArima) -1\n",
    "train_bigDf['err_salesArima'] = np.abs(train_bigDf['predArimaSales'] - train_bigDf['sales'])\n",
    "test_bigDf['err_salesArima'] = np.abs(test_bigDf['predArimaSales'] - test_bigDf['sales'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bigDf[['err_LogSales','err_arimaLogSales','improvedScoreByLGBM']].hist(bins=50),test_bigDf[['err_LogSales','err_arimaLogSales','improvedScoreByLGBM']].hist(bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### what got worse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# err_logSales = (logPred - arimaLog - logSales + arimaLog)²\n",
    "# err_arimaLogSales = (arimaLog - logSales)²\n",
    "test_bigDf.sort_values(by='improvedScoreByLGBM')  # which got worse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "badPredictionsTest = test_bigDf.loc[test_bigDf.improvedScoreByLGBM>0]\n",
    "goodPredictionsTest = test_bigDf.loc[test_bigDf.improvedScoreByLGBM<0]\n",
    "badPredictionsTrain = train_bigDf.loc[train_bigDf.improvedScoreByLGBM>0]\n",
    "goodPredictionsTrain = train_bigDf.loc[train_bigDf.improvedScoreByLGBM<0]\n",
    "\n",
    "#mainly family 29 & 17 got better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only family & store seem to have significant differences between good/bad\n",
    "plotDistOfTestTrain('family',badPredictionsTest,goodPredictionsTest,'bad','good')\n",
    "plotDistOfTestTrain('store_nbr',badPredictionsTest,goodPredictionsTest,'bad','good')\n",
    "#plotDistOfTestTrain('onpromotion',badPredictionsTest,goodPredictionsTest,'bad','good')\n",
    "#plotDistOfTestTrain('city',badPredictionsTest,goodPredictionsTest,'bad','good')\n",
    "#plotDistOfTestTrain('state',badPredictionsTest,goodPredictionsTest,'bad','good')\n",
    "#plotDistOfTestTrain('type',badPredictionsTest,goodPredictionsTest,'bad','good')\n",
    "#plotDistOfTestTrain('cluster',badPredictionsTest,goodPredictionsTest,'bad','good')\n",
    "#plotDistOfTestTrain('weekday',badPredictionsTest,goodPredictionsTest,'bad','good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotDistOfTestTrain('family',badPredictionsTrain,goodPredictionsTrain,'bad','good')\n",
    "plotDistOfTestTrain('store_nbr',badPredictionsTrain,goodPredictionsTrain,'bad','good')\n",
    "#plotDistOfTestTrain('onpromotion',badPredictionsTrain,goodPredictionsTrain,'bad','good')\n",
    "#plotDistOfTestTrain('city',badPredictionsTrain,goodPredictionsTrain,'bad','good')\n",
    "#plotDistOfTestTrain('state',badPredictionsTrain,goodPredictionsTrain,'bad','good')\n",
    "#plotDistOfTestTrain('type',badPredictionsTrain,goodPredictionsTrain,'bad','good')\n",
    "#plotDistOfTestTrain('cluster',badPredictionsTrain,goodPredictionsTrain,'bad','good')\n",
    "#plotDistOfTestTrain('weekday',badPredictionsTrain,goodPredictionsTrain,'bad','good')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### what is generally bad\n",
    "families\n",
    "- 29,17,14,13 are good - 29&17 are often 0\n",
    "- 31,18,12,0 bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bigDf.sort_values(by='err_LogSales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "badPredictionsTest = test_bigDf.loc[test_bigDf.err_LogSales>1]\n",
    "goodPredictionsTest = test_bigDf.loc[test_bigDf.err_LogSales<1]\n",
    "badPredictionsTrain = train_bigDf.loc[train_bigDf.err_LogSales>1]\n",
    "goodPredictionsTrain = train_bigDf.loc[train_bigDf.err_LogSales<1]\n",
    "\n",
    "# bad families: 0,2,6,7,10-14,16,18,19,20,27,31,32\n",
    "# bad stores: 0,10-23,25,26,28-36,39,40,43,51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flippedPropDicts['family']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "badFreq, goodFreq = display2Ddifferences('store_nbr','family', badPredictionsTest, goodPredictionsTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqDf = pd.merge(badFreq, goodFreq, on=['store_nbr','family'], how='outer',suffixes=('bad','good'))\n",
    "freqDf['diff'] = freqDf.fgood-freqDf.fbad\n",
    "freqDf.sort_values(by='diff').head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bigDf.loc[(train_bigDf.family==0) & (train_bigDf.store_nbr==48)].sales.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only family & store seem to have significant differences between good/bad\n",
    "plotDistOfTestTrain('family',badPredictionsTest,goodPredictionsTest,'bad','good')\n",
    "plotDistOfTestTrain('store_nbr',badPredictionsTest,goodPredictionsTest,'bad','good')\n",
    "#plotDistOfTestTrain('onpromotion',badPredictionsTest,goodPredictionsTest,'bad','good')\n",
    "#plotDistOfTestTrain('city',badPredictionsTest,goodPredictionsTest,'bad','good')\n",
    "#plotDistOfTestTrain('state',badPredictionsTest,goodPredictionsTest,'bad','good')\n",
    "#plotDistOfTestTrain('type',badPredictionsTest,goodPredictionsTest,'bad','good')\n",
    "#plotDistOfTestTrain('cluster',badPredictionsTest,goodPredictionsTest,'bad','good')\n",
    "#plotDistOfTestTrain('weekday',badPredictionsTest,goodPredictionsTest,'bad','good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotDistOfTestTrain('family',badPredictionsTrain,goodPredictionsTrain,'bad','good')\n",
    "plotDistOfTestTrain('store_nbr',badPredictionsTrain,goodPredictionsTrain,'bad','good')\n",
    "#plotDistOfTestTrain('onpromotion',badPredictionsTrain,goodPredictionsTrain,'bad','good')\n",
    "#plotDistOfTestTrain('city',badPredictionsTrain,goodPredictionsTrain,'bad','good')\n",
    "#plotDistOfTestTrain('state',badPredictionsTrain,goodPredictionsTrain,'bad','good')\n",
    "#plotDistOfTestTrain('type',badPredictionsTrain,goodPredictionsTrain,'bad','good')\n",
    "#plotDistOfTestTrain('cluster',badPredictionsTrain,goodPredictionsTrain,'bad','good')\n",
    "#plotDistOfTestTrain('weekday',badPredictionsTrain,goodPredictionsTrain,'bad','good')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### is test/train different - Answer:no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotDistOfTestTrain('family',badPredictionsTest,badPredictionsTrain)\n",
    "plotDistOfTestTrain('store_nbr',badPredictionsTest,badPredictionsTrain)\n",
    "plotDistOfTestTrain('onpromotion',badPredictionsTest,badPredictionsTrain)\n",
    "plotDistOfTestTrain('city',badPredictionsTest,badPredictionsTrain)\n",
    "plotDistOfTestTrain('state',badPredictionsTest,badPredictionsTrain)\n",
    "plotDistOfTestTrain('type',badPredictionsTest,badPredictionsTrain)\n",
    "plotDistOfTestTrain('cluster',badPredictionsTest,badPredictionsTrain)\n",
    "plotDistOfTestTrain('weekday',badPredictionsTest,badPredictionsTrain)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "storeSales",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
