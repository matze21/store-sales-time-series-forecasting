{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goals\n",
    "1. predict for every store individually\n",
    "- make stationary target by diff, yes/no?\n",
    "- z score normalization on train data\n",
    "- predict next 16 values directly vs recursively?\n",
    "2. predict store individually but with every pair/family as parameter\n",
    "- needs zscore normalization\n",
    "- stationary target yes/no?\n",
    "3. predict all store/family pairs simultaneously\n",
    "- zscore? maybe not needed\n",
    "- stationary?\n",
    "\n",
    "features:\n",
    "1. time features:\n",
    "- linear timestamp\n",
    "- sin/cos of year, check for (week/month) if pattern present\n",
    "- encoding of weekday, maybe also month\n",
    "2. oil/holidays/location should be ok\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import plotly.offline as pyo\n",
    "from plotly import subplots\n",
    "import plotly.graph_objects as go\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.tsa.api as smt\n",
    "\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "#from tf.keras.layers import LSTM, Dense,Input,concatenate\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from baseFunctions import *\n",
    "from data_helpers import processData6\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, propDicts, flippedPropDicts = processData6()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check transactions\n",
    "-> where transaction has a nan value store is closed!\n",
    "- test data has to be open all the time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check why some products have a weird shape (produce)\n",
    "- some correlation to on promotion, but nothing obvious\n",
    "- let's ignore all data previous to july 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = data.loc[(data.store_nbr == 2) & (data.family == 19)]\n",
    "fig = subplots.make_subplots(rows=3, cols=1, shared_xaxes='all')\n",
    "fig.add_trace(go.Scattergl(x=a.date, y=a.sales, name='sales'), col=1, row = 1)\n",
    "fig.add_trace(go.Scattergl(x=a.date, y=a.onpromotion,name='promotion'), col=1, row = 3)\n",
    "fig.add_trace(go.Scattergl(x=a.date, y=a.holidayType,name='holiday'), col=1, row = 2)\n",
    "fig.add_trace(go.Scattergl(x=a.date, y=a.store_closed,name='closed'), col=1, row = 2)\n",
    "fig.add_trace(go.Scattergl(x=a.date, y=a.dcoilwtico,name='oil'), col=1, row = 3)\n",
    "fig.add_trace(go.Scattergl(x=a.date, y=a.transactions,name='transactions'), col=1, row = 3)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature engineering\n",
    "\n",
    "aggregated data\n",
    "- there is some linear trend "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dailyData = data.groupby('date')['sales'].sum()\n",
    "dec = sm.tsa.seasonal_decompose(dailyData,period = 12, model = 'additive').plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_periodogram(dailyData, 365, n_domFreq=30)\n",
    "\n",
    "# strong frequencies     => TimePeriod \n",
    "# 52 (weekly) 365/52     = 7 days\n",
    "# 24 (biweekly) 365/24   = 15 days (half-month)\n",
    "# 104 (halfweek) 365/104 = 3.5 = 3.5 days \n",
    "# 12 (monthly)  365/12   = 30 days\n",
    "# 6 (bimonthly)          = 60 days\n",
    "# 4 (quarters)           = 90 days\n",
    "# 3 (thirds)             = 120 days\n",
    "# 2 (half-year)          = 182\n",
    "# 1 (yearly)             = 365 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, timeFeatures = featureEngineering(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotidx(ind,pred, time_index):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    mintimeIdx = test_index[ind] - look_back\n",
    "    plt.plot(time_index[mintimeIdx:test_index[ind]], X_test[0][ind,:,0], label='Train')\n",
    "    plt.plot(time_index[test_index][ind:ind+n_predictedValues], pred[ind,:], label='Predicted')\n",
    "    plt.plot(time_index[test_index][ind:ind+n_predictedValues], labels[test_index][ind,:], label='actual test')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Value')\n",
    "    #plt.title(f'ARIMA Forecast (RMSLE: {rmsle:.4f})')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "for i in range(1,4):\n",
    "    plotidx(i*10,forecast,train.date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data cleaning / outliers\n",
    "- bunch of outliers\n",
    "- product families are all very similar\n",
    "- stores are all very similar as well (dropouts of stores are the same)\n",
    "- sometimes stores are closed! (store 18) -> use transaction data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stores that opened later than 2013\n",
    "for storeId in data1.store_nbr.unique():\n",
    "    store = data1.loc[(data1.dataT == 'train') & (data1.store_nbr == storeId)]\n",
    "    example = store.groupby('date')['sales'].sum()#.plot()\n",
    "    b = example.cumsum()\n",
    "    idx = b.loc[b>0].index[0]\n",
    "    if str(idx) != '2013-01-02 00:00:00':\n",
    "        print(storeId, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find profucts per store that weren't offered in the beginning\n",
    "for storeId in data1.store_nbr.unique():\n",
    "    for family in data1.family.unique():\n",
    "        store = data1.loc[(data1.dataT == 'train') & (data1.store_nbr == storeId) & (data1.family == family)]\n",
    "        example = store.groupby('date')['sales'].sum()#.plot()\n",
    "        b = example.cumsum()\n",
    "        if b.tail(1).iloc[0] == 0:\n",
    "            print(storeId, family,flippedPropDicts['family'][family], 'NO product available')\n",
    "        else:\n",
    "            idx = b.loc[b>0].index[0]\n",
    "            if str(idx) != '2013-01-02 00:00:00':\n",
    "                print(idx,storeId, family, flippedPropDicts['family'][family],'date: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visulize data that has dropouts -> we need to filter outliers!!\n",
    "storeId = 1\n",
    "for familyId in data1.family.unique():\n",
    "        familyDf = data1.loc[(data1.dataT == 'train') & (data1.store_nbr == storeId) & (data1.family == familyId)].copy()\n",
    "        nInvalidSequences = 0\n",
    "        nValidS = 0\n",
    "    \n",
    "        look_back = 100\n",
    "        n_predictedValues=16\n",
    "        for i in range(familyDf.shape[0]-look_back-n_predictedValues):\n",
    "            startS0 = i\n",
    "            endS0 = startS0 + look_back\n",
    "            endS1 = endS0 + n_predictedValues\n",
    "            block1 = familyDf.loc[:,['sales','date']].iloc[startS0:endS1].copy()\n",
    "            cumsum = block1.sales.cumsum()\n",
    "            isSequenceValid = ((block1.sales == 0) & (cumsum.diff(20)==0)).sum() < 1 #newyear is included!\n",
    "            if not isSequenceValid:\n",
    "                nInvalidSequences = nInvalidSequences +1\n",
    "            else:\n",
    "                nValidS = nValidS +1\n",
    "        stat, p = shapiro(familyDf.sales)\n",
    "        print(familyId, stat,p, 'sample is gaussian:',p>0.05)\n",
    "        if nInvalidSequences > 0 and familyId>17 and familyId != 1:\n",
    "            print(familyId, nInvalidSequences, nValidS, flippedPropDicts['family'][familyId])\n",
    "            \n",
    "            familyDf['cumsum0'] = familyDf.sales.cumsum()\n",
    "            familyDf.loc[:,'rolling7'] = familyDf.sales.rolling(14).mean()\n",
    "            familyDf.loc[:,'rolling7std'] = familyDf.sales.rolling(14).std()\n",
    "            familyDf.loc[:,'rollingThreshold'] = (familyDf['rolling7'] + 5* familyDf['rolling7std']).shift(1)\n",
    "\n",
    "            familyDf.loc[:,'absMean'] = familyDf.loc[familyDf.cumsum0 > 0].sales.mean() + 5*familyDf.loc[familyDf.cumsum0 > 0].sales.std()\n",
    "            fig = subplots.make_subplots(rows=1, cols=1, shared_xaxes='all')\n",
    "            fig.add_trace(go.Scattergl(x=familyDf.date, y=familyDf.sales), col=1, row = 1)\n",
    "            fig.add_trace(go.Scattergl(x=familyDf.date, y=familyDf.rollingThreshold), col=1, row = 1)\n",
    "            fig.add_trace(go.Scattergl(x=familyDf.date, y=familyDf.absMean), col=1, row = 1)\n",
    "            fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "from scipy.stats import shapiro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id in [20]:\n",
    "        familyDf = data.loc[(data.dataT == 'train') & (data.store_nbr == 1) & (data.family == id)].copy() #4 #16 #15\n",
    "        # Assuming X is your data\n",
    "        #kmeans = KMeans(n_clusters=2, random_state=0).fit(familyDf.sales)\n",
    "        familyDf.loc[:,'rolling7'] = familyDf.sales.rolling(14).mean()\n",
    "        familyDf.loc[:,'rolling7std'] = familyDf.sales.rolling(14).std()\n",
    "        familyDf.loc[:,'rollingThreshold'] = (familyDf['rolling7'] + 5* familyDf['rolling7std']).shift(1)\n",
    "        familyDf.loc[:,'absMean'] = familyDf.sales.mean() + 5*familyDf.sales.std()\n",
    "        fig = subplots.make_subplots(rows=1, cols=1, shared_xaxes='all')\n",
    "        fig.add_trace(go.Scattergl(x=familyDf.date, y=familyDf.sales), col=1, row = 1)\n",
    "        fig.add_trace(go.Scattergl(x=familyDf.date, y=familyDf.rollingThreshold), col=1, row = 1)\n",
    "        fig.add_trace(go.Scattergl(x=familyDf.date, y=familyDf.absMean), col=1, row = 1)\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find out why some data series are so wild (produce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data cleaning / visualizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotSales(df, storeId : int, family :str, familyId, p_value, save=False):\n",
    "    plt.figure(figsize=(15,20))\n",
    "\n",
    "    family=family.replace('/','-')\n",
    "    \n",
    "    fig, axs = plt.subplots(nrows=3, ncols=1, figsize=(8, 6))\n",
    "    axs[0].plot(df.date, df.sales, color='blue',label='Original')\n",
    "    axs[0].plot(df.date, df.sales_outRem, color='red',label='out_rem')\n",
    "    axs[0].set_title(str(storeId)+'  '+str(familyId) + family + ' p_value:' + str(p_value))\n",
    "    \n",
    "\n",
    "    axs[1].plot(df.date, np.log(df.sales+1), color='blue')\n",
    "    axs[1].set_title('log sales')\n",
    "    axs[2].plot(df.date ,np.log(df.sales+1).diff(21), color='blue')\n",
    "    axs[2].set_title('log sales, diff 3 weeks')\n",
    "\n",
    "    fig.subplots_adjust(hspace=0.5)\n",
    "    #orig = plt.plot(df.sales, color='blue',label='Original')\n",
    "    #orig = plt.plot(df.sales_outRem, color='red',label='out_rem')\n",
    "\n",
    "    if save:\n",
    "        plt.savefig('graphs/plot_'+str(storeId)+'_'+str(familyId) + family+'.jpg')\n",
    "    else:\n",
    "        plt.show(block=False)\n",
    "\n",
    "def filterDataForOutliers(data, familyId, storeId, flippedPropDicts, render = False, saveFig = False):\n",
    "    a = data.loc[(data.store_nbr == storeId) & (data.family == familyId) & (data.dataT == 'train')].copy()\n",
    "\n",
    "    #remove feb29\n",
    "    #a = a.loc[~((a.date.dt.day==29) & (a.date.dt.month==2))]\n",
    "\n",
    "    a.loc[:,'cumsum0'] = a.sales.cumsum()\n",
    "\n",
    "    # filter out if product is not offered\n",
    "    a = a.loc[a.cumsum0 > 0]\n",
    "\n",
    "    # only consider data after july 2015 /other stuff seems to be too old\n",
    "    #a = a.loc[a.date > \"2015-07-01\"]\n",
    "\n",
    "    # check if stationary\n",
    "    try:\n",
    "        #p_value = test_stationarity(np.log(a.sales+1).diff(21), 12, True)\n",
    "        p_value = test_stationarity(a.sales, 12, True)\n",
    "    except:\n",
    "        p_value = 1e6\n",
    "    isStationary = p_value < 0.05\n",
    "    \n",
    "    # check if lots of 0s\n",
    "    counts, bins = np.histogram(a.sales, bins=50)\n",
    "    binZero = counts[0]\n",
    "    binNextZero = -1\n",
    "    for i,count in enumerate(counts):\n",
    "        if i > 0 and count != 0:\n",
    "            binNextZero = count\n",
    "            break\n",
    "    isZeroSinglePeak = binZero > 2*binNextZero\n",
    "\n",
    "    countsSorted = np.sort(counts)[::-1]\n",
    "    significantZeroPart = binZero > countsSorted[1]\n",
    "\n",
    "    fishy = (significantZeroPart and isZeroSinglePeak)# or not isStationary\n",
    "\n",
    "    sigInterval = 5 if fishy else 7\n",
    "\n",
    "    # remove outliers  ---- seems to work ok-ish\n",
    "    a.loc[:,'rolling7'] = a.sales.rolling(14).mean()\n",
    "    a.loc[:,'rolling7std'] = a.sales.rolling(14).std()\n",
    "    a.loc[:,'rollingThreshold'] = (a['rolling7'] + 5* a['rolling7std']).shift(1)\n",
    "    a['absMean'] = a.sales.mean() + 5*a.sales.std()\n",
    "    a['sales_outRem'] = a.sales\n",
    "\n",
    "    if fishy:\n",
    "        a.loc[(a.sales>2*a.absMean) & (a.sales>a.rollingThreshold) & (a.sales>20),'sales_outRem'] = np.nan\n",
    "    else:\n",
    "        a.loc[(a.sales>a.absMean) & (a.sales>a.rollingThreshold),'sales_outRem'] = np.nan\n",
    "\n",
    "    hasOutliers = a.sales_outRem.isna().sum()\n",
    "    a['sales_outRem'] = a.sales_outRem.interpolate(limit_direction='both')\n",
    "\n",
    "    \n",
    "    if fishy or render or hasOutliers:\n",
    "        print(storeId, familyId, 'stationary ',isStationary, p_value, 'n_outliers: ',hasOutliers,flippedPropDicts['family'][familyId])\n",
    "        plotSales(a, storeId, flippedPropDicts['family'][familyId], familyId, p_value, save=saveFig)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for storeId in data.store_nbr.unique():\n",
    "    for familyId in data.family.unique():\n",
    "        a = filterDataForOutliers(data, familyId, storeId, flippedPropDicts, render=True, saveFig = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### try augmented dickey-fuller test to find which targets have flaws\n",
    "\n",
    "doesn't determine if we have dropouts, but gives good info anyways\n",
    "-> only use data after july 2015, before seems too different\n",
    "- sometimes even a new trend in the new year (2017)\n",
    "\n",
    "Problems:\n",
    "- lots of portions with 0s\n",
    "- hard to determine which portions to drop\n",
    "- not stationary\n",
    "- some fat outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id in [19]:#,24,28, 30,31]:\n",
    "        familyDf = data.loc[(data.dataT == 'train') & (data.store_nbr == 1) & (data.family == id)].copy()\n",
    "        familyDf = familyDf.set_index('date')\n",
    "        dftest = adfuller(familyDf.sales, autolag='AIC')\n",
    "        p_value = dftest[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distributions -> check if gaussian distribution doesn't owrk well\n",
    "for familyId in data.family.unique():\n",
    "    familyDf = data.loc[(data.dataT == 'train') & (data.store_nbr == 1) & (data.family == familyId)]\n",
    "    counts, bins, patches = plt.hist(familyDf.sales, bins=50, edgecolor='black')\n",
    "    binZero = counts[0]\n",
    "    countsSorted = np.sort(counts)[::-1]\n",
    "    significantZeroPart = binZero > countsSorted[1] #checks if zero bin is one of the 2 biggest bins\n",
    "        \n",
    "    try:\n",
    "        p_value = test_stationarity(familyDf.sales, 12, True)\n",
    "    except:\n",
    "        p_value = 1e6\n",
    "    print(familyId, p_value, 'is stationary?: ', p_value < 0.05, ' has lots of 0:', significantZeroPart)\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# individual prediction\n",
    "\n",
    "some approaches work for large values but not for small and vice versa\n",
    "predicting large & small values at the same time is hard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flippedPropDicts['family']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data1.loc[(data1.dataT == 'train') & (data1.store_nbr == 1) & (data1.family == 4)]# & (data1.date > idx)]\n",
    "fig = subplots.make_subplots(rows=2, cols=1, shared_xaxes='all')\n",
    "fig.add_trace(go.Scattergl(x=train.date, y=train.sales), col=1, row = 1)\n",
    "fig.add_trace(go.Scattergl(x=train.date, y=train.weekday), col=1, row = 2)\n",
    "fig.add_trace(go.Scattergl(x=train.date, y=train.holidayType), col=1, row = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores = cross_val_score(model, X, y, cv=5)\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "\n",
    "print(\"Mean CV score:\", cv_scores.mean())\n",
    "print(\"Standard deviation of CV scores:\", cv_scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainF = [\n",
    "       #'store_nbr', 'family', \n",
    "       #'sales',\n",
    "       'onpromotion', \n",
    "       #'dataT',\n",
    "       #'city', 'state', 'type', 'cluster',\n",
    "       'dcoilwtico', \n",
    "       'holidayType',\n",
    "       'description', 'transferred', \n",
    "       'linear_time', \n",
    "       'day_of_year',\n",
    "       'weekday',\n",
    "       'month',\n",
    "\n",
    "       #'day_of_year_f1_0', \n",
    "       # 'day_of_year_f1_60', 'day_of_year_f1_120',\n",
    "       #'day_of_year_f1_180', 'day_of_year_f1_240', 'day_of_year_f1_300',\n",
    "       #'day_of_year_f2_0', \n",
    "       # 'day_of_year_f2_60', 'day_of_year_f2_120',\n",
    "       #'day_of_year_f2_180', 'day_of_year_f2_240', 'day_of_year_f2_300',\n",
    "       #'day_of_year_f3_0',\n",
    "       #  'day_of_year_f3_60', 'day_of_year_f3_120',\n",
    "       #'day_of_year_f3_180', 'day_of_year_f3_240', 'day_of_year_f3_300',\n",
    "       #'day_of_year_f4_0',# 'day_of_year_f4_60', 'day_of_year_f4_120',\n",
    "       #'day_of_year_f4_180', 'day_of_year_f4_240', 'day_of_year_f4_300',\n",
    "       #'day_of_year_f6_0', #'day_of_year_f6_60', 'day_of_year_f6_120',\n",
    "       #'day_of_year_f6_180', 'day_of_year_f6_240', 'day_of_year_f6_300',\n",
    "       'day_of_year_f12_0',\n",
    "          #'day_of_year_f12_60',\n",
    "       #'day_of_year_f12_120',\n",
    "       #'day_of_year_f12_180',# \n",
    "       #'day_of_year_f12_240', #'day_of_year_f12_300',\n",
    "       'day_of_year_f104_0',\n",
    "        # 'day_of_year_f104_60',\n",
    "       #'day_of_year_f104_120',\n",
    "       #'day_of_year_f104_180',# \n",
    "       #'day_of_year_f104_240',# 'day_of_year_f104_300',\n",
    "       'day_of_year_f24_0', \n",
    "       #'day_of_year_f24_60', \n",
    "       #'day_of_year_f24_120',\n",
    "       #'day_of_year_f24_180', \n",
    "       #'day_of_year_f24_240',# 'day_of_year_f24_300',\n",
    "       'day_of_year_f52_0',\n",
    "       # 'day_of_year_f52_60', \n",
    "       #'day_of_year_f52_120',\n",
    "       #'day_of_year_f52_180'#, \n",
    "       #'day_of_year_f52_240'#, 'day_of_year_f52_300'\n",
    "       \n",
    "       ]\n",
    "\n",
    "train0 = trainF + ['sales']\n",
    "\n",
    "train = data1.loc[(data1.dataT == 'train') & (data1.store_nbr == 1) & (data1.family == 18)] # family 18\n",
    "\n",
    "n_predictedValues = 16\n",
    "look_back = 100\n",
    "zScoreNorm = False\n",
    "\n",
    "sequence0 = []\n",
    "sequence1 = []\n",
    "labels = []\n",
    "\n",
    "# zscore over all values -> not ideal bc test data\n",
    "if zScoreNorm:\n",
    "    mean = train.sales.mean()\n",
    "    mean = 0 # modified zScore, not in mean = 0\n",
    "    std = max(train.sales.std(), 1)\n",
    "    train.loc[:,'sales'] = (train.sales - mean) / std\n",
    "\n",
    "for i in range(train.shape[0]-look_back-n_predictedValues):\n",
    "    startS0 = i\n",
    "    endS0 = startS0 + look_back\n",
    "    endS1 = endS0 + n_predictedValues\n",
    "    block1 = train[['sales','date']].iloc[startS0:endS1]\n",
    "    isSequenceValid = (block1.sales == 0).sum() < 2 #newyear is included!\n",
    "    if isSequenceValid:\n",
    "        sequence0.append(train[train0].iloc[startS0:endS0].to_numpy().flatten())\n",
    "        sequence1.append(train[trainF].iloc[endS0:endS1].to_numpy().flatten())\n",
    "        labels.append(train['sales'].iloc[endS0:endS1])\n",
    "sequence0 = np.stack(sequence0, axis = 0)\n",
    "sequence1 = np.stack(sequence1, axis=0)\n",
    "labels    = np.stack(labels, axis = 0)\n",
    "\n",
    "\n",
    "\n",
    "X = np.concatenate((sequence0, sequence1), axis=1)\n",
    "y = labels\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "#train_data = lgb.Dataset(X_train, label=y_train)\n",
    "#test_data = lgb.Dataset(X_test, label=y_test)\n",
    "\n",
    "# Set parameters for LGBM model\n",
    "params = {\n",
    "    'objective': 'regression',  # Assuming you're doing regression\n",
    "    'metric': 'rmsle',  # Mean squared error\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "\n",
    "# Train the model\n",
    "num_round = 10\n",
    "\n",
    "gbms = [lgb.train(params, lgb.Dataset(X_train, label=y_train[:, i]),num_round, valid_sets=[lgb.Dataset(X_test, label=y_test[:,i])]) for i in range(y_train.shape[1])]\n",
    "\n",
    "\n",
    "forecast = np.column_stack([gbm.predict(X_train, num_iteration=gbm.best_iteration) for gbm in gbms])\n",
    "if (forecast<0).any():\n",
    "    print('negative values!!!')\n",
    "    forecast = np.clip(forecast, 0, 1e29)\n",
    "if zScoreNorm:\n",
    "    forecast = forecast *std  + mean\n",
    "    y_train = y_train *std + mean\n",
    "rmsleTrain = np.sqrt(mean_squared_log_error(forecast, y_train))\n",
    "forecast = np.column_stack([gbm.predict(X_test, num_iteration=gbm.best_iteration) for gbm in gbms])\n",
    "if zScoreNorm:\n",
    "    forecast = forecast *std  + mean\n",
    "    y_test = y_test*std + mean\n",
    "rmsleTest = np.sqrt(mean_squared_log_error(forecast, y_test))\n",
    "print('errors:  ', round(rmsleTrain,3), round(rmsleTest,3), y_train.shape[0], y_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGBM per store/family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainF = [\n",
    "       #'store_nbr', 'family', \n",
    "       #'sales',\n",
    "       'onpromotion', \n",
    "       #'dataT',\n",
    "       #'city', 'state', 'type', 'cluster',\n",
    "       'dcoilwtico', \n",
    "       'holidayType',\n",
    "       'description', 'transferred', \n",
    "       'linear_time', \n",
    "       'day_of_year',\n",
    "       'weekday',\n",
    "       'month',\n",
    "\n",
    "       #'day_of_year_f1_0', \n",
    "       # 'day_of_year_f1_60', 'day_of_year_f1_120',\n",
    "       #'day_of_year_f1_180', 'day_of_year_f1_240', 'day_of_year_f1_300',\n",
    "       #'day_of_year_f2_0', \n",
    "       # 'day_of_year_f2_60', 'day_of_year_f2_120',\n",
    "       #'day_of_year_f2_180', 'day_of_year_f2_240', 'day_of_year_f2_300',\n",
    "       #'day_of_year_f3_0',\n",
    "       #  'day_of_year_f3_60', 'day_of_year_f3_120',\n",
    "       #'day_of_year_f3_180', 'day_of_year_f3_240', 'day_of_year_f3_300',\n",
    "       #'day_of_year_f4_0',# 'day_of_year_f4_60', 'day_of_year_f4_120',\n",
    "       #'day_of_year_f4_180', 'day_of_year_f4_240', 'day_of_year_f4_300',\n",
    "       #'day_of_year_f6_0', #'day_of_year_f6_60', 'day_of_year_f6_120',\n",
    "       #'day_of_year_f6_180', 'day_of_year_f6_240', 'day_of_year_f6_300',\n",
    "       'day_of_year_f12_0',\n",
    "          #'day_of_year_f12_60',\n",
    "       #'day_of_year_f12_120',\n",
    "       #'day_of_year_f12_180',# \n",
    "       #'day_of_year_f12_240', #'day_of_year_f12_300',\n",
    "       'day_of_year_f104_0',\n",
    "        # 'day_of_year_f104_60',\n",
    "       #'day_of_year_f104_120',\n",
    "       #'day_of_year_f104_180',# \n",
    "       #'day_of_year_f104_240',# 'day_of_year_f104_300',\n",
    "       'day_of_year_f24_0', \n",
    "       #'day_of_year_f24_60', \n",
    "       #'day_of_year_f24_120',\n",
    "       #'day_of_year_f24_180', \n",
    "       #'day_of_year_f24_240',# 'day_of_year_f24_300',\n",
    "       'day_of_year_f52_0',\n",
    "       # 'day_of_year_f52_60', \n",
    "       #'day_of_year_f52_120',\n",
    "       #'day_of_year_f52_180'#, \n",
    "       #'day_of_year_f52_240'#, 'day_of_year_f52_300'\n",
    "       \n",
    "       ]\n",
    "\n",
    "train0 = trainF + ['sales']\n",
    "\n",
    "n_predictedValues = 16\n",
    "look_back = 100\n",
    "zScoreNorm = False\n",
    "\n",
    "errors = []\n",
    "predictions = []\n",
    "\n",
    "for storeId in data1.store_nbr.unique():\n",
    "    for familyId in data1.family.unique():\n",
    "        tic = time.time()\n",
    "        familyDf = data1.loc[(data1.dataT == 'train') & (data1.store_nbr == storeId) & (data1.family == familyId)]\n",
    "        compDf   = data1.loc[(data1.dataT == 'test') & (data1.store_nbr == storeId) & (data1.family == familyId)]\n",
    "        toc1 = time.time()\n",
    "        sequence0 = []\n",
    "        sequence1 = []\n",
    "        labels = []\n",
    "        nInvalidSequences = 0\n",
    "    \n",
    "        # zscore over all values -> not ideal bc test data\n",
    "        if zScoreNorm:\n",
    "            mean = familyDf.sales.mean()\n",
    "            mean = 0 # modified zScore, not in mean = 0\n",
    "            std = max(familyDf.sales.std(), 1)\n",
    "            familyDf.loc[:,'sales'] = (familyDf.sales - mean) / std\n",
    "    \n",
    "        for i in range(familyDf.shape[0]-look_back-n_predictedValues):\n",
    "            startS0 = i\n",
    "            endS0 = startS0 + look_back\n",
    "            endS1 = endS0 + n_predictedValues\n",
    "            block1 = familyDf[['sales','date']].iloc[startS0:endS1]\n",
    "            cumsum = block1.sales.cumsum()\n",
    "            isSequenceValid = ((block1.sales == 0) & (cumsum.diff(20)==0)).sum() < 1 #newyear is included!\n",
    "            if isSequenceValid:\n",
    "                sequence0.append(familyDf[train0].iloc[startS0:endS0].to_numpy().flatten())\n",
    "                sequence1.append(familyDf[trainF].iloc[endS0:endS1].to_numpy().flatten())\n",
    "                labels.append(familyDf['sales'].iloc[endS0:endS1])\n",
    "            else:\n",
    "                nInvalidSequences = nInvalidSequences +1\n",
    "\n",
    "        if len(labels) == 0:\n",
    "            print('no valid sequence', storeId, familyId)\n",
    "            compDf.loc[:,'sales'] = 0\n",
    "            predictions.append(compDf[['id','sales']])\n",
    "            continue\n",
    "        sequence0 = np.stack(sequence0, axis = 0)\n",
    "        sequence1 = np.stack(sequence1, axis=0)\n",
    "        labels    = np.stack(labels, axis = 0)\n",
    "        toc2 = time.time()\n",
    "    \n",
    "        X = np.concatenate((sequence0, sequence1), axis=1)\n",
    "        y = labels\n",
    "        \n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)\n",
    "    \n",
    "        # Set parameters for LGBM model\n",
    "        params = {\n",
    "            'objective': 'regression',  # Assuming you're doing regression\n",
    "            'metric': 'rmsle',  # Mean squared error\n",
    "            'num_leaves': 31,\n",
    "            'learning_rate': 0.05,\n",
    "            'feature_fraction': 0.9,\n",
    "            'bagging_fraction': 0.8,\n",
    "            'bagging_freq': 5,\n",
    "            'verbose': -1\n",
    "        }\n",
    "        toc3 = time.time()\n",
    "    \n",
    "        # Train the model\n",
    "        num_round = 10\n",
    "\n",
    "        gbms = [lgb.train(params, lgb.Dataset(X_train, label=y_train[:, i]),num_round, valid_sets=[lgb.Dataset(X_test, label=y_test[:,i])]) for i in range(y_train.shape[1])]\n",
    "        toc4 = time.time()\n",
    "    \n",
    "        forecast = np.column_stack([gbm.predict(X_train, num_iteration=gbm.best_iteration) for gbm in gbms])\n",
    "        if (forecast<0).any():\n",
    "            print('negative values!!!')\n",
    "            forecast = np.clip(forecast, 0, 1e29)\n",
    "        if zScoreNorm:\n",
    "            forecast = forecast *std  + mean\n",
    "            y_train = y_train *std + mean\n",
    "        rmsleTrain = np.sqrt(mean_squared_log_error(forecast, y_train))\n",
    "        forecast = np.column_stack([gbm.predict(X_test, num_iteration=gbm.best_iteration) for gbm in gbms])\n",
    "        if (forecast<0).any():\n",
    "            print('negative values!!!')\n",
    "            forecast = np.clip(forecast, 0, 1e29)\n",
    "        if zScoreNorm:\n",
    "            forecast = forecast *std  + mean\n",
    "            y_test = y_test*std + mean\n",
    "        rmsleTest = np.sqrt(mean_squared_log_error(forecast, y_test))\n",
    "        \n",
    "        print('errors:  ', round(rmsleTrain,3), round(rmsleTest,3), familyId, storeId,flippedPropDicts['family'][familyId])\n",
    "        print(y_train.shape[0],y_test.shape[0],nInvalidSequences)\n",
    "        toc5 = time.time()\n",
    "\n",
    "        X_comp = np.concatenate((np.reshape(familyDf[train0].tail(look_back).to_numpy().flatten(), (1,-1)), np.reshape(compDf[trainF].to_numpy().flatten(), (1,-1))), axis=1)\n",
    "        y_pred = np.column_stack([gbm.predict(X_comp, num_iteration=gbm.best_iteration) for gbm in gbms])\n",
    "        if (y_pred<0).any():\n",
    "            print('negative values!!!')\n",
    "            y_pred = np.clip(y_pred, 0, 1e29)\n",
    "        if zScoreNorm:\n",
    "            compDf.loc[:,'sales'] = np.reshape((y_pred*std + mean), (16,))\n",
    "        else:\n",
    "            compDf.loc[:,'sales'] = np.reshape((y_pred), (16,))\n",
    "        predictions.append(compDf[['id','sales']])\n",
    "        toc6 = time.time()\n",
    "        errors.append({'store':storeId,'fam':flippedPropDicts['family'][familyId],'trainE':round(rmsleTrain,3), 'testE':round(rmsleTest,3),'dt':toc6-tic})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = toc6-tic\n",
    "def calcTimeDiff(tic,toc,dt):\n",
    "    print((toc-tic)*100/dt)\n",
    "\n",
    "calcTimeDiff(tic,toc1,dt)\n",
    "calcTimeDiff(toc1,toc2,dt)\n",
    "calcTimeDiff(toc2,toc3,dt)\n",
    "calcTimeDiff(toc3,toc4,dt)\n",
    "calcTimeDiff(toc4,toc5,dt)\n",
    "calcTimeDiff(toc5,toc6,dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predDf = pd.concat(predictions)\n",
    "predDf = predDf.sort_values(by='id').reset_index().drop('index',axis=1)\n",
    "predDf = predDf.set_index('id')\n",
    "predDf.to_csv(\"predictions_individual_lgbm.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errorsDf = pd.DataFrame(errors)\n",
    "errorsDf.to_csv('errors_predictions_individual_lgbm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stores 52, 18 & 25 have issues, \n",
    "# families lawn and garden, liquour wine bear, school and office supplies\n",
    "errorsDf.loc[(errorsDf.testE > 1) & (errorsDf.store == 52)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errorsDf[['trainE','testE']].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM approach\n",
    "only works either for small OR for large values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainF = [\n",
    "       #'store_nbr', 'family', \n",
    "       #'sales',\n",
    "       'onpromotion', \n",
    "       #'dataT',\n",
    "       #'city', 'state', 'type', 'cluster',\n",
    "       'dcoilwtico', \n",
    "       'holidayType',\n",
    "       'description', 'transferred', \n",
    "       'linear_time', \n",
    "       'day_of_year',\n",
    "       'weekday',\n",
    "       'month',\n",
    "\n",
    "       #'day_of_year_f1_0', \n",
    "       # 'day_of_year_f1_60', 'day_of_year_f1_120',\n",
    "       #'day_of_year_f1_180', 'day_of_year_f1_240', 'day_of_year_f1_300',\n",
    "       #'day_of_year_f2_0', \n",
    "       # 'day_of_year_f2_60', 'day_of_year_f2_120',\n",
    "       #'day_of_year_f2_180', 'day_of_year_f2_240', 'day_of_year_f2_300',\n",
    "       #'day_of_year_f3_0',\n",
    "       #  'day_of_year_f3_60', 'day_of_year_f3_120',\n",
    "       #'day_of_year_f3_180', 'day_of_year_f3_240', 'day_of_year_f3_300',\n",
    "       #'day_of_year_f4_0',# 'day_of_year_f4_60', 'day_of_year_f4_120',\n",
    "       #'day_of_year_f4_180', 'day_of_year_f4_240', 'day_of_year_f4_300',\n",
    "       #'day_of_year_f6_0', #'day_of_year_f6_60', 'day_of_year_f6_120',\n",
    "       #'day_of_year_f6_180', 'day_of_year_f6_240', 'day_of_year_f6_300',\n",
    "       'day_of_year_f12_0',\n",
    "          #'day_of_year_f12_60',\n",
    "       #'day_of_year_f12_120',\n",
    "       #'day_of_year_f12_180',# \n",
    "       #'day_of_year_f12_240', #'day_of_year_f12_300',\n",
    "       'day_of_year_f104_0',\n",
    "        # 'day_of_year_f104_60',\n",
    "       #'day_of_year_f104_120',\n",
    "       #'day_of_year_f104_180',# \n",
    "       #'day_of_year_f104_240',# 'day_of_year_f104_300',\n",
    "       'day_of_year_f24_0', \n",
    "       #'day_of_year_f24_60', \n",
    "       #'day_of_year_f24_120',\n",
    "       #'day_of_year_f24_180', \n",
    "       #'day_of_year_f24_240',# 'day_of_year_f24_300',\n",
    "       'day_of_year_f52_0',\n",
    "       # 'day_of_year_f52_60', \n",
    "       #'day_of_year_f52_120',\n",
    "       #'day_of_year_f52_180'#, \n",
    "       #'day_of_year_f52_240'#, 'day_of_year_f52_300'\n",
    "       \n",
    "       ]\n",
    "\n",
    "train0 = trainF + ['sales']\n",
    "\n",
    "train = data1.loc[(data1.dataT == 'train') & (data1.store_nbr == 1) & (data1.family == 18)] # family 18\n",
    "\n",
    "n_predictedValues = 16\n",
    "look_back = 100\n",
    "zScoreNorm = True\n",
    "\n",
    "sequence0 = []\n",
    "sequence1 = []\n",
    "labels = []\n",
    "\n",
    "# zscore over all values -> not ideal bc test data\n",
    "if zScoreNorm:\n",
    "    mean = train.sales.mean()\n",
    "    mean = 0 # modified zScore, not in mean = 0\n",
    "    std = train.sales.std()\n",
    "    train['sales'] = (train.sales - mean) / std\n",
    "\n",
    "for i in range(train.shape[0]-look_back-n_predictedValues):\n",
    "    startS0 = i\n",
    "    endS0 = startS0 + look_back\n",
    "    endS1 = endS0 + n_predictedValues\n",
    "    sequence0.append(train[train0].iloc[startS0:endS0])\n",
    "    sequence1.append(train[trainF].iloc[endS0:endS1])\n",
    "    labels.append(train['sales'].iloc[endS0:endS1])\n",
    "sequence0 = np.stack(sequence0, axis = 0)\n",
    "sequence1 = np.stack(sequence1, axis=0)\n",
    "labels    = np.stack(labels, axis = 0)\n",
    "\n",
    "\n",
    "try:\n",
    "    tf.keras.utils.set_random_seed(42)\n",
    "except:\n",
    "    print('using new tf')\n",
    "\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "n_features = len(train0)\n",
    "\n",
    "input1 = Input(shape=(look_back, n_features))\n",
    "input2 = Input(shape=(n_predictedValues, n_features-1))\n",
    "\n",
    "lstm1 = LSTM(64, activation='relu', return_sequences=False)(input1)\n",
    "lstm2 = LSTM(64, activation='relu', return_sequences=False)(input2)\n",
    "\n",
    "#lstm1 = LSTM(64, activation='relu', return_sequences=False, kernel_regularizer=regularizers.l2(0.001))(lstm1)\n",
    "#lstm2 = LSTM(64, activation='relu', return_sequences=False, kernel_regularizer=regularizers.l2(0.001))(lstm2)\n",
    "\n",
    "#lstm2 = Dense(n_predictedValues, activation='relu')(lstm2)\n",
    "x = tf.keras.layers.concatenate([lstm1, lstm2])\n",
    "x = Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "if zScoreNorm:\n",
    "    output = Dense(n_predictedValues, activation='relu')(x)\n",
    "else:\n",
    "    output = Dense(n_predictedValues, activation='relu')(x)\n",
    "\n",
    "# Define the model\n",
    "model = Model(inputs=[input1, input2], outputs=output)\n",
    "\n",
    "optimizer = Adam(learning_rate=0.00001)\n",
    "model.compile(optimizer='adam', loss='mae', metrics=[tf.keras.losses.MSLE]) \n",
    "#model.compile(optimizer='adam', loss=tf.keras.losses.MSLE, metrics=['mae']) \n",
    "\n",
    "n_splits = 5\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "X = [sequence0,sequence1]\n",
    "y = labels\n",
    "\n",
    "for train_index, test_index in tscv.split(sequence0):\n",
    "    X_train = [sequence0[train_index],sequence1[train_index]]\n",
    "    X_test  = [sequence0[test_index], sequence1[test_index]]\n",
    "    y_train, y_test = labels[train_index], labels[test_index]\n",
    "\n",
    "    model.fit(X_train, y_train, epochs=20, batch_size=32,validation_data=(X_test, y_test))\n",
    "\n",
    "\n",
    "    forecast = model.predict(X_train, verbose=False)\n",
    "    if zScoreNorm:\n",
    "        forecast = forecast *std  + mean\n",
    "        y_train = y_train *std + mean\n",
    "    rmsleTrain = np.sqrt(mean_squared_log_error(forecast, y_train))\n",
    "    forecast = model.predict(X_test, verbose=False)\n",
    "    if zScoreNorm:\n",
    "        forecast = forecast *std  + mean\n",
    "        y_test = y_test*std + mean\n",
    "    rmsleTest = np.sqrt(mean_squared_log_error(forecast, y_test))\n",
    "    print('errors:  ', round(rmsleTrain,3), round(rmsleTest,3), y_train.shape[0], y_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test, forecast\n",
    "# --------------seed = 0 -------------------------------------------------------------------------------------------------------------\n",
    "# --------------familyId = 3 (Beverages), store id = 1 -----------(train 1307 test 261)---------- 7.219 7.652 == all 0 ----------------\n",
    "# --------------5 splits, 10 epochs per split, 32 batch size---------------------------------------------------------------------------\n",
    "# errors:   0.856 0.740     without all the time featuers: 'dcoilwtico', 'holidayType','description', 'transferred', 'linear_time', 'day_of_year','weekday', 'month',\n",
    "# errors:   7.219 7.652     with all the time featuers\n",
    "# errors:   2.885 4.493     without oil a lot worse :o     'holidayType','description', 'transferred', 'linear_time', 'day_of_year','weekday', 'month',\n",
    "# errors:   1.533 2.071     without descrip.transf         'dcoilwtico', 'holidayType', 'linear_time', 'day_of_year','weekday', 'month',\n",
    "# errors:   1.262 1.156     without time features          'dcoilwtico', 'holidayType','description', 'transferred',\n",
    "# errors:   7.219 7.653     without day of year & month    'dcoilwtico', 'holidayType','description', 'transferred', 'linear_time','weekday',\n",
    "# errors:   7.219 7.653     without month                  'dcoilwtico', 'holidayType','description', 'transferred', 'linear_time', 'day_of_year','weekday',\n",
    "# errors:   1.247 1.128     without lin time & day of y    'dcoilwtico', 'holidayType','description', 'transferred','weekday', 'month',\n",
    "# errors:   7.219 7.653     without day of year            'dcoilwtico', 'holidayType','description', 'transferred', 'linear_time','weekday', 'month',\n",
    "\n",
    "# testing fourier features, lock those features (always use) 'dcoilwtico', 'holidayType','description', 'transferred', 'linear_time', 'day_of_year','weekday', 'month',\n",
    "# errors:   0.624 0.694     frequency: 12, 24, 52, 104 only 0 phase diff:       day_of_year_f12_0, day_of_year_f104_0, day_of_year_f24_0, day_of_year_f52_0\n",
    "# errors:   3.162 3.661     frequency: 12, 24, 52, 104  6x 60° phase diff:\n",
    "# errors ->nans  frequency: 12, 24, 52, 104 only 0 & 180°:\n",
    "# errors:   0.705 0.731     frequency: 12, 24, 52, 104 only 0, 120, 240° phase diff:\n",
    "# errors:   6.92  7.155     frequency: 1,2,3,4,6,12, 24, 52, 104 only 0 phase diff:\n",
    "# errors:   7.211 7.595     frequency: 1,2,6,12, 24, 52, 104 only 0 phase diff:\n",
    "# errors:   4.929 5.107     frequency: 6,12, 24, 52, 104 only 0 phase diff:\n",
    "# errors ->nans  frequency: 4, 12, 24, 52, 104 only 0:\n",
    "# errors:   2.155 1.544     frequency: 3,12, 24, 52, 104 only 0 phase diff:\n",
    "# errors ->nans  frequency: 1, 12, 24, 52, 104 only 0:\n",
    "# errors ->nans  frequency: 2, 12, 24, 52, 104 only 0:\n",
    "\n",
    "# --------------familyId = 3 (Beverages), store id = 18 ----------(train 1307 test 261)---------- 3.873 5.123 == all 0 ----------------\n",
    "# --------------5 splits, 10 epochs per split, 32 batch size---------------------------------------------------------------------------\n",
    "#errors:   3.873 5.123      'dcoilwtico', 'holidayType','description', 'transferred', 'linear_time', 'day_of_year','weekday', 'month', frequency: 12, 24, 52, 104 only 0 phase diff\n",
    "# -> zscaling doesn't help, nothing really works for this one\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(forecast != 0).any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict in one big dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = (data1.loc[data1.dataT == 'train'].pivot(index='date', columns=['store_nbr', 'family']))#.transpose#()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "17f27e0cbed1f29822e509a94c958ed0e80b7d9abe162097755686a614de4732"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
