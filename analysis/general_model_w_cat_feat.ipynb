{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import plotly.offline as pyo\n",
    "from plotly import subplots\n",
    "import plotly.graph_objects as go\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.tsa.api as smt\n",
    "\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from tensorflow.keras.models import Sequential\n",
    "#from tensorflow.keras.layers import LSTM, Dense,Input,concatenate\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import lightgbm as lgb\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "from baseFunctions import *\n",
    "from data_helpers import processData6, featureEngineering, getSequencesFast, removeOutliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, propDicts, flippedPropDicts = processData6()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, timeFeatures = featureEngineering(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train 1 lgbm per family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainF = [\n",
    "       'store_nbr', 'family', \n",
    "       #'sales', \n",
    "       'onpromotion',# 'dataT',\n",
    "       'city', 'state', 'type', 'cluster', 'dcoilwtico', 'holidayType',\n",
    "       'description', 'transferred', \n",
    "       #'transactions', \n",
    "       'store_closed',\n",
    "       'linear_time', 'day_of_year', 'day_of_year_f12_0', 'day_of_year_f104_0',\n",
    "       'day_of_year_f24_0', 'day_of_year_f52_0', 'weekday', 'month'\n",
    "       ]\n",
    "\n",
    "trainF2 = trainF + ['sales']\n",
    "\n",
    "\n",
    "\n",
    "n_predictedValues = 16\n",
    "look_back = 42\n",
    "zScoreNorm = True\n",
    "\n",
    "# Date string\n",
    "date_string = \"2017-05-01\"\n",
    "date_object = datetime.strptime(date_string, '%Y-%m-%d')\n",
    "days_ago = date_object - timedelta(days=(look_back + n_predictedValues -1 +2))\n",
    "days_ago_string = days_ago.strftime('%Y-%m-%d')\n",
    "days_ago_string\n",
    "\n",
    "maskTrain = data.date < date_string\n",
    "maskTest = data.date  >  days_ago_string #\"2017-03-03\" #42days + 15 day between (15 because we only want to iterate one value more from the test set)\n",
    "\n",
    "log = {}\n",
    "\n",
    "train = data.loc[data.dataT == 'train']\n",
    "\n",
    "for familyId in data.family.unique():\n",
    "    print(familyId)\n",
    "\n",
    "    familyDf = train.loc[train.family==familyId]\n",
    "\n",
    "    \n",
    "    stdDict = {}\n",
    "    meanDict= {}\n",
    "    X_train = []\n",
    "    X_test  = []\n",
    "    y_train = []\n",
    "    y_test  = []\n",
    "    std_train, std_test = [], []\n",
    "    mean_train, mean_test = [], []\n",
    "    init= False\n",
    "    for storeId in data.store_nbr.unique():\n",
    "           storeDf = familyDf.loc[(familyDf.store_nbr == storeId)]\n",
    "\n",
    "           X_train0,y_train0,mean,std = getSequencesFast(storeDf.loc[maskTrain], trainF, look_back, n_predictedValues, zScoreNorm=zScoreNorm)\n",
    "           X_test0, y_test0           = getSequencesFast(storeDf.loc[maskTest], trainF, look_back, n_predictedValues, zScoreNorm=False, applyZScoreNorm=True, meanZ=mean, stdZ=std)\n",
    "\n",
    "           stdDict[storeId] = std\n",
    "           meanDict[storeId] = mean\n",
    "           if init:\n",
    "                  X_train = np.concatenate((X_train, X_train0), axis=0)\n",
    "                  X_test  = np.concatenate((X_test,  X_test0), axis=0)\n",
    "                  y_train = np.concatenate((y_train, y_train0), axis=0)\n",
    "                  y_test  = np.concatenate((y_test,  y_test0), axis=0)\n",
    "                  std_train = np.concatenate((std_train, np.ones(y_train0.shape)*std), axis=0)\n",
    "                  std_test  = np.concatenate((std_test,  np.ones(y_test0.shape)*std), axis=0)\n",
    "                  mean_train = np.concatenate((mean_train, np.ones(y_train0.shape)*mean), axis=0)\n",
    "                  mean_test = np.concatenate((mean_test, np.ones(y_test0.shape)*mean), axis=0)\n",
    "           else:\n",
    "                  X_train, X_test, y_train,y_test = X_train0, X_test0, y_train0, y_test0\n",
    "                  std_train  =np.ones(y_train0.shape)*std\n",
    "                  std_test   =np.ones(y_test0.shape)*std\n",
    "                  mean_train =np.ones(y_train0.shape)*mean\n",
    "                  mean_test  =np.ones(y_test0.shape)*mean\n",
    "                  init=True \n",
    "\n",
    "\n",
    "    # Set parameters for LGBM model\n",
    "    params = {\n",
    "        'objective': 'regression',  # Assuming you're doing regression\n",
    "        'metric': 'rmsle',  # Mean squared error\n",
    "        'num_leaves': 31,\n",
    "        'learning_rate': 0.05,\n",
    "        'feature_fraction': 0.9,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 5,\n",
    "        'verbose': 0\n",
    "    }   \n",
    "\n",
    "\n",
    "    # Train the model\n",
    "    num_round = 10  \n",
    "\n",
    "    gbms = [lgb.train(params, lgb.Dataset(X_train, label=y_train[:, i]),num_round, valid_sets=[lgb.Dataset(X_test, label=y_test[:,i])]) for i in range(y_train.shape[1])]   \n",
    "\n",
    "    forecast = np.column_stack([gbm.predict(X_train, num_iteration=gbm.best_iteration) for gbm in gbms])\n",
    "    if (forecast<0).any():\n",
    "        print('negative values!!!')\n",
    "        forecast = np.clip(forecast, 0, 1e29)\n",
    "    if zScoreNorm:\n",
    "        forecast = forecast *std_train  + mean_train\n",
    "        y_train = y_train *std_train + mean_train\n",
    "    rmsleTrain = np.sqrt(mean_squared_log_error(forecast, y_train))\n",
    "    forecast = np.column_stack([gbm.predict(X_test, num_iteration=gbm.best_iteration) for gbm in gbms])\n",
    "    if zScoreNorm:\n",
    "        forecast = forecast *std_test  + mean_test\n",
    "        y_test = y_test*std_test + mean_test\n",
    "    rmsleTest = np.sqrt(mean_squared_log_error(forecast, y_test))\n",
    "    print('familyId:', familyId, 'errors:  ', round(rmsleTrain,3), round(rmsleTest,3), y_train.shape[0], y_test.shape[0])\n",
    "\n",
    "    subD = {'trainE':round(rmsleTrain,3),'testE':round(rmsleTest,3),'gbms':gbms}\n",
    "    log[familyId] = subD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Save the dictionary as a pickle file\n",
    "with open('log_moedle_per_family_S42_16_normalized.pkl', 'wb') as f:\n",
    "    #pickle.dump(log, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the dictionary back from the file\n",
    "with open('log_moedle_per_family_S42_16_normalized.pkl', 'rb') as f:\n",
    "    log = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# investigaet per family approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ids.shape, X_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred.shape, X_test0.shape, X_train0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for familyId in data.family.unique():\n",
    "    print(familyId, log[familyId]['trainE'], log[familyId]['testE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainF = [\n",
    "       'store_nbr', 'family', \n",
    "       #'sales', \n",
    "       'onpromotion',# 'dataT',\n",
    "       'city', 'state', 'type', 'cluster', 'dcoilwtico', 'holidayType',\n",
    "       'description', 'transferred', \n",
    "       #'transactions', \n",
    "       'store_closed',\n",
    "       'linear_time', 'day_of_year', 'day_of_year_f12_0', 'day_of_year_f104_0',\n",
    "       'day_of_year_f24_0', 'day_of_year_f52_0', 'weekday', 'month'\n",
    "       ]\n",
    "\n",
    "trainF2 = trainF + ['sales']\n",
    "\n",
    "\n",
    "\n",
    "n_predictedValues = 16\n",
    "look_back = 42\n",
    "zScoreNorm = True\n",
    "\n",
    "# Date string\n",
    "date_string = \"2017-05-01\"\n",
    "date_object = datetime.strptime(date_string, '%Y-%m-%d')\n",
    "days_ago = date_object - timedelta(days=(look_back + n_predictedValues -1 +2))\n",
    "days_ago_string = days_ago.strftime('%Y-%m-%d')\n",
    "days_ago_string\n",
    "\n",
    "maskTrain = data.date < date_string\n",
    "maskTest = data.date  >  days_ago_string #\"2017-03-03\" #42days + 15 day between (15 because we only want to iterate one value more from the test set)\n",
    "\n",
    "log = {}\n",
    "\n",
    "train = data.loc[data.dataT == 'train']\n",
    "test = data.loc[data.dataT == 'test']\n",
    "predDf = []\n",
    "\n",
    "for familyId in data.family.unique(): # [22]: # 31 #\n",
    "    print(familyId)\n",
    "    familyDf = train.loc[train.family==familyId]\n",
    "    familyDfTest = test.loc[test.family==familyId]\n",
    "\n",
    "    \n",
    "    stdDict = {}\n",
    "    meanDict= {}\n",
    "    X_train = []\n",
    "    X_test  = []\n",
    "    y_train = []\n",
    "    y_test  = []\n",
    "    X_pred = []\n",
    "    std_train, std_test, std_pred = [], [], []\n",
    "    mean_train, mean_test, mean_pred = [], [], []\n",
    "    pred_ids = []\n",
    "    init= False\n",
    "    for storeId in data.store_nbr.unique():\n",
    "        storeDf = familyDf.loc[(familyDf.store_nbr == storeId)]\n",
    "        storeDf = storeDf.loc[storeDf.date > \"2015-07-01\"]\n",
    "\n",
    "        storeDf = removeOutliers(storeDf)\n",
    "        storeDf.loc[:,'sales'] = storeDf['sales_outRem']\n",
    "         \n",
    "\n",
    "        X_train0,y_train0,mean,std = getSequencesFast(storeDf.loc[maskTrain], trainF, look_back, n_predictedValues, zScoreNorm=zScoreNorm)\n",
    "        X_test0, y_test0           = getSequencesFast(storeDf.loc[maskTest], trainF, look_back, n_predictedValues, zScoreNorm=False, applyZScoreNorm=zScoreNorm, meanZ=mean, stdZ=std)\n",
    "\n",
    "        # prepare data for prediction\n",
    "        storeDfTest = familyDfTest.loc[familyDfTest.store_nbr == storeId]\n",
    "        #scale all  sales\n",
    "\n",
    "        pastS   = storeDf[trainF2].tail(look_back)                #past sequence\n",
    "        pastS.loc[:,'sales'] = (pastS.sales-mean)/std\n",
    "        futureS = storeDfTest[trainF].to_numpy()                  #future sequence\n",
    "        ids = storeDfTest.id.to_numpy()\n",
    "        ids = np.reshape(ids, (1,-1))\n",
    "\n",
    "        a1 = np.reshape(pastS.to_numpy(), (1,-1))\n",
    "        b1 = np.reshape(futureS, (1,-1))\n",
    "        X_pred0 = np.concatenate((a1,b1), axis=1)\n",
    "\n",
    "        stdDict[storeId] = std\n",
    "        meanDict[storeId] = mean\n",
    "        if init:\n",
    "               X_train = np.concatenate((X_train, X_train0), axis=0)\n",
    "               X_test  = np.concatenate((X_test,  X_test0), axis=0)\n",
    "               X_pred  = np.concatenate((X_pred, X_pred0), axis=0)\n",
    "               y_train = np.concatenate((y_train, y_train0), axis=0)\n",
    "               y_test  = np.concatenate((y_test,  y_test0), axis=0)\n",
    "               std_train = np.concatenate((std_train, np.ones(y_train0.shape)*std), axis=0)\n",
    "               std_test  = np.concatenate((std_test,  np.ones(y_test0.shape)*std), axis=0)\n",
    "               std_pred  = np.concatenate((std_pred,  np.ones(ids.shape)*std), axis=0)\n",
    "               mean_train = np.concatenate((mean_train, np.ones(y_train0.shape)*mean), axis=0)\n",
    "               mean_test = np.concatenate((mean_test, np.ones(y_test0.shape)*mean), axis=0)\n",
    "               pred_ids = np.concatenate((pred_ids, ids), axis = 0)\n",
    "        else:\n",
    "               X_train, X_test, y_train,y_test = X_train0, X_test0, y_train0, y_test0\n",
    "               X_pred = X_pred0\n",
    "               std_train  =np.ones(y_train0.shape)*std\n",
    "               std_test   =np.ones(y_test0.shape)*std\n",
    "               std_pred   =np.ones(ids.shape)*std\n",
    "               mean_train =np.ones(y_train0.shape)*mean\n",
    "               mean_test  =np.ones(y_test0.shape)*mean\n",
    "               pred_ids = ids\n",
    "               init=True \n",
    "    \n",
    "\n",
    "    # Set parameters for LGBM model\n",
    "    params = {\n",
    "        'objective': 'regression',  # Assuming you're doing regression\n",
    "        'metric': 'rmsle',  # Mean squared error\n",
    "        'num_leaves': 31,\n",
    "        'learning_rate': 0.05,\n",
    "        'feature_fraction': 0.9,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 5,\n",
    "        'verbose': 0\n",
    "    }   \n",
    "\n",
    "\n",
    "    # Train the model\n",
    "    num_round = 100  \n",
    "\n",
    "    gbms = [lgb.train(params, lgb.Dataset(X_train, label=y_train[:, i]),num_round, valid_sets=[lgb.Dataset(X_test, label=y_test[:,i])]) for i in range(y_train.shape[1])]   \n",
    "\n",
    "    forecast = np.column_stack([gbm.predict(X_train, num_iteration=gbm.best_iteration) for gbm in gbms])\n",
    "    if (forecast<0).any():\n",
    "        print('negative values!!!')\n",
    "        forecast = np.clip(forecast, 0, 1e29)\n",
    "    if zScoreNorm:\n",
    "        forecast = forecast *std_train  + mean_train\n",
    "        y_train = y_train *std_train + mean_train\n",
    "    rmsleTrain = np.sqrt(mean_squared_log_error(forecast, y_train))\n",
    "    forecast = np.column_stack([gbm.predict(X_test, num_iteration=gbm.best_iteration) for gbm in gbms])\n",
    "    if (forecast<0).any():\n",
    "        print('negative values!!!')\n",
    "        forecast = np.clip(forecast, 0, 1e29)\n",
    "    if zScoreNorm:\n",
    "        forecast = forecast *std_test  + mean_test\n",
    "        y_test = y_test*std_test + mean_test\n",
    "    rmsleTest = np.sqrt(mean_squared_log_error(forecast, y_test))\n",
    "    print('familyId:', familyId, 'errors:  ', round(rmsleTrain,3), round(rmsleTest,3), y_train.shape[0], y_test.shape[0])\n",
    "\n",
    "    forecast = np.column_stack([gbm.predict(X_pred, num_iteration=gbm.best_iteration) for gbm in gbms])\n",
    "    if (forecast<0).any():\n",
    "        print('negative values!!!')\n",
    "        forecast = np.clip(forecast, 0, 1e29)\n",
    "    if zScoreNorm:\n",
    "        forecast = forecast *std_pred  #+ mean_test\n",
    "\n",
    "    a = pd.DataFrame()\n",
    "    a['sales'] = np.reshape(forecast, (-1))\n",
    "    a['id'] = np.reshape(pred_ids, (-1))\n",
    "    predDf.append(a)\n",
    "\n",
    "    subD = {'trainE':round(rmsleTrain,3),'testE':round(rmsleTest,3),'gbms':gbms}\n",
    "    log[familyId] = subD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pd.concat(predDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.sort_values(by='id').set_index('id').to_csv('predictions_familyWiseLBGM.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast, pred_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default, only training data after july 1 2015, after \"2017-05-01\" test data\n",
    "# 42 / 16 sequence length, no outlier removal\n",
    "\n",
    "#familyId: 22 errors:   1.33 0.682 32886 5778\n",
    "#familyId: 31 errors:   0.774 0.714 32886 5778\n",
    "\n",
    "# all training data \n",
    "#familyId: 22 errors:   1.686 0.677 82026 5778\n",
    "#familyId: 31 errors:   0.661 0.492 82026 5778\n",
    "\n",
    "# remove outliers\n",
    "#familyId: 22 errors:   1.315 0.662 32886 5778\n",
    "\n",
    "# numrounds = 100 instead of 10 + outlier removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train 1 lbgm for everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainF = [\n",
    "       'store_nbr', 'family', \n",
    "       #'sales', \n",
    "       'onpromotion',# 'dataT',\n",
    "       'city', 'state', 'type', 'cluster', 'dcoilwtico', 'holidayType',\n",
    "       'description', 'transferred', \n",
    "       #'transactions', \n",
    "       'store_closed',\n",
    "       'linear_time', 'day_of_year', 'day_of_year_f12_0', 'day_of_year_f104_0',\n",
    "       'day_of_year_f24_0', 'day_of_year_f52_0', 'weekday', 'month'\n",
    "       ]\n",
    "\n",
    "trainF2 = trainF + ['sales']\n",
    "\n",
    "\n",
    "\n",
    "n_predictedValues = 16\n",
    "look_back = 42\n",
    "zScoreNorm = True\n",
    "\n",
    "# Date string\n",
    "date_string = \"2017-05-01\"\n",
    "date_object = datetime.strptime(date_string, '%Y-%m-%d')\n",
    "days_ago = date_object - timedelta(days=(look_back + n_predictedValues -1 +2))\n",
    "days_ago_string = days_ago.strftime('%Y-%m-%d')\n",
    "days_ago_string\n",
    "\n",
    "maskTrain = data.date < date_string\n",
    "maskTest = data.date  >  days_ago_string #\"2017-03-03\" #42days + 15 day between (15 because we only want to iterate one value more from the test set)\n",
    "\n",
    "stdDict = {}\n",
    "meanDict= {}\n",
    "X_train = []\n",
    "X_test  = []\n",
    "y_train = []\n",
    "y_test  = []\n",
    "std_train, std_test = [], []\n",
    "mean_train, mean_test = [], []\n",
    "init= False\n",
    "\n",
    "train = data.loc[data.dataT == 'train']\n",
    "\n",
    "for familyId in data.family.unique():\n",
    "    familyDf = train.loc[train.family==familyId]\n",
    "    for storeId in data.store_nbr.unique():\n",
    "           storeDf = familyDf.loc[(familyDf.store_nbr == storeId)]\n",
    "\n",
    "           X_train0,y_train0,mean,std = getSequencesFast(storeDf.loc[maskTrain], trainF, look_back, n_predictedValues, zScoreNorm=zScoreNorm)\n",
    "           X_test0, y_test0           = getSequencesFast(storeDf.loc[maskTest], trainF, look_back, n_predictedValues, zScoreNorm=False, applyZScoreNorm=True, meanZ=mean, stdZ=std)\n",
    "\n",
    "           stdDict[storeId] = std\n",
    "           meanDict[storeId] = mean\n",
    "           if init:\n",
    "                  X_train = np.concatenate((X_train, X_train0), axis=0)\n",
    "                  X_test  = np.concatenate((X_test,  X_test0), axis=0)\n",
    "                  y_train = np.concatenate((y_train, y_train0), axis=0)\n",
    "                  y_test  = np.concatenate((y_test,  y_test0), axis=0)\n",
    "                  std_train = np.concatenate((std_train, np.ones(y_train0.shape)*std), axis=0)\n",
    "                  std_test  = np.concatenate((std_test,  np.ones(y_test0.shape)*std), axis=0)\n",
    "                  mean_train = np.concatenate((mean_train, np.ones(y_train0.shape)*mean), axis=0)\n",
    "                  mean_test = np.concatenate((mean_test, np.ones(y_test0.shape)*mean), axis=0)\n",
    "           else:\n",
    "                  X_train, X_test, y_train,y_test = X_train0, X_test0, y_train0, y_test0\n",
    "                  std_train  =np.ones(y_train0.shape)*std\n",
    "                  std_test   =np.ones(y_test0.shape)*std\n",
    "                  mean_train =np.ones(y_train0.shape)*mean\n",
    "                  mean_test  =np.ones(y_test0.shape)*mean\n",
    "                  init=True \n",
    "\n",
    "print('processed data: done')\n",
    "\n",
    "# Set parameters for LGBM model\n",
    "params = {\n",
    "    'objective': 'regression',  # Assuming you're doing regression\n",
    "    'metric': 'rmsle',  # Mean squared error\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "\n",
    "# Train the model\n",
    "num_round = 10\n",
    "\n",
    "gbms = [lgb.train(params, lgb.Dataset(X_train, label=y_train[:, i]),num_round, valid_sets=[lgb.Dataset(X_test, label=y_test[:,i])]) for i in range(y_train.shape[1])]\n",
    "\n",
    "forecast = np.column_stack([gbm.predict(X_train, num_iteration=gbm.best_iteration) for gbm in gbms])\n",
    "if (forecast<0).any():\n",
    "    print('negative values!!!')\n",
    "    forecast = np.clip(forecast, 0, 1e29)\n",
    "if zScoreNorm:\n",
    "    forecast = forecast *std_train  + mean_train\n",
    "    y_train = y_train *std_train + mean_train\n",
    "rmsleTrain = np.sqrt(mean_squared_log_error(forecast, y_train))\n",
    "forecast = np.column_stack([gbm.predict(X_test, num_iteration=gbm.best_iteration) for gbm in gbms])\n",
    "if zScoreNorm:\n",
    "    forecast = forecast *std_test  + mean_test\n",
    "    y_test = y_test*std_test + mean_test\n",
    "rmsleTest = np.sqrt(mean_squared_log_error(forecast, y_test))\n",
    "print('errors:  ', round(rmsleTrain,3), round(rmsleTest,3), y_train.shape[0], y_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('train_test_Sequences_42_tp_16_normalized.npz', arr1=X_train, arr2=y_train, arr3=std_train, arr4=X_test, arr5=y_test, arr6=std_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
