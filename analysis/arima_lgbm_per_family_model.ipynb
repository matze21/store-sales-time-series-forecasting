{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import plotly.offline as pyo\n",
    "from plotly import subplots\n",
    "import plotly.graph_objects as go\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.tsa.api as smt\n",
    "\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from tensorflow.keras.models import Sequential\n",
    "#from tensorflow.keras.layers import LSTM, Dense,Input,concatenate\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import lightgbm as lgb\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from baseFunctions import *\n",
    "from data_helpers import processData6, featureEngineering, getSequencesFast, removeOutliers, create_sequences, processAllData\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import plotly.express as px\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, propDicts, flippedPropDicts = processData6()\n",
    "data, timeFeatures = featureEngineering(data,splits=[2,2,2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_lag = 16 # 16 = independent from previous predictions\n",
    "targetLags = [1,2,3,4,5,6,7,8,9,10]\n",
    "featureLags = [1,2,3,4,5,6,7,8,9,10]\n",
    "rolling = [7,21]\n",
    "\n",
    "data, addedF = processAllData(data, targetLags, featureLags, rolling, initial_lag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcLossArima(pred, y, logTransform):\n",
    "    if logTransform:\n",
    "        a = np.exp(pred) -1\n",
    "        y = np.exp(y) -1 \n",
    "    else:\n",
    "        a = (pred)\n",
    "        y = (y)\n",
    "\n",
    "    if (a < 0).any():\n",
    "        a = np.clip(a, 0, 1e20)\n",
    "\n",
    "    rmsleTrain = np.sqrt(mean_squared_log_error(a,y))\n",
    "    return rmsleTrain\n",
    "\n",
    "def calcLossLGBM(pred, y):\n",
    "    logPred = np.reshape(pred, (pred.shape[0],1))\n",
    "    a = np.exp(logPred) -1\n",
    "\n",
    "    logy = np.reshape(y, (pred.shape[0],1))\n",
    "    y = np.exp(logy) -1\n",
    "\n",
    "    rmsleTrain = np.sqrt(np.mean((a-y)**2))\n",
    "    return rmsleTrain\n",
    "def mse(pred,y):\n",
    "    return np.sqrt(np.mean((pred-y)**2))\n",
    "def calcLossLGBMArima1(pred, sales, arima, predictDiff):\n",
    "    if predictDiff:\n",
    "        pred = np.reshape(pred, (pred.shape[0],1)) + np.reshape(arima, (pred.shape[0],1))\n",
    "    a = np.exp(pred) -1\n",
    "\n",
    "    if (a < 0).any():\n",
    "        a = np.clip(a, 0, 1e20)\n",
    "    rmsleTrain = np.sqrt(mean_squared_log_error(a,sales))\n",
    "    return rmsleTrain\n",
    "def calcLossLGBMArima2(pred, y, arima, predictDiff):\n",
    "    if predictDiff:\n",
    "        pred = np.reshape(pred, (pred.shape[0],1)) + np.reshape(arima, (pred.shape[0],1))\n",
    "        y = np.reshape(y, (pred.shape[0],1)) + np.reshape(arima, (pred.shape[0],1))\n",
    "    a = np.exp(pred) -1\n",
    "    y = np.exp(y) -1\n",
    "\n",
    "    if (a < 0).any():\n",
    "        a = np.clip(a, 0, 1e20)\n",
    "    rmsleTrain = np.sqrt(mean_squared_log_error(a,y))\n",
    "    return rmsleTrain\n",
    "def plotLGBM(i, logTransform, salesDomain, pred, y, base):\n",
    "    if salesDomain:\n",
    "        pred = np.reshape(pred, (pred.shape[0],1)) + np.reshape(base, (pred.shape[0],1))\n",
    "        y = np.reshape(y, (pred.shape[0],1)) + np.reshape(base, (pred.shape[0],1))\n",
    "    if logTransform:\n",
    "        a = np.exp(pred[i:i+16])-1\n",
    "        y = np.exp(y[i:i+16]) -1\n",
    "        arima = np.exp(base[i:i+16]) - 1\n",
    "    else:\n",
    "        a = (pred[i:i+16])\n",
    "        y = (y[i:i+16])\n",
    "        arima = base[i:i+16]\n",
    "    x = range(len(a))   \n",
    "\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=1, figsize=(8, 6))\n",
    "    axs.plot(x, y, color='blue',label='Original')\n",
    "    axs.plot(x, a, color='red',label='pred')\n",
    "    axs.plot(x, arima, color='orange',label='arima')\n",
    "    axs.set_title('index: '+str(i))\n",
    "    fig.subplots_adjust(hspace=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" define features and test/train partition \"\"\"\n",
    "trainF = [\n",
    "       'store_nbr', 'family', \n",
    "       #'sales', \n",
    "       'onpromotion',# 'dataT',\n",
    "       'city', 'state', 'type', 'cluster', \n",
    "       'dcoilwtico', \n",
    "       'holidayType',\n",
    "       'description', \n",
    "       'transferred', \n",
    "       #'transactions', \n",
    "       'store_closed']\n",
    "timeF = [\n",
    "       'linear_time', 'day_of_year', 'day_of_year_f12_0', 'day_of_year_f104_0','day_of_year_f24_0',  'day_of_year_f52_0',\n",
    "       'day_of_year_f12_180', 'day_of_year_f104_180','day_of_year_f24_180','day_of_year_f52_180', \n",
    "       'weekday', 'month'\n",
    "       ]\n",
    "\n",
    "allF = addedF + trainF + timeF\n",
    "\n",
    "data['predTarget'] = np.nan\n",
    "\n",
    "for family in data.family.unique():\n",
    "       date_string_test = \"2017-08-01\"    \n",
    "       train_bigDf = data.loc[(data.date < date_string_test)  & (data.dataT =='train') & (data.family ==family)] #& (data.date > \"2015-05-01\")\n",
    "       test_bigDf  = data.loc[(data.date >= date_string_test) & (data.dataT !='test') & (data.family ==family)]\n",
    "       val_bigDf   = data.loc[ (data.dataT =='test') & (data.family ==family)]     \n",
    "\n",
    "       predictDiff16 = True\n",
    "       targetF, refF = 'target', 'ref'\n",
    "       logTransform16 = True\n",
    "\n",
    "       X_train16, X_test16, X_pred16 = train_bigDf[allF],test_bigDf[allF],val_bigDf[allF]\n",
    "       y_train16, y_test16 = train_bigDf[targetF], test_bigDf[targetF]\n",
    "       baseTrain16, baseTest16, basePred16 = train_bigDf[refF], test_bigDf[refF],val_bigDf[refF]\n",
    "\n",
    "\n",
    "       # compare against base lgbm that just predicts always t+16\n",
    "       sample_weights = (train_bigDf.date > \"2017-06-01\") + 1#train_bigDf.logSales+1 #np.reshape(( * train_bigDf.onpromotion.std()) + train_bigDf.onpromotion.mean()+ 1, (-1,1))\n",
    "       sample_weights = sample_weights/ max(sample_weights)\n",
    "\n",
    "\n",
    "       # Set parameters for LGBM model\n",
    "       params = {\n",
    "           'boosting':'gbdt',#'gbdt', #'rf' #'dart'\n",
    "           'objective': 'regression',  # Assuming you're doing regression\n",
    "           'metric': 'mse',  # Mean squared error\n",
    "           'num_leaves': 7,\n",
    "           'feature_fraction': 0.9,\n",
    "           'bagging_fraction': 0.8,\n",
    "           'bagging_freq': 5,\n",
    "           'verbose': -1,\n",
    "           'force_col_wise':True,\n",
    "       }   \n",
    "\n",
    "       # Train the model\n",
    "       gbm = lgb.train(params, lgb.Dataset(X_train16, label=y_train16\n",
    "             # , weight=sample_weights\n",
    "              ), 2000,valid_sets=[lgb.Dataset(X_test16, label=y_test16)]#,num_boost_round=100\n",
    "       ,callbacks=[lgb.early_stopping(stopping_rounds=1000)]\n",
    "       )  \n",
    "       predtrainLGBM = gbm.predict(X_train16)\n",
    "       predtestLGBM = gbm.predict(X_test16)\n",
    "       predvalLGBM = gbm.predict(X_pred16)\n",
    "       print('---------------',family,'--------------------')\n",
    "       #print('arima errors: ', calcLossArima(train_subDf.ref, train_subDf.logSales, logTransform), calcLossArima(test_subDf.ref, test_subDf.logSales, logTransform))#, calcLossArima(val_subDf.ref, val_subDf.logSales, logTransform, baseTest))\n",
    "       print('arima errors: ', calcLossArima(train_bigDf.ref, train_bigDf.logSales, logTransform16), calcLossArima(test_bigDf.ref, test_bigDf.logSales, logTransform16))\n",
    "       #print('lgbm errors: ', calcLossLGBM(predtrainLGBM, y_train), calcLossLGBM(predtestLGBM, y_test))#, calcLossLGBM(predvalLGBM, y_val))\n",
    "\n",
    "       print('sales errors: ', mse(predtrainLGBM, y_train16), mse(predtestLGBM, y_test16))\n",
    "       print('sales errors: ', calcLossLGBMArima2(predtrainLGBM, y_train16, baseTrain16, predictDiff16), calcLossLGBMArima2(predtestLGBM, y_test16, baseTest16,predictDiff16))#, calcLossLGBMArima2(predvalLGBM, y_val,baseVal))\n",
    "       #print('sales errors: ', calcLossLGBMArima1(predtrainLGBM, train_subDf.sales, baseTrain), calcLossLGBMArima1(predtestLGBM, test_subDf.sales, baseTest), calcLossLGBMArima1(predvalLGBM, val_subDf.sales,baseVal))\n",
    "\n",
    "       data.loc[train_bigDf.index,['predTarget']] = predtrainLGBM\n",
    "       data.loc[test_bigDf.index,['predTarget']] = predtestLGBM\n",
    "       data.loc[val_bigDf.index,['predTarget']] = predvalLGBM\n",
    "\n",
    "# 0.399 test score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['logError'] = (data['predTarget'] + data['ref'] - data['logSales'])**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_string_test = \"2017-08-01\"    \n",
    "train_bigDf = data.loc[(data.date < date_string_test) & (data.date > \"2015-05-01\") & (data.dataT =='train')]\n",
    "test_bigDf  = data.loc[(data.date >= date_string_test) & (data.dataT !='test')]\n",
    "val_bigDf   = data.loc[ (data.dataT =='test')]\n",
    "\n",
    "np.sqrt(train_bigDf['logError'].mean()),np.sqrt(test_bigDf['logError'].mean())\n",
    "\n",
    "# (0.39592463725045063, 0.39190685821584076)   10 leaves\n",
    "# (0.40743601263950485, 0.39219064204608006)    5 leaves\n",
    "# (0.3744699019480408, 0.39237606632021116)    15 leaves\n",
    "\n",
    "# 0.38728236949929384, 0.3842126947865215 more features - no weight, vs 0.38269330293604925 0.38686356026792235 for general model\n",
    "# 0.38441747955591077, 0.38421150630874995 more feat, same as above, but more data (all past)\n",
    "# 0.38356193996058435, 0.38461630057313745 same above, but 7 trees -> 0.409 score\n",
    "\n",
    "# 0.36132912353473373, 0.3494057402494539 with cross validation & last data in train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cross validation pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['predTarget'] = np.nan\n",
    "print('watchout \"test\" data is included in training')\n",
    "\n",
    "for family in data.family.unique():\n",
    "    params = {\n",
    "               'boosting':'gbdt',#'gbdt', #'rf' #'dart'\n",
    "               'objective': 'regression',  # Assuming you're doing regression\n",
    "               'metric': 'mse',  # Mean squared error\n",
    "               'num_leaves': 10,\n",
    "               'feature_fraction': 0.9,\n",
    "               'bagging_fraction': 0.8,\n",
    "               'bagging_freq': 5,\n",
    "               'verbose': -1,\n",
    "               'force_col_wise':True,\n",
    "\n",
    "        #       colsample_bytree=0.7,\n",
    "        #learning_rate=0.055,\n",
    "        #min_child_samples=10,\n",
    "        #num_leaves=19,\n",
    "           }   \n",
    "    \n",
    "    date_string_test = \"2017-08-01\"   \n",
    "    train_bigDf = data.loc[(data.dataT =='train') & (data.family ==family)] #& (data.date > \"2015-05-01\")\n",
    "    test_bigDf  = data.loc[(data.date >= date_string_test) & (data.dataT !='test') & (data.family ==family)] # just for validation\n",
    "    val_bigDf   = data.loc[ (data.dataT =='test') & (data.family ==family)]   \n",
    "    \n",
    "    predictDiff16 = True\n",
    "    targetF, refF = 'target', 'ref'\n",
    "    logTransform16 = True\n",
    "    X_train16, X_test16, X_pred16 = train_bigDf[allF],test_bigDf[allF],val_bigDf[allF]\n",
    "    y_train16, y_test16 = train_bigDf[targetF], test_bigDf[targetF]\n",
    "    baseTrain16, baseTest16, basePred16 = train_bigDf[refF], test_bigDf[refF],val_bigDf[refF]\n",
    "    \n",
    "    nsplits=5\n",
    "    num_iter=2000\n",
    "    \n",
    "    kf = KFold(n_splits=nsplits, shuffle=False)# , random_state=42) #random doesn't help\n",
    "    splits = kf.split(X_train16,y_train16)\n",
    "    \n",
    "    cv_results = lgb.cv(\n",
    "        params,\n",
    "        lgb.Dataset(X_train16, label=y_train16\n",
    "        #, weight=sample_weights\n",
    "        ),\n",
    "        num_boost_round=num_iter,\n",
    "        folds=splits,\n",
    "        stratified=False,  # Set to True for stratified sampling in classification\n",
    "        #early_stopping_rounds=50,  # Stop if score doesn't improve for 50 rounds\n",
    "        metrics=['mse'],  # Evaluation metrics to track\n",
    "        seed=42,  # Set a seed for reproducibility\n",
    "        return_cvbooster=True\n",
    "        ,callbacks=[lgb.early_stopping(stopping_rounds=1000)]\n",
    "    )\n",
    "    \n",
    "    lenIter = len(cv_results['valid l2-mean'])\n",
    "    print('---------------',family,'--------------------')\n",
    "    print(cv_results['valid l2-mean'][lenIter-1], cv_results['valid l2-stdv'][lenIter-1])\n",
    "    trainLGBM, testLGBM, predLGBM = [],[],[]\n",
    "    init=False\n",
    "    for i in range(nsplits):\n",
    "        predtrainLGBM = cv_results['cvbooster'].boosters[i].predict(X_train16)\n",
    "        predtestLGBM = cv_results['cvbooster'].boosters[i].predict(X_test16)\n",
    "        predvalLGBM = cv_results['cvbooster'].boosters[i].predict(X_pred16)\n",
    "        if init:\n",
    "            trainLGBM = trainLGBM + predtrainLGBM\n",
    "            testLGBM = testLGBM + predtestLGBM\n",
    "            predLGBM = predLGBM + predvalLGBM\n",
    "        else:\n",
    "            trainLGBM = predtrainLGBM\n",
    "            testLGBM = predtestLGBM\n",
    "            predLGBM = predvalLGBM\n",
    "            init=True\n",
    "    \n",
    "        print('sales errors: ', calcLossLGBMArima2(predtrainLGBM, y_train16, baseTrain16, predictDiff16), calcLossLGBMArima2(predtestLGBM, y_test16, baseTest16,predictDiff16))\n",
    "    \n",
    "    trainLGBM = trainLGBM/nsplits\n",
    "    testLGBM = testLGBM/nsplits\n",
    "    predLGBM = predLGBM/nsplits\n",
    "    print('overall',calcLossLGBMArima2(trainLGBM, y_train16, baseTrain16, predictDiff16), calcLossLGBMArima2(testLGBM, y_test16, baseTest16,predictDiff16))\n",
    "    data.loc[train_bigDf.index,['predTarget']] = trainLGBM\n",
    "    data.loc[test_bigDf.index,['predTarget']] = testLGBM\n",
    "    data.loc[val_bigDf.index,['predTarget']] = predLGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#overall 0.5828023480900187 0.6080170446516043 7treest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM per family "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a bidirectional lstm per family  https://www.kaggle.com/code/markbquant/jul-21-timeseries-windows-rnn\n",
    " \n",
    "branch[i] = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(wsize, return_sequences=True))(inp)\n",
    "branch[i] = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(wsize))(branch[i])\n",
    "branch[i] = tf.keras.layers.Dense(wsize, activation=\"relu\")(branch[i])\n",
    "branch[i] = tf.keras.layers.Dense(wsize//2, activation=\"relu\")(branch[i])\n",
    "branch[i] = tf.keras.layers.Dense(1, activation=\"relu\")(branch[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analyze residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotDistOfTestTrain(feat, test, train,labelA ='test',labelB='train'):\n",
    "    a = test[[feat]].value_counts()\n",
    "    b = train[[feat]].value_counts()\n",
    "\n",
    "    indexa, indexb = [],[]\n",
    "    for i, val in enumerate(a.index):\n",
    "        indexa.append(val[0])\n",
    "    for i, val in enumerate(b.index):\n",
    "        indexb.append(val[0])\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.bar(indexa, a.values/np.sum(a), alpha=0.5, label=labelA)\n",
    "    ax.bar(indexb, b.values/np.sum(b), alpha=0.5, label=labelB)\n",
    "    ax.set_title('Distribution of' + feat)\n",
    "    ax.set_xlabel('Category')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    plt.xticks(rotation=90)\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "def display2Ddifferences(feata, featb, dfa, dfb):\n",
    "    a = dfa[[feata,featb]].value_counts().sort_index()\n",
    "    b = dfb[[feata,featb]].value_counts().sort_index()\n",
    "    xa, xb = [],[]\n",
    "    ya, yb = [],[]\n",
    "    for i, val in enumerate(a.index):\n",
    "        xa.append(val[0])\n",
    "        ya.append(val[1])\n",
    "    for i, val in enumerate(b.index):\n",
    "        xb.append(val[0])\n",
    "        yb.append(val[1])\n",
    "\n",
    "    df1 = pd.DataFrame()\n",
    "    df1[feata] = xa\n",
    "    df1[featb]=ya\n",
    "    df1['f'] = a.values/np.sum(a)*100\n",
    "\n",
    "    df2 = pd.DataFrame()\n",
    "    df2[feata] = xb\n",
    "    df2[featb]=yb\n",
    "    df2['f'] = b.values/np.sum(b)*100\n",
    "\n",
    "\n",
    "    fig = px.scatter(df1,\n",
    "        y=feata,\n",
    "    x=featb,\n",
    "        color='f',title='bad'\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "    fig1 = px.scatter(df2,\n",
    "        y=feata,\n",
    "        x=featb,\n",
    "        color='f',title='good'\n",
    "    )\n",
    "    fig1.show()\n",
    "\n",
    "    return df1, df2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bigDf = data.loc[(data.date < date_string_test) & (data.date > \"2015-05-01\") & (data.dataT =='train')]\n",
    "test_bigDf  = data.loc[(data.date >= date_string_test) & (data.dataT !='test')]\n",
    "val_bigDf   = data.loc[ (data.dataT =='test')]\n",
    "\n",
    "train_bigDf['err_predTarget'] = np.abs(train_bigDf['predTarget'] - train_bigDf['target'])\n",
    "test_bigDf['err_predTarget'] = np.abs(test_bigDf['predTarget'] - test_bigDf['target'])\n",
    "\n",
    "\n",
    "\n",
    "train_bigDf['predLogSales'] = train_bigDf['predTarget']+train_bigDf['salesArima']\n",
    "test_bigDf['predLogSales'] = test_bigDf['predTarget']+test_bigDf['salesArima']\n",
    "train_bigDf['err_LogSales'] = (train_bigDf['predLogSales'] - train_bigDf['logSales'])**2\n",
    "test_bigDf['err_LogSales'] =(test_bigDf['predLogSales'] - test_bigDf['logSales'])**2\n",
    "train_bigDf['err_arimaLogSales'] = (train_bigDf['salesArima'] - train_bigDf['logSales'])**2\n",
    "test_bigDf['err_arimaLogSales'] = (test_bigDf['salesArima'] - test_bigDf['logSales'])**2\n",
    "\n",
    "# arimaLogError > errLogSales = positive (= reduction of errLogSales)\n",
    "train_bigDf['improvedScoreByLGBM'] = train_bigDf['err_arimaLogSales'] - train_bigDf['err_LogSales']\n",
    "test_bigDf['improvedScoreByLGBM'] = test_bigDf['err_arimaLogSales'] - test_bigDf['err_LogSales']\n",
    "\n",
    "train_bigDf['predSales'] = np.exp(train_bigDf.predLogSales) -1\n",
    "test_bigDf['predSales'] = np.exp(test_bigDf.predLogSales) -1#\n",
    "train_bigDf['err_sales'] = np.abs(train_bigDf['predSales'] - train_bigDf['sales'])\n",
    "test_bigDf['err_sales'] = np.abs(test_bigDf['predSales'] - test_bigDf['sales'])\n",
    "\n",
    "train_bigDf['predArimaSales'] = np.exp(train_bigDf.salesArima) -1\n",
    "test_bigDf['predArimaSales'] = np.exp(test_bigDf.salesArima) -1\n",
    "train_bigDf['err_salesArima'] = np.abs(train_bigDf['predArimaSales'] - train_bigDf['sales'])\n",
    "test_bigDf['err_salesArima'] = np.abs(test_bigDf['predArimaSales'] - test_bigDf['sales'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_bigDf['sales'] = np.exp(val_bigDf.predTarget + val_bigDf.salesArima)-1\n",
    "\n",
    "val_bigDf[['id','sales']].set_index('id').to_csv('arima_lgbm_perFam_crossVal_10trees.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "badPredictionsTest = test_bigDf.loc[test_bigDf.improvedScoreByLGBM>0]\n",
    "goodPredictionsTest = test_bigDf.loc[test_bigDf.improvedScoreByLGBM<0]\n",
    "badPredictionsTrain = train_bigDf.loc[train_bigDf.improvedScoreByLGBM>0]\n",
    "goodPredictionsTrain = train_bigDf.loc[train_bigDf.improvedScoreByLGBM<0]\n",
    "\n",
    "plotDistOfTestTrain('family',badPredictionsTest,goodPredictionsTest,'bad','good')\n",
    "plotDistOfTestTrain('store_nbr',badPredictionsTest,goodPredictionsTest,'bad','good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "badPredictionsTest = test_bigDf.loc[test_bigDf.err_LogSales>1]\n",
    "goodPredictionsTest = test_bigDf.loc[test_bigDf.err_LogSales<1]\n",
    "badPredictionsTrain = train_bigDf.loc[train_bigDf.err_LogSales>1]\n",
    "goodPredictionsTrain = train_bigDf.loc[train_bigDf.err_LogSales<1]\n",
    "\n",
    "badFreq, goodFreq = display2Ddifferences('store_nbr','family', badPredictionsTest, goodPredictionsTest)\n",
    "badFreq, goodFreq = display2Ddifferences('store_nbr','family', badPredictionsTrain, goodPredictionsTrain)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "storeSales",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
