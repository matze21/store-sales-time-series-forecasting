{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import plotly.offline as pyo\n",
    "from plotly import subplots\n",
    "import plotly.graph_objects as go\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.tsa.api as smt\n",
    "\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from tensorflow.keras.models import Sequential\n",
    "#from tensorflow.keras.layers import LSTM, Dense,Input,concatenate\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import lightgbm as lgb\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "from baseFunctions import *\n",
    "from data_helpers import processData6, featureEngineering, getSequencesFast, removeOutliers, create_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, propDicts, flippedPropDicts = processData6()\n",
    "data, timeFeatures = featureEngineering(data,splits=[2,2,2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainF = [\n",
    "       'store_nbr', 'family', \n",
    "       #'sales', \n",
    "       'onpromotion',# 'dataT',\n",
    "       'city', 'state', 'type', 'cluster', 'dcoilwtico', 'holidayType',\n",
    "       'description', 'transferred', \n",
    "       #'transactions', \n",
    "       'store_closed',\n",
    "       'linear_time', 'day_of_year', 'day_of_year_f12_0', 'day_of_year_f104_0',\n",
    "       'day_of_year_f24_0', 'day_of_year_f52_0', 'weekday', 'month'\n",
    "       ]\n",
    "\n",
    "trainF2 = trainF + ['sales']\n",
    "\n",
    "\n",
    "\n",
    "n_predictedValues = 16\n",
    "look_back = 50\n",
    "zScoreNorm = False\n",
    "logTransform = True\n",
    "\n",
    "# Date string\n",
    "date_string = \"2017-05-01\"\n",
    "date_object = datetime.strptime(date_string, '%Y-%m-%d')\n",
    "days_ago = date_object - timedelta(days=(look_back + n_predictedValues -1 +2))\n",
    "days_ago_string = days_ago.strftime('%Y-%m-%d')\n",
    "days_ago_string\n",
    "\n",
    "maskTrain = data.date < date_string\n",
    "maskTest = data.date  > days_ago_string #\"2017-03-03\" #42days + 15 day between (15 because we only want to iterate one value more from the test set)\n",
    "\n",
    "log = {}\n",
    "\n",
    "train = data.loc[data.dataT == 'train']\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "stdDict = {}\n",
    "meanDict= {}\n",
    "X_train = []\n",
    "X_test  = []\n",
    "y_train = []\n",
    "y_test  = []\n",
    "std_train, std_test = [], []\n",
    "mean_train, mean_test = [], []\n",
    "init= False\n",
    "for familyId in [0]: #data.family.unique():\n",
    "       # start with only some families\n",
    "       if familyId > 8:\n",
    "          continue\n",
    "\n",
    "       print(familyId)\n",
    "       familyDf = train.loc[train.family==familyId]  \n",
    "\n",
    "       for storeId in data.store_nbr.unique():\n",
    "              storeDf = familyDf.loc[(familyDf.store_nbr == storeId)] \n",
    "\n",
    "              # ln tranformation\n",
    "              if logTransform:\n",
    "                     storeDf.loc[:,'sales'] = np.log(storeDf.sales + 1)\n",
    "              if zScoreNorm:\n",
    "                     X_train0,y_train0,mean,std = getSequencesFast(storeDf.loc[maskTrain], trainF, look_back, n_predictedValues, zScoreNorm=zScoreNorm)\n",
    "                     X_test0, y_test0           = getSequencesFast(storeDf.loc[maskTest], trainF, look_back, n_predictedValues, zScoreNorm=False, applyZScoreNorm=True, meanZ=mean, stdZ=std)\n",
    "                     stdDict[storeId] = std\n",
    "                     meanDict[storeId] = mean\n",
    "              else:\n",
    "                     X_train0,y_train0 = getSequencesFast(storeDf.loc[maskTrain], trainF, look_back, n_predictedValues, zScoreNorm=False)\n",
    "                     X_test0, y_test0  = getSequencesFast(storeDf.loc[maskTest], trainF, look_back, n_predictedValues, zScoreNorm=False, applyZScoreNorm=False)  \n",
    "              \n",
    "              if init:\n",
    "                     X_train = np.concatenate((X_train, X_train0), axis=0)\n",
    "                     X_test  = np.concatenate((X_test,  X_test0), axis=0)\n",
    "                     y_train = np.concatenate((y_train, y_train0), axis=0)\n",
    "                     y_test  = np.concatenate((y_test,  y_test0), axis=0)\n",
    "                     if zScoreNorm:\n",
    "                            std_train = np.concatenate((std_train, np.ones(y_train0.shape)*std), axis=0)\n",
    "                            std_test  = np.concatenate((std_test,  np.ones(y_test0.shape)*std), axis=0)\n",
    "                            mean_train = np.concatenate((mean_train, np.ones(y_train0.shape)*mean), axis=0)\n",
    "                            mean_test = np.concatenate((mean_test, np.ones(y_test0.shape)*mean), axis=0)\n",
    "              else:\n",
    "                     X_train, X_test, y_train,y_test = X_train0, X_test0, y_train0, y_test0\n",
    "                     if zScoreNorm:\n",
    "                            std_train  =np.ones(y_train0.shape)*std\n",
    "                            std_test   =np.ones(y_test0.shape)*std\n",
    "                            mean_train =np.ones(y_train0.shape)*mean\n",
    "                            mean_test  =np.ones(y_test0.shape)*mean\n",
    "                     init=True \n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, y_test, X_test.shape, X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('train_test_Sequences_100_16_StdNormalized_fam0-7.npz', arr1=X_train, arr2=y_train, arr3=std_train, arr4=X_test, arr5=y_test, arr6=std_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LGBM\n",
    "\n",
    "using a ln transformation and rmse vs. rmsle: -> maybe a bit less overfitting (0.628/0.561 vs. 0.581/0.588)\n",
    "\n",
    "#familyId: 0 errors:   0.695 0.634\n",
    "\n",
    "\n",
    "- familyId: 0 errors:   0.502 0.513  num round = 100, logTransform, look_back = 200 \n",
    "\n",
    "- familyId: 0 errors:   0.692 0.632  num round = 10, logTransform, look_back = 200\n",
    " \n",
    "- familyId: 0 errors:   0.683 0.625  num round = 10, log Transform, look_back = 400\n",
    "\n",
    "- familyId: 0 errors:   0.496 0.512  num round = 100 log transform, look_back = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters for LGBM model\n",
    "params = {\n",
    "    'objective': 'regression',  # Assuming you're doing regression\n",
    "    'metric': 'rmsle',  # Mean squared error\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': -1\n",
    "}   \n",
    "\n",
    "# Train the model\n",
    "num_round = 10  \n",
    "gbms = [lgb.train(params, lgb.Dataset(X_train, label=y_train[:, i]),num_round, valid_sets=[lgb.Dataset(X_test, label=y_test[:,i])]) for i in range(y_train.shape[1])]   \n",
    "\n",
    "\n",
    "forecast = np.column_stack([gbm.predict(X_train, num_iteration=gbm.best_iteration) for gbm in gbms])\n",
    "if (forecast<0).any():\n",
    "    print('negative values!!!')\n",
    "    forecast = np.clip(forecast, 0, 1e29)\n",
    "if zScoreNorm:\n",
    "    forecast = forecast *std_train  + mean_train\n",
    "    y_train = y_train *std_train + mean_train\n",
    "rmsleTrain = np.sqrt(mean_squared_log_error(forecast, y_train))\n",
    "forecast = np.column_stack([gbm.predict(X_test, num_iteration=gbm.best_iteration) for gbm in gbms])\n",
    "if zScoreNorm:\n",
    "    forecast = forecast *std_test  + mean_test\n",
    "    y_test = y_test*std_test + mean_test\n",
    "rmsleTest = np.sqrt(mean_squared_log_error(forecast, y_test))\n",
    "print('familyId:', familyId, 'errors:  ', round(rmsleTrain,3), round(rmsleTest,3), y_train.shape[0], y_test.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOG version\n",
    "params = {\n",
    "    'objective': 'regression',  # Assuming you're doing regression\n",
    "    'metric': 'rmse',#'rmsle',  # Mean squared error\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': -1\n",
    "}   \n",
    "\n",
    "# Train the model\n",
    "num_round = 100 \n",
    "gbms = [lgb.train(params, lgb.Dataset(X_train, label=y_train[:, i]),num_round, valid_sets=[lgb.Dataset(X_test, label=y_test[:,i])]) for i in range(y_train.shape[1])]   \n",
    "\n",
    "\n",
    "forecast = np.column_stack([gbm.predict(X_train, num_iteration=gbm.best_iteration) for gbm in gbms])\n",
    "if logTransform:\n",
    "    forecast0 = np.exp(forecast) - 1\n",
    "    if (forecast0<0).any():\n",
    "        print('negative values!!!')\n",
    "        forecast0 = np.clip(forecast0, 0, 1e29)\n",
    "    y_train0  = np.exp(y_train) - 1\n",
    "    rmsleTrain = np.sqrt(mean_squared_log_error(forecast0, y_train0))\n",
    "forecast = np.column_stack([gbm.predict(X_test, num_iteration=gbm.best_iteration) for gbm in gbms])\n",
    "if logTransform:\n",
    "    forecast1 = np.exp(forecast) - 1\n",
    "    if (forecast1<0).any():\n",
    "        print('negative values!!!')\n",
    "        forecast1 = np.clip(forecast1, 0, 1e29)\n",
    "    y_test1   = np.exp(y_test) - 1\n",
    "    rmsleTest = np.sqrt(mean_squared_log_error(forecast1, y_test1))\n",
    "print('familyId:', familyId, 'errors:  ', round(rmsleTrain,3), round(rmsleTest,3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#familyId: 0 errors:   0.695 0.634\n",
    "\n",
    "# num round = 100, logTransform, look_back = 200\n",
    "#familyId: 0 errors:   0.502 0.513\n",
    "# num round = 10, logTransform, look_back = 200\n",
    "#familyId: 0 errors:   0.692 0.632\n",
    "# num round = 10, log Transform, look_back = 400\n",
    "#familyId: 0 errors:   0.683 0.625\n",
    "# num round = 100 log transform, look_back = 400\n",
    "#familyId: 0 errors:   0.496 0.512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# facebook prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fbprophet import Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is a pandas dataframe with 'ds' and 'y' columns\n",
    "m = Prophet()\n",
    "m.fit(df)\n",
    "\n",
    "# Make future dataframe for 16 periods\n",
    "future = m.make_future_dataframe(periods=16)\n",
    "forecast = m.predict(future)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "\n",
    "idea: we could use positional embeddings to define the relation between store and product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training data for transformer / nn architectures\n",
    "from data_helpers import create_sequences\n",
    "def getSequencesFastNOTFlattened(train, trainF, look_back, n_predictedValues, zScoreNorm = False, logTransf=False, applyZScoreNorm = False, meanZ = 0, stdZ = 0,):\n",
    "    # zscore over all values -> not ideal bc test data\n",
    "    if zScoreNorm:\n",
    "        #mean = train.sales.mean()\n",
    "        mean = 0 # modified zScore, not in mean = 0\n",
    "        std = max(train.sales.std(), 1)\n",
    "        train.loc[:,'sales'] = (train.sales - mean) / std\n",
    "    if applyZScoreNorm:\n",
    "        train.loc[:,'sales'] = (train.sales-meanZ)/stdZ\n",
    "    if logTransf:\n",
    "        train.loc[:,'sales'] = np.log(train.sales + 1)\n",
    "\n",
    "    trainF2 = trainF + ['sales']\n",
    "\n",
    "\n",
    "    pastS   = create_sequences(train[trainF2].to_numpy(), look_back)                                   #past sequence\n",
    "    futureS = create_sequences(train[trainF].iloc[look_back:-1].to_numpy(), n_predictedValues)        #future sequence\n",
    "    label   = create_sequences(train[['sales']].iloc[look_back:-1].to_numpy(), n_predictedValues) #label\n",
    "\n",
    "    maxLen = train.shape[0] - look_back-n_predictedValues\n",
    "    pastS = pastS[0:maxLen,:,:].transpose([0,2,1])\n",
    "    futureS = futureS[0:maxLen,:,:].transpose([0,2,1])\n",
    "    label = label[0:maxLen,:].transpose([0,2,1])\n",
    "\n",
    "    if zScoreNorm:\n",
    "        return pastS, futureS, label, mean, std\n",
    "    elif applyZScoreNorm:\n",
    "        return pastS, futureS, label\n",
    "    else:\n",
    "        return pastS, futureS, label\n",
    "\n",
    "trainF = [\n",
    "       'store_nbr', 'family', \n",
    "       #'sales', \n",
    "       'onpromotion',# 'dataT',\n",
    "       'city', 'state', 'type', 'cluster', 'dcoilwtico', 'holidayType',\n",
    "       'description', 'transferred', \n",
    "       #'transactions', \n",
    "       'store_closed',\n",
    "       'linear_time', 'day_of_year', 'day_of_year_f12_0', 'day_of_year_f104_0',\n",
    "       'day_of_year_f24_0', 'day_of_year_f52_0', 'weekday', 'month'\n",
    "       ]\n",
    "\n",
    "trainF2 = trainF + ['sales']\n",
    "\n",
    "\n",
    "\n",
    "n_predictedValues = 16\n",
    "look_back = 100\n",
    "zScoreNorm = False\n",
    "logTransform = True\n",
    "\n",
    "# Date string\n",
    "date_string = \"2017-05-01\"\n",
    "date_object = datetime.strptime(date_string, '%Y-%m-%d')\n",
    "days_ago = date_object - timedelta(days=(look_back + n_predictedValues -1 +2))\n",
    "days_ago_string = days_ago.strftime('%Y-%m-%d')\n",
    "days_ago_string\n",
    "\n",
    "maskTrain = data.date < date_string\n",
    "maskTest = data.date  > days_ago_string #\"2017-03-03\" #42days + 15 day between (15 because we only want to iterate one value more from the test set)\n",
    "\n",
    "log = {}\n",
    "\n",
    "train = data.loc[data.dataT == 'train']\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "stdDict = {}\n",
    "meanDict= {}\n",
    "X_trainS0 = []\n",
    "X_testS0  = []\n",
    "X_trainS1 = []\n",
    "X_testS1  = []\n",
    "y_train = []\n",
    "y_test  = []\n",
    "std_train, std_test = [], []\n",
    "mean_train, mean_test = [], []\n",
    "init= False\n",
    "for familyId in [0]: #data.family.unique():\n",
    "       print(familyId)\n",
    "       familyDf = train.loc[train.family==familyId]  \n",
    "\n",
    "       for storeId in data.store_nbr.unique():\n",
    "              if storeId >1:\n",
    "                continue\n",
    "              print(storeId)\n",
    "              storeDf = familyDf.loc[(familyDf.store_nbr == storeId)] \n",
    "              if zScoreNorm:\n",
    "                X_train0,X_train1,y_train0,mean,std = getSequencesFastNOTFlattened(storeDf.loc[maskTrain], trainF, look_back, n_predictedValues, zScoreNorm=zScoreNorm)\n",
    "                X_test0,X_test1, y_test0            = getSequencesFastNOTFlattened(storeDf.loc[maskTest], trainF, look_back, n_predictedValues, zScoreNorm=False, applyZScoreNorm=True, meanZ=mean, stdZ=std)       \n",
    "                stdDict[storeId] = std\n",
    "                meanDict[storeId] = mean\n",
    "              else:\n",
    "                X_train0,X_train1,y_train0 = getSequencesFastNOTFlattened(storeDf.loc[maskTrain], trainF, look_back, n_predictedValues, zScoreNorm=False, logTransf=logTransform)\n",
    "                X_test0,X_test1, y_test0   = getSequencesFastNOTFlattened(storeDf.loc[maskTest], trainF, look_back, n_predictedValues, zScoreNorm=False, logTransf=logTransform, applyZScoreNorm=False, meanZ=0, stdZ=0) \n",
    "              if init:\n",
    "                     X_trainS0 = np.concatenate((X_trainS0, X_train0), axis=0)\n",
    "                     X_testS0  = np.concatenate((X_testS0,  X_test0), axis=0)\n",
    "                     X_trainS1 = np.concatenate((X_trainS1, X_train1), axis=0)\n",
    "                     X_testS1  = np.concatenate((X_testS1,  X_test1), axis=0)\n",
    "                     y_train = np.concatenate((y_train, y_train0), axis=0)\n",
    "                     y_test  = np.concatenate((y_test,  y_test0), axis=0)\n",
    "                     if  zScoreNorm:\n",
    "                        std_train = np.concatenate((std_train, np.ones(y_train0.shape)*std), axis=0)\n",
    "                        std_test  = np.concatenate((std_test,  np.ones(y_test0.shape)*std), axis=0)\n",
    "                        mean_train = np.concatenate((mean_train, np.ones(y_train0.shape)*mean), axis=0)\n",
    "                        mean_test = np.concatenate((mean_test, np.ones(y_test0.shape)*mean), axis=0)\n",
    "              else:\n",
    "                     X_trainS0, X_trainS1 = X_train0, X_train1\n",
    "                     X_testS0, X_testS1   = X_test0, X_test1\n",
    "                     y_train,y_test = y_train0, y_test0\n",
    "                     if zScoreNorm:\n",
    "                        std_train  =np.ones(y_train0.shape)*std\n",
    "                        std_test   =np.ones(y_test0.shape)*std\n",
    "                        mean_train =np.ones(y_train0.shape)*mean\n",
    "                        mean_test  =np.ones(y_test0.shape)*mean\n",
    "                     init=True \n",
    "\n",
    "X_test = [X_testS0, X_testS1]\n",
    "X_train = [X_trainS0, X_trainS1]\n",
    "X_train[0].shape\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainS0.shape, X_testS0.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transformer from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "print(module_path)\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Transformer import encoder, decoder, embedding, attention, feed_forward\n",
    "from Transformer import transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sanity checks\n",
    "embed_pt = PositionalEmbedding(sequence_length=100, d_model=21)\n",
    "\n",
    "s0 = embed_pt(X_trainS0[0:10,:,:])\n",
    "\n",
    "embed_pt = PositionalEmbedding(sequence_length=16, d_model=20)\n",
    "\n",
    "s1 = embed_pt(X_trainS1[0:10,:,:])\n",
    "print('embedding',s1.shape)\n",
    "\n",
    "sample_ca = CrossAttention(num_heads=2, key_dim=512)\n",
    "\n",
    "print('crossAttnetion',sample_ca(x=s1,context=s0).shape)\n",
    "\n",
    "# Instantiate the encoder.\n",
    "sample_encoder = Encoder(num_layers=4,\n",
    "                         d_model=21,\n",
    "                         num_heads=8,\n",
    "                         dff=64,\n",
    "                         sequence_length=100)\n",
    "\n",
    "sample_encoder_output = sample_encoder(X_trainS0[0:10,:,:], training=False)\n",
    "\n",
    "print('encoder',sample_encoder_output.shape)  # Shape `(batch_size, input_seq_len, d_model)`.\n",
    "\n",
    "# Instantiate the decoder.\n",
    "sample_decoder = Decoder(num_layers=4,\n",
    "                         d_model=20,\n",
    "                         num_heads=8,\n",
    "                         dff=64,\n",
    "                         sequence_length=16)\n",
    "\n",
    "output = sample_decoder(\n",
    "    x=s1,\n",
    "    context=s0)\n",
    "\n",
    "# Print the shapes.\n",
    "print('decoder',output.shape)\n",
    "\n",
    "transformer = Transformer(\n",
    "    enc_dec_num_layers=1,\n",
    "    input_seq_dim_enc = 21,#X_trainS0.shape[2],\n",
    "    input_seq_dim_dec = 20,#X_trainS1.shape[2],\n",
    "    num_heads=8,\n",
    "    fully_connected_size=64,\n",
    "    input_sequence_len=look_back,\n",
    "    output_sequence_len=1, #n_predictedValues,\n",
    "    dropout_rate=0)\n",
    "    \n",
    "output = transformer(([X_trainS0[0:10,:,:],X_trainS1[0:10,:,:]]))\n",
    "\n",
    "print('transformer',output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.set_random_seed(42)\n",
    "\n",
    "\n",
    "\n",
    "transformer = Transformer(\n",
    "    enc_dec_num_layers=1,\n",
    "    input_seq_dim_enc = X_trainS0.shape[2],\n",
    "    input_seq_dim_dec = X_trainS1.shape[2],\n",
    "    num_heads=8,\n",
    "    fully_connected_size=64,\n",
    "    input_sequence_len=look_back,\n",
    "    output_sequence_len=1, #n_predictedValues,\n",
    "    dropout_rate=0)\n",
    "\n",
    "transformer.compile(\n",
    "    loss=root_mean_squared_error,  #if I log transform my data, the loss is just rmse\n",
    "    optimizer='adam',\n",
    "    metrics=['mae'])\n",
    "\n",
    "transformer.fit(X_train, y_train,\n",
    "    epochs=10,\n",
    "    validation_data=(X_test, y_test), batch_size=64)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast = transformer.predict(X_train)\n",
    "if (forecast<0).any():\n",
    "    print('negative values!!!')\n",
    "    forecast = np.clip(forecast, 0, 1e29)\n",
    "if zScoreNorm:\n",
    "    forecast = forecast *std_train  + mean_train\n",
    "    y_train = y_train *std_train + mean_train\n",
    "rmsleTrain = np.sqrt(mean_squared_log_error(np.reshape(forecast,(-1,16)), np.reshape(y_train, (-1,16))))\n",
    "forecast = transformer.predict(X_test)\n",
    "if zScoreNorm:\n",
    "    forecast = forecast *std_test  + mean_test\n",
    "    y_test = y_test*std_test + mean_test\n",
    "rmsleTest = np.sqrt(mean_squared_log_error(np.reshape(forecast,(-1,16)), np.reshape(y_test, (-1,16))))\n",
    "print('errors:  ', round(rmsleTrain,3), round(rmsleTest,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU - sequence input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.set_random_seed(42)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(GRU(32, return_sequences=True, input_shape=(100, 20)))\n",
    "model.add(GRU(32))\n",
    "model.add(Dense(16))\n",
    "\n",
    "model.compile(loss='rmse', optimizer='adam')\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32,validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.set_random_seed(42)\n",
    "\n",
    "# Define an input sequence.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "\n",
    "# Create a GRU layer\n",
    "encoder = GRU(latent_dim, return_state=True)\n",
    "\n",
    "# Get the output and state\n",
    "encoder_outputs, state_h = encoder(encoder_inputs)\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "\n",
    "# Create another GRU layer\n",
    "decoder_gru = GRU(latent_dim, return_sequences=True, return_state=True)\n",
    "\n",
    "# Get the output and state\n",
    "decoder_outputs, _ = decoder_gru(decoder_inputs, initial_state=state_h)\n",
    "\n",
    "# Create a dense layer\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "\n",
    "# Get the final output\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lagged features instead of input sequence\n",
    "\n",
    "- LGBM easily: 0.3355479688664862 0.4432190296261781\n",
    "- simple NN:   0.4243960256089085 0.43396152825264606\n",
    "\n",
    "predicting differences log(y+1) - log(y_t-21 +1)\n",
    "- LGBM errors: 0.35169724505847927 0.4570765686387128\n",
    "- simple NN:   0.48616113434909736 0.5235771146613244\n",
    "\n",
    "predicting difference & sin & cos fourier features \n",
    "- LGBM errors:  0.34940512929957196 0.4623122103961216\n",
    "\n",
    "\n",
    "- LGBM errors:  0.3633924223339021  0.45014057094761073 predicting difference + base features + only sales lagged\n",
    "- LGBM errors:  0.34825128156856566 0.4453217678640429  predicting direct val + base features + only sales lagged\n",
    "- LGBM errors:  0.3353492659854237  0.44503837906251603 predicting direct val + base features + only sales lagged + LGBM per timestamp    -> one LGBM per timestamp no benefit\n",
    "- LGBM errors:  0.35188109186124034 0.4416272716760396  predicting difference + base features + only sales lagged + LGBM per timestamp    -> difference or direct, hardly impact\n",
    "- LGBM errors:  0.345641142461171   0.44866497500612895 predicting difference + base features + all features lagged + LGBM per timestamp  -> using all lagged features no benefit\n",
    "- LGBM errors:  0.35220141034766933 0.44360505503406444 predicting difference + base features + all features lagged + LGBM per timestamp + remove duplicates\n",
    "\n",
    "- LGBM errors:  0.33374897953217064 0.45021149896074164 difference + base features + all lagged + add transaction feature\n",
    "\n",
    "\n",
    "Experimenting with fitting next day:\n",
    "- errors:  0.29418915523321926 0.46566462585278806   diff t-1  + logTransform\n",
    "- errors:  0.33719271375013815 0.4479348120202338    log Transform \n",
    "- errors:  0.3584148918907327  0.540613165753143     diff t-28 + logTransform\n",
    "- errors:  0.07694627970367869 0.45788518867147826   diff t-7 + logTransform - fit train data well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "create training data based on lagged features not 2 sequences\n",
    "\"\"\"\n",
    "trainF = [\n",
    "       #'store_nbr', 'family', \n",
    "       #'sales', \n",
    "       'onpromotion',# 'dataT',\n",
    "       #'city', 'state', 'type', 'cluster', \n",
    "       'dcoilwtico', \n",
    "       'holidayType',\n",
    "       'description', \n",
    "       'transferred', \n",
    "       #'transactions', \n",
    "       'store_closed']\n",
    "timeF = [\n",
    "       'linear_time', 'day_of_year', 'day_of_year_f12_0', 'day_of_year_f104_0','day_of_year_f24_0',  'day_of_year_f52_0',\n",
    "       'day_of_year_f12_180', 'day_of_year_f104_180','day_of_year_f24_180','day_of_year_f52_180', \n",
    "       'weekday', 'month'\n",
    "       ]\n",
    "\n",
    "\n",
    "\n",
    "n_predictedValues = 16\n",
    "refLag = 28\n",
    "zScoreNorm = False\n",
    "logTransform = True\n",
    "predictDiff = True\n",
    "lags = 56\n",
    "rolling = [7,14,21]\n",
    "\n",
    "# Date string\n",
    "date_string = \"2016-07-01\"#\"2017-05-01\"\n",
    "\n",
    "log = {}\n",
    "\n",
    "train = data.loc[(data.dataT == 'train')]# & (data.date > \"2015-07-01\")]\n",
    "\n",
    "    \n",
    "stdDict = {}\n",
    "meanDict= {}\n",
    "X_train = []\n",
    "X_test  = []\n",
    "y_train = []\n",
    "y_test  = []\n",
    "y_trainBase, y_trainBase = [], []\n",
    "std_train, std_test = [], []\n",
    "mean_train, mean_test = [], []\n",
    "init= False\n",
    "for familyId in [0]: #data.family.unique():\n",
    "       # start with only some families\n",
    "       if familyId > 8:\n",
    "          continue\n",
    "\n",
    "       print(familyId)\n",
    "       familyDf = train.loc[train.family==familyId]  \n",
    "\n",
    "       for storeId in [1]: #data.store_nbr.unique():\n",
    "              print('store',storeId)\n",
    "              storeDf = familyDf.loc[(familyDf.store_nbr == storeId)] \n",
    "\n",
    "              # ln tranformation\n",
    "              if logTransform:\n",
    "                     storeDf.loc[:,'sales'] = np.log(storeDf.sales + 1)\n",
    "              if zScoreNorm: # just use entire training data for now\n",
    "                     std = storeDf['sales'].std()\n",
    "                     mean = storeDf['sales'].mean()\n",
    "                     storeDf.loc[:,'sales'] = (storeDf.sales - mean)/std\n",
    "\n",
    "              if predictDiff:\n",
    "                     storeDf.loc[:,['ref']] = storeDf['sales'].shift(refLag)  #28\n",
    "                     storeDf.loc[:,['target']] = storeDf['sales'] - storeDf['ref']\n",
    "              else:\n",
    "                     storeDf.loc[:,['target']] = storeDf['sales']\n",
    "\n",
    "              # lag features / how many past datapoints are we tain\n",
    "              featuresForLag = ['target']\n",
    "              lagF = []#trainF\n",
    "              for i in range(lags):\n",
    "                     lag = i+1#+n_predictedValues\n",
    "                     newF = [featuresForLag[j] + '_lag' + str(lag) for j in range(len(featuresForLag))]\n",
    "                     lagF = lagF + newF\n",
    "                     storeDf.loc[:,newF] = storeDf[featuresForLag].shift(lag).to_numpy()\n",
    "\n",
    "              # rolling features\n",
    "              rollingF = []\n",
    "              for rol in rolling:\n",
    "                     for i in range(len(lagF)):\n",
    "                            #if 'sales_t-16'  in lagF[i]:\n",
    "                            if 'target'  in lagF[i]:\n",
    "                                   fm = lagF[i]+'_rollingM' + str(rol)\n",
    "                                   fs = lagF[i]+'_rollingS' + str(rol)\n",
    "                                   rollingF.append(fm)\n",
    "                                   rollingF.append(fs)\n",
    "                                   storeDf.loc[:,[fm]] = storeDf[lagF[i]].rolling(rol).mean()#.copy()\n",
    "                                   storeDf.loc[:,[fs]] = storeDf[lagF[i]].rolling(rol).std()#.copy()\n",
    "\n",
    "\n",
    "              allF = lagF + rollingF + timeF + trainF\n",
    "\n",
    "              # we get a matrix that predicts only 1 timestamp -> stride it\n",
    "              storeDf = storeDf.iloc[lags+n_predictedValues+max(rolling)+1:-1]\n",
    "\n",
    "              train_subDf = storeDf.loc[storeDf.date < date_string]\n",
    "              test_subDf  = storeDf.loc[storeDf.date >= date_string]\n",
    "\n",
    "              X_train0 = np.lib.stride_tricks.sliding_window_view(train_subDf[allF].to_numpy(), (n_predictedValues, len(allF)))[:,0,:,:]\n",
    "              X_test0 = np.lib.stride_tricks.sliding_window_view(test_subDf[allF].to_numpy(), (n_predictedValues, len(allF)))[:,0,:,:]\n",
    "\n",
    "              if predictDiff:\n",
    "                     y_train0 = np.lib.stride_tricks.sliding_window_view(train_subDf[['target']].to_numpy(), (n_predictedValues, 1))[:,0,:,:]\n",
    "                     y_trainBase0 = np.lib.stride_tricks.sliding_window_view(train_subDf[['ref']].to_numpy(), (n_predictedValues, 1))[:,0,:,:]\n",
    "                     y_test0 = np.lib.stride_tricks.sliding_window_view(test_subDf[['target']].to_numpy(), (n_predictedValues, 1))[:,0,:,:]\n",
    "                     y_testBase0 = np.lib.stride_tricks.sliding_window_view(test_subDf[['ref']].to_numpy(), (n_predictedValues, 1))[:,0,:,:]\n",
    "              else:\n",
    "                     y_train0 = np.lib.stride_tricks.sliding_window_view(train_subDf[['sales']].to_numpy(), (n_predictedValues, 1))[:,0,:,:]\n",
    "                     y_test0 = np.lib.stride_tricks.sliding_window_view(test_subDf[['sales']].to_numpy(), (n_predictedValues, 1))[:,0,:,:]\n",
    "              \n",
    "              if init:\n",
    "                     X_train = np.concatenate((X_train, X_train0), axis=0)\n",
    "                     X_test  = np.concatenate((X_test,  X_test0), axis=0)\n",
    "                     y_train = np.concatenate((y_train, y_train0), axis=0)\n",
    "                     y_test  = np.concatenate((y_test,  y_test0), axis=0)\n",
    "                     if zScoreNorm:\n",
    "                            std_train = np.concatenate((std_train, np.ones(y_train0.shape)*std), axis=0)\n",
    "                            std_test  = np.concatenate((std_test,  np.ones(y_test0.shape)*std), axis=0)\n",
    "                            mean_train = np.concatenate((mean_train, np.ones(y_train0.shape)*mean), axis=0)\n",
    "                            mean_test = np.concatenate((mean_test, np.ones(y_test0.shape)*mean), axis=0)\n",
    "                     if predictDiff:\n",
    "                            y_trainBase = np.concatenate((y_trainBase, y_trainBase0), axis=0)\n",
    "                            y_testBase  = np.concatenate((y_testBase,  y_testBase0), axis=0)\n",
    "              else:\n",
    "                     X_train, X_test, y_train,y_test = X_train0, X_test0, y_train0, y_test0\n",
    "                     if zScoreNorm:\n",
    "                            std_train  =np.ones(y_train0.shape)*std\n",
    "                            std_test   =np.ones(y_test0.shape)*std\n",
    "                            mean_train =np.ones(y_train0.shape)*mean\n",
    "                            mean_test  =np.ones(y_test0.shape)*mean\n",
    "                     if predictDiff:\n",
    "                            y_trainBase, y_testBase = y_trainBase0, y_testBase0\n",
    "                     init=True \n",
    "\n",
    "if predictDiff:\n",
    "       print(y_trainBase.shape, y_testBase.shape)\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check duplicated columns in flattened df\n",
    "X_train0 = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "X_test0 = np.reshape(X_test, (X_test.shape[0], -1))   \n",
    "\n",
    "f = []\n",
    "for i in range(15):\n",
    "    if i == 0:\n",
    "        f = train_subDf[lagF].columns.to_list()\n",
    "    f = f + train_subDf[lagF].columns.to_list()\n",
    "f = np.array(f)   # get all columns in the falttened shape\n",
    "\n",
    "\n",
    "_,i = np.unique(X_train0, axis = 1, return_index=True)\n",
    "_,j = np.unique(X_test0, axis = 1, return_index=True)\n",
    "\n",
    "missingIdx = []\n",
    "for idx in i:\n",
    "    if not idx in j:\n",
    "        missingIdx.append(idx)\n",
    "\n",
    "f[missingIdx]\n",
    "X_test0[:,i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare against 16 lgbms one for each t+i (e.g. one for t+1, t+2, t+3,..)\n",
    "\n",
    "# Set parameters for LGBM model\n",
    "params = {\n",
    "    'objective': 'regression',  # Assuming you're doing regression\n",
    "    'metric': 'rmse',  # Mean squared error\n",
    "    'num_leaves': 15,\n",
    "    'learning_rate': 0.04,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': -1\n",
    "}   \n",
    "\n",
    "\n",
    "X_train0 = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "X_test0 = np.reshape(X_test, (X_test.shape[0], -1)) \n",
    "\n",
    "X_train0,i = np.unique(X_train0, axis = 1, return_index=True)\n",
    "X_test0 = X_test0[:,i]\n",
    "\n",
    "# Train the model\n",
    "num_round = 100  \n",
    "gbms = [lgb.train(params, lgb.Dataset(X_train0, label=y_train[:,i, :]),num_round, valid_sets=[lgb.Dataset(X_test0, label=y_test[:,i, :])]) for i in range(16)]\n",
    "\n",
    "\n",
    "predtrainLGBM = [gbm.predict(X_train0, num_iteration=gbm.best_iteration) for i,gbm in enumerate(gbms)]\n",
    "predtestLGBM = [gbm.predict(X_test0, num_iteration=gbm.best_iteration) for i,gbm in enumerate(gbms)]\n",
    "print('transform log', logTransform, 'predict diff', predictDiff)\n",
    "for i in range(16):\n",
    "    print('timestamp:',i,'errors: ', calcLossLGBM(predtrainLGBM[i], y_train[:,i, :], logTransform, predictDiff, y_trainBase[:,i, :]), calcLossLGBM(predtestLGBM[i], y_test[:,i, :], logTransform, predictDiff, y_testBase[:,i, :]))\n",
    "\n",
    "print('overall:','errors: ',calcLoss(np.reshape(np.column_stack(predtrainLGBM), y_trainBase.shape), y_train, logTransform, predictDiff, y_trainBase),calcLoss(np.reshape(np.column_stack(predtestLGBM),y_testBase.shape), y_test, logTransform, predictDiff, y_testBase))\n",
    "\n",
    "for i in range(5):\n",
    "    j = i*16\n",
    "    plot(j, logTransform, np.reshape(np.column_stack(predtestLGBM), y_testBase.shape), y_test, predictDiff, y_testBase)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare against base lgbm that just predicts always t+16\n",
    "\n",
    "def calcLossLGBM(pred, y, logTransform, predictDiff, base):\n",
    "    if predictDiff:\n",
    "        pred = np.reshape(pred, (pred.shape[0],1)) + base\n",
    "        y = y + base\n",
    "\n",
    "    if logTransform:\n",
    "        a = np.exp(pred)\n",
    "        y = np.exp(y)\n",
    "    else:\n",
    "        a = (pred)\n",
    "        y = (y)\n",
    "\n",
    "    if (a < 0).any():\n",
    "        a = np.clip(a, 0, 1e20)\n",
    "    rmsleTrain = np.sqrt(mean_squared_log_error(a,y))\n",
    "    return rmsleTrain\n",
    "def plotLGBM(i, logTransform, pred, y, predictDiff, base):\n",
    "    if predictDiff:\n",
    "        pred = np.reshape(pred, (pred.shape[0],1)) + base\n",
    "        y = y + base\n",
    "    if logTransform:\n",
    "        a = np.exp(pred[i:i+16])\n",
    "        y = np.exp(y[i:i+16])\n",
    "    else:\n",
    "        a = (pred[i:i+16])\n",
    "        y = (y[i:i+16])\n",
    "    x = range(len(a))   \n",
    "\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=1, figsize=(8, 6))\n",
    "    axs.plot(x, y, color='blue',label='Original')\n",
    "    axs.plot(x, a, color='red',label='pred')\n",
    "    axs.set_title('index: '+str(i))\n",
    "    fig.subplots_adjust(hspace=0.5)\n",
    "\n",
    "# Set parameters for LGBM model\n",
    "params = {\n",
    "    'boosting':'gbdt',#'gbdt', #'rf' #'dart'\n",
    "    'objective': 'regression',  # Assuming you're doing regression\n",
    "    'metric': 'mse',  # Mean squared error\n",
    "    'num_leaves': 15,\n",
    "    #'lambda_l1': 0.1,\n",
    "    #'lambda_l2': 0.2,\n",
    "    #'max_depth':10,\n",
    "    #'learning_rate': 0.1,\n",
    "    #'feature_fraction': 0.9,\n",
    "    #'bagging_fraction': 0.8,\n",
    "    #'bagging_freq': 5,\n",
    "    'verbose': 1,\n",
    "    'force_col_wise':True,\n",
    "    'num_iterations':200\n",
    "}   \n",
    "\n",
    "targetF = 'sales'\n",
    "baseTrain, baseTest = [],[]\n",
    "if predictDiff:\n",
    "    targetF = 'target'\n",
    "    baseTrain = train_subDf[['ref']].to_numpy()\n",
    "    baseTest  = test_subDf[['ref']].to_numpy()\n",
    "    \n",
    "\n",
    "X_train0 = train_subDf[allF].to_numpy()\n",
    "y_train0 = train_subDf[[targetF]].to_numpy()\n",
    "X_test0 =  test_subDf[allF].to_numpy()\n",
    "y_test0 =   test_subDf[[targetF]].to_numpy()           \n",
    "\n",
    "# Train the model\n",
    "gbm = lgb.train(params, lgb.Dataset(X_train0, label=y_train0), valid_sets=[lgb.Dataset(X_test0, label=y_test0)]#,num_boost_round=100\n",
    "#,callbacks=[lgb.early_stopping(stopping_rounds=30)]\n",
    ")  \n",
    "predtrainLGBM = gbm.predict(X_train0)#, num_iteration=gbm.best_iteration)\n",
    "predtestLGBM = gbm.predict(X_test0)#, num_iteration=gbm.best_iteration)\n",
    "print('transform log', logTransform, 'predict diff', predictDiff)\n",
    "print('errors: ', calcLossLGBM(predtrainLGBM, y_train0, logTransform, predictDiff, baseTrain), calcLossLGBM(predtestLGBM, y_test0, logTransform, predictDiff, baseTest))\n",
    "\n",
    "for i in range(1):\n",
    "    plotLGBM(i*16, logTransform, predtestLGBM, y_test0, predictDiff, baseTest)\n",
    "    plotLGBM(i*16, logTransform, predtrainLGBM, y_train0, predictDiff, baseTrain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature importances\n",
    "importances = gbm.feature_importance()\n",
    "for name, importance in zip(allF, importances):\n",
    "    print(f'{name}: {importance}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_mod = lgb.cv(params, \n",
    "                lgb.Dataset(X_train0, label=y_train0), \n",
    "                50, \n",
    "                nfold = 10, \n",
    "                stratified = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### analyze residuals\n",
    "\n",
    "ATTENTION:\n",
    "if I try to fit just for the next day I already get big errors!\n",
    "\n",
    "- holiday info is needed before & after holiday \n",
    "- check for transferred info, would be better if transferred signal is also indicated at day of peak\n",
    "\n",
    "\n",
    "transaction might be helpful -> predict transaction first & then with transaction the sales\n",
    "- doesn't seem like it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addStuffToDf(test_subDf, predtestLGBM, y_test0, baseTest, logTransform, predictDiff):\n",
    "    test_subDf.loc[:,['day_name']] = test_subDf.date.dt.day_name()\n",
    "    pred = np.reshape(predtestLGBM, (predtestLGBM.shape[0],1))\n",
    "    y = y_test0\n",
    "    test_subDf.loc[:,['pred0']] = pred\n",
    "    test_subDf.loc[:,['y0']] = y\n",
    "\n",
    "    if predictDiff:\n",
    "        y = y + baseTest\n",
    "        pred = pred + baseTest\n",
    "        test_subDf.loc[:,['base']] = baseTest\n",
    "    test_subDf.loc[:,['pred1']] = pred\n",
    "    test_subDf.loc[:,['y1']] = y\n",
    "    if logTransform:\n",
    "        test_subDf.loc[:,['orig_sales']] = np.exp(y) -1\n",
    "        test_subDf.loc[:,['pred']] = np.exp(pred) -1\n",
    "    else:\n",
    "        test_subDf.loc[:,['orig_sales']] = y\n",
    "        test_subDf.loc[:,['pred']] =  pred\n",
    "    return test_subDf\n",
    "\n",
    "def plotDf(test_subDf,i):\n",
    "    f = [#'sales',\n",
    "            'orig_sales', 'onpromotion',\n",
    "            #'dcoilwtico', \n",
    "            'holidayType',\n",
    "           #'description', \n",
    "           'transferred', 'store_closed',\n",
    "           #'linear_time', \n",
    "           #'day_of_year', \n",
    "           #'day_of_year_f12_0',\n",
    "           #'day_of_year_f12_180', 'day_of_year_f104_0', 'day_of_year_f104_180',\n",
    "           #'day_of_year_f24_0', 'day_of_year_f24_180', 'day_of_year_f52_0',\n",
    "           #'day_of_year_f52_180', \n",
    "           'weekday', \n",
    "           'day_name','pred', #'pred0','y0'\n",
    "           #'month'\n",
    "           ]\n",
    "    f2 =['transactions']\n",
    "    f3 = ['pred0','y0', 'base'] if predictDiff else ['pred0','y0']\n",
    "    #f2 = ['pred1','y1']\n",
    "\n",
    "    test_subDf[f].iloc[i:i+50].plot()\n",
    "    #test_subDf[f3].iloc[i:i+50].plot()\n",
    "    #test_subDf[f2].iloc[i:i+50].plot()\n",
    "    #test_subDf[['dcoilwtico']].iloc[i:i+50].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_subDf = addStuffToDf(test_subDf, predtestLGBM, y_test0, baseTest, logTransform, predictDiff)\n",
    "train_subDf = addStuffToDf(train_subDf, predtrainLGBM, y_train0, baseTrain, logTransform, predictDiff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in train_subDf.columns:\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=1, figsize=(8, 6))\n",
    "    axs.scatter(train_subDf['sales'], train_subDf[f], color='blue',label='Original')\n",
    "    axs.set_title(f)\n",
    "    fig.subplots_adjust(hspace=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotDf(test_subDf, 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(int(train_subDf.shape[0] / 50)):  \n",
    "    plotDf(train_subDf, i*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f0 = ['sales','orig_sales', 'onpromotion',\n",
    "        #'dcoilwtico', \n",
    "        'holidayType',\n",
    "       'description', \n",
    "       'transferred', 'store_closed',\n",
    "       'weekday', \n",
    "       'day_name','pred', 'pred0','y0'\n",
    "       #'month'\n",
    "       ]\n",
    "\n",
    "test_subDf[f0].head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" use a fully connected NN \"\"\"\n",
    "from keras import backend as K\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1))\n",
    "\n",
    "tf.keras.utils.set_random_seed(42)\n",
    "\n",
    "n_features = X_train.shape[2]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64,input_shape=(n_predictedValues, n_features)))\n",
    "model.add(Dense(64))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "model.fit(X_train, y_train, epochs=400, batch_size=32,validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1))\n",
    "\n",
    "tf.keras.utils.set_random_seed(42)\n",
    "\n",
    "n_features = X_train.shape[2]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(GRU(64, return_sequences=True, input_shape=(n_predictedValues, n_features)))\n",
    "model.add(GRU(64,return_sequences=False))\n",
    "#model.add(Dense(64))\n",
    "model.add(Dense(n_predictedValues))\n",
    "#model.add(TimeDistributed(Dense(1)))\n",
    "\n",
    "model.compile(loss=root_mean_squared_error, optimizer='adam')\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32,validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcLoss(pred, y, logTransform, predictDiff, base):\n",
    "    if predictDiff:\n",
    "        pred = pred + base\n",
    "        y = y + base\n",
    "    if logTransform:\n",
    "        a = np.exp(pred)\n",
    "        y = np.exp(y)\n",
    "    else:\n",
    "        a = (pred)\n",
    "        y = (y)\n",
    "\n",
    "    if (a < 0).any():\n",
    "        a = np.clip(a, 0, 1e20)\n",
    "    rmsleTrain = np.sqrt(mean_squared_log_error(np.reshape(a,(-1,16)), np.reshape(y, (-1,16))))\n",
    "    return rmsleTrain\n",
    "    \n",
    "predtrain = model.predict(X_train, verbose=False)\n",
    "predtest  = model.predict(X_test, verbose=False)\n",
    "\n",
    "print('errors: ', calcLoss(predtrain, y_train, logTransform, predictDiff, y_trainBase), calcLoss(predtest, y_test, logTransform, predictDiff, y_testBase))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot(i, logTransform, pred, y, predictDiff, base):\n",
    "    if predictDiff:\n",
    "        pred = pred+ base\n",
    "        y = y + base\n",
    "    if logTransform:\n",
    "        a = np.exp(pred[i,:])\n",
    "        y = np.exp(y[i,:])\n",
    "    else:\n",
    "        a = (pred[i,:])\n",
    "        y = (y[i,:])\n",
    "    x = range(len(a))   \n",
    "\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=1, figsize=(8, 6))\n",
    "    axs.plot(x, y, color='blue',label='Original')\n",
    "    axs.plot(x, a, color='red',label='pred')\n",
    "    axs.set_title('index: '+str(i))\n",
    "    fig.subplots_adjust(hspace=0.5)\n",
    "\n",
    "for i in range(5):\n",
    "    plot(i*10, logTransform, predtest, y_test, predictDiff, y_testBase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.set_random_seed(42)\n",
    "\n",
    "# Define an input sequence.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "\n",
    "# Create a LSTM layer\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "\n",
    "# Get the output and states\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "\n",
    "# Create another LSTM layer\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "\n",
    "# Get the output and states\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "\n",
    "# Create a dense layer\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "\n",
    "# Get the final output\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "17f27e0cbed1f29822e509a94c958ed0e80b7d9abe162097755686a614de4732"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
