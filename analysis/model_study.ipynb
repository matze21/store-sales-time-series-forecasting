{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import plotly.offline as pyo\n",
    "from plotly import subplots\n",
    "import plotly.graph_objects as go\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.tsa.api as smt\n",
    "\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from tensorflow.keras.models import Sequential\n",
    "#from tensorflow.keras.layers import LSTM, Dense,Input,concatenate\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import lightgbm as lgb\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "from baseFunctions import *\n",
    "from data_helpers import processData6, featureEngineering, getSequencesFast, removeOutliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3029400, 7) (3029400, 11)\n",
      "dcoilwtico    529\n",
      "dtype: int64\n",
      "date          0\n",
      "dcoilwtico    0\n",
      "dtype: int64\n",
      "(3029400, 12) (3029400, 11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matthiaskargl/Codes/kaggle/store-sales-time-series-forecasting/analysis/data_helpers.py:38: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/Users/matthiaskargl/Codes/kaggle/store-sales-time-series-forecasting/analysis/data_helpers.py:39: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/Users/matthiaskargl/Codes/kaggle/store-sales-time-series-forecasting/analysis/data_helpers.py:40: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/Users/matthiaskargl/Codes/kaggle/store-sales-time-series-forecasting/analysis/data_helpers.py:41: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3029400, 12) (3029400, 15) data2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matthiaskargl/Codes/kaggle/store-sales-time-series-forecasting/analysis/data_helpers.py:47: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/Users/matthiaskargl/Codes/kaggle/store-sales-time-series-forecasting/analysis/data_helpers.py:48: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/Users/matthiaskargl/Codes/kaggle/store-sales-time-series-forecasting/analysis/data_helpers.py:49: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3029400, 18) (3029400, 15) data3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matthiaskargl/Codes/kaggle/store-sales-time-series-forecasting/analysis/data_helpers.py:55: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3029400, 18) (3029400, 21)\n",
      "finished data4\n",
      "(3029400, 21) (3029400, 15)\n",
      "finished data5\n"
     ]
    }
   ],
   "source": [
    "data, propDicts, flippedPropDicts = processData6()\n",
    "data, timeFeatures = featureEngineering(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'AUTOMOTIVE',\n",
       " 1: 'BABY CARE',\n",
       " 2: 'BEAUTY',\n",
       " 3: 'BEVERAGES',\n",
       " 4: 'BOOKS',\n",
       " 5: 'BREAD/BAKERY',\n",
       " 6: 'CELEBRATION',\n",
       " 7: 'CLEANING',\n",
       " 8: 'DAIRY',\n",
       " 9: 'DELI',\n",
       " 10: 'EGGS',\n",
       " 11: 'FROZEN FOODS',\n",
       " 12: 'GROCERY I',\n",
       " 13: 'GROCERY II',\n",
       " 14: 'HARDWARE',\n",
       " 15: 'HOME AND KITCHEN I',\n",
       " 16: 'HOME AND KITCHEN II',\n",
       " 17: 'HOME APPLIANCES',\n",
       " 18: 'HOME CARE',\n",
       " 19: 'LADIESWEAR',\n",
       " 20: 'LAWN AND GARDEN',\n",
       " 21: 'LINGERIE',\n",
       " 22: 'LIQUOR,WINE,BEER',\n",
       " 23: 'MAGAZINES',\n",
       " 24: 'MEATS',\n",
       " 25: 'PERSONAL CARE',\n",
       " 26: 'PET SUPPLIES',\n",
       " 27: 'PLAYERS AND ELECTRONICS',\n",
       " 28: 'POULTRY',\n",
       " 29: 'PREPARED FOODS',\n",
       " 30: 'PRODUCE',\n",
       " 31: 'SCHOOL AND OFFICE SUPPLIES',\n",
       " 32: 'SEAFOOD'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flippedPropDicts['family']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "trainF = [\n",
    "       'store_nbr', 'family', \n",
    "       #'sales', \n",
    "       'onpromotion',# 'dataT',\n",
    "       'city', 'state', 'type', 'cluster', 'dcoilwtico', 'holidayType',\n",
    "       'description', 'transferred', \n",
    "       #'transactions', \n",
    "       'store_closed',\n",
    "       'linear_time', 'day_of_year', 'day_of_year_f12_0', 'day_of_year_f104_0',\n",
    "       'day_of_year_f24_0', 'day_of_year_f52_0', 'weekday', 'month'\n",
    "       ]\n",
    "\n",
    "trainF2 = trainF + ['sales']\n",
    "\n",
    "\n",
    "\n",
    "n_predictedValues = 16\n",
    "look_back = 100\n",
    "zScoreNorm = True\n",
    "\n",
    "# Date string\n",
    "date_string = \"2017-05-01\"\n",
    "date_object = datetime.strptime(date_string, '%Y-%m-%d')\n",
    "days_ago = date_object - timedelta(days=(look_back + n_predictedValues -1 +2))\n",
    "days_ago_string = days_ago.strftime('%Y-%m-%d')\n",
    "days_ago_string\n",
    "\n",
    "maskTrain = data.date < date_string\n",
    "maskTest = data.date  > days_ago_string #\"2017-03-03\" #42days + 15 day between (15 because we only want to iterate one value more from the test set)\n",
    "\n",
    "log = {}\n",
    "\n",
    "train = data.loc[data.dataT == 'train']\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "stdDict = {}\n",
    "meanDict= {}\n",
    "X_train = []\n",
    "X_test  = []\n",
    "y_train = []\n",
    "y_test  = []\n",
    "std_train, std_test = [], []\n",
    "mean_train, mean_test = [], []\n",
    "init= False\n",
    "for familyId in data.family.unique():\n",
    "       # start with only some families\n",
    "       if familyId > 8:\n",
    "          continue\n",
    "\n",
    "       print(familyId)\n",
    "       familyDf = train.loc[train.family==familyId]  \n",
    "\n",
    "       for storeId in data.store_nbr.unique():\n",
    "              storeDf = familyDf.loc[(familyDf.store_nbr == storeId)] \n",
    "              X_train0,y_train0,mean,std = getSequencesFast(storeDf.loc[maskTrain], trainF, look_back, n_predictedValues, zScoreNorm=zScoreNorm)\n",
    "              X_test0, y_test0           = getSequencesFast(storeDf.loc[maskTest], trainF, look_back, n_predictedValues, zScoreNorm=False, applyZScoreNorm=True, meanZ=mean, stdZ=std)       \n",
    "              stdDict[storeId] = std\n",
    "              meanDict[storeId] = mean\n",
    "              if init:\n",
    "                     X_train = np.concatenate((X_train, X_train0), axis=0)\n",
    "                     X_test  = np.concatenate((X_test,  X_test0), axis=0)\n",
    "                     y_train = np.concatenate((y_train, y_train0), axis=0)\n",
    "                     y_test  = np.concatenate((y_test,  y_test0), axis=0)\n",
    "                     std_train = np.concatenate((std_train, np.ones(y_train0.shape)*std), axis=0)\n",
    "                     std_test  = np.concatenate((std_test,  np.ones(y_test0.shape)*std), axis=0)\n",
    "                     mean_train = np.concatenate((mean_train, np.ones(y_train0.shape)*mean), axis=0)\n",
    "                     mean_test = np.concatenate((mean_test, np.ones(y_test0.shape)*mean), axis=0)\n",
    "              else:\n",
    "                     X_train, X_test, y_train,y_test = X_train0, X_test0, y_train0, y_test0\n",
    "                     std_train  =np.ones(y_train0.shape)*std\n",
    "                     std_test   =np.ones(y_test0.shape)*std\n",
    "                     mean_train =np.ones(y_train0.shape)*mean\n",
    "                     mean_test  =np.ones(y_test0.shape)*mean\n",
    "                     init=True \n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('train_test_Sequences_100_16_StdNormalized_fam0-7.npz', arr1=X_train, arr2=y_train, arr3=std_train, arr4=X_test, arr5=y_test, arr6=std_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters for LGBM model\n",
    "params = {\n",
    "    'objective': 'regression',  # Assuming you're doing regression\n",
    "    'metric': 'rmsle',  # Mean squared error\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': 0\n",
    "}   \n",
    "\n",
    "# Train the model\n",
    "num_round = 10  \n",
    "gbms = [lgb.train(params, lgb.Dataset(X_train, label=y_train[:, i]),num_round, valid_sets=[lgb.Dataset(X_test, label=y_test[:,i])]) for i in range(y_train.shape[1])]   \n",
    "\n",
    "\n",
    "forecast = np.column_stack([gbm.predict(X_train, num_iteration=gbm.best_iteration) for gbm in gbms])\n",
    "if (forecast<0).any():\n",
    "    print('negative values!!!')\n",
    "    forecast = np.clip(forecast, 0, 1e29)\n",
    "if zScoreNorm:\n",
    "    forecast = forecast *std_train  + mean_train\n",
    "    y_train = y_train *std_train + mean_train\n",
    "rmsleTrain = np.sqrt(mean_squared_log_error(forecast, y_train))\n",
    "forecast = np.column_stack([gbm.predict(X_test, num_iteration=gbm.best_iteration) for gbm in gbms])\n",
    "if zScoreNorm:\n",
    "    forecast = forecast *std_test  + mean_test\n",
    "    y_test = y_test*std_test + mean_test\n",
    "rmsleTest = np.sqrt(mean_squared_log_error(forecast, y_test))\n",
    "print('familyId:', familyId, 'errors:  ', round(rmsleTrain,3), round(rmsleTest,3), y_train.shape[0], y_test.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# facebook prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fbprophet import Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is a pandas dataframe with 'ds' and 'y' columns\n",
    "m = Prophet()\n",
    "m.fit(df)\n",
    "\n",
    "# Make future dataframe for 16 periods\n",
    "future = m.make_future_dataframe(periods=16)\n",
    "forecast = m.predict(future)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "\n",
    "idea: we could use positional embeddings to define the relation between store and product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.set_random_seed(42)\n",
    "\n",
    "# Define the input\n",
    "inputs = Input(shape=(1,), dtype=tf.string)\n",
    "\n",
    "\n",
    "# Create a Transformer layer\n",
    "transformer_layer = tfa.layers.transformer.Transformer(num_layers=2, num_heads=2, \n",
    "                                                       dim_model=32, dim_feedforward=128)\n",
    "transformer_outputs = transformer_layer(tokenized_inputs)\n",
    "\n",
    "# Create a dense layer for the output\n",
    "outputs = Dense(num_classes, activation='softmax')(transformer_outputs)\n",
    "\n",
    "# Define the model\n",
    "model = Model(inputs, outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Normalization and Attention\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "    )(x, x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    # Feed Forward Part\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "    return x + res\n",
    "\n",
    "def build_model(\n",
    "    input_shape_oldS,\n",
    "    input_shape_newS,\n",
    "    head_size,\n",
    "    num_heads,\n",
    "    ff_dim,\n",
    "    num_transformer_blocks,\n",
    "    mlp_units,\n",
    "    dropout=0,\n",
    "    mlp_dropout=0,\n",
    "):\n",
    "    inputs = keras.Input(shape=input_shape_oldS)\n",
    "    x = inputs\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "    for dim in mlp_units:\n",
    "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(mlp_dropout)(x)\n",
    "    outputs = layers.Dense(1)(x)\n",
    "    return keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transformer from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2d positional encoding (feature & position)\n",
    "def positional_encoding(length, depth):\n",
    "  depth = depth/2\n",
    "\n",
    "  positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
    "  depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n",
    "\n",
    "  angle_rates = 1 / (10000**depths)         # (1, depth)\n",
    "  angle_rads = positions * angle_rates      # (pos, depth)\n",
    "\n",
    "  pos_encoding = np.concatenate(\n",
    "      [np.sin(angle_rads), np.cos(angle_rads)],\n",
    "      axis=-1) \n",
    "\n",
    "  return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "  def __init__(self, sequence_length, d_model):\n",
    "    super().__init__()\n",
    "    self.d_model = d_model #d_model or model depth = dimensionality of encoding vector\n",
    "    # no need for an embedding layer since my input is a time series and not a word token\n",
    "    #self.embedding = tf.keras.layers.Embedding(sequence_length, d_model, mask_zero=True) \n",
    "    self.pos_encoding = positional_encoding(length=sequence_length, depth=d_model)\n",
    "\n",
    "    #TODO implement an encoding for the store and family\n",
    "    #self.store_fam_encoding\n",
    "\n",
    "  def compute_mask(self, *args, **kwargs):\n",
    "    return self.embedding.compute_mask(*args, **kwargs)\n",
    "\n",
    "  def call(self, x):\n",
    "    length = tf.shape(x)[1]\n",
    "\n",
    "    #x = self.embedding(x)\n",
    "    # This factor sets the relative scale of the embedding and positonal_encoding.\n",
    "    #x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "\n",
    "    x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, **kwargs):\n",
    "    super().__init__()\n",
    "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "    self.add = tf.keras.layers.Add()\n",
    "\n",
    "class CrossAttention(BaseAttention):\n",
    "  def call(self, x, context):\n",
    "    attn_output, attn_scores = self.mha(\n",
    "        query=x,\n",
    "        key=context,\n",
    "        value=context,\n",
    "        return_attention_scores=True)\n",
    "\n",
    "    # Cache the attention scores for plotting later.\n",
    "    self.last_attn_scores = attn_scores\n",
    "\n",
    "    x = self.add([x, attn_output])\n",
    "    x = self.layernorm(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "class GlobalSelfAttention(BaseAttention):\n",
    "  def call(self, x):\n",
    "    attn_output = self.mha(\n",
    "        query=x,\n",
    "        value=x,\n",
    "        key=x)\n",
    "    x = self.add([x, attn_output])\n",
    "    x = self.layernorm(x)\n",
    "    return x\n",
    "\n",
    "# only connect sequence to past & not to future\n",
    "class CausalSelfAttention(BaseAttention):\n",
    "  def call(self, x):\n",
    "    attn_output = self.mha(\n",
    "        query=x,\n",
    "        value=x,\n",
    "        key=x,\n",
    "        use_causal_mask = True)\n",
    "    x = self.add([x, attn_output])\n",
    "    x = self.layernorm(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, dff, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "    self.seq = tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),\n",
    "      tf.keras.layers.Dense(d_model),\n",
    "      tf.keras.layers.Dropout(dropout_rate)\n",
    "    ])\n",
    "    self.add = tf.keras.layers.Add()\n",
    "    self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.add([x, self.seq(x)])\n",
    "    x = self.layer_norm(x) \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "\n",
    "    self.self_attention = GlobalSelfAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=d_model,\n",
    "        dropout=dropout_rate)\n",
    "\n",
    "    self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.self_attention(x)\n",
    "    x = self.ffn(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, *, num_layers, d_model, num_heads,\n",
    "               dff, sequence_length, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "\n",
    "    self.pos_embedding = PositionalEmbedding(\n",
    "        sequence_length=sequence_length, d_model=d_model)\n",
    "\n",
    "    self.enc_layers = [\n",
    "        EncoderLayer(d_model=d_model,\n",
    "                     num_heads=num_heads,\n",
    "                     dff=dff,\n",
    "                     dropout_rate=dropout_rate)\n",
    "        for _ in range(num_layers)]\n",
    "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "  def call(self, x):\n",
    "    # `x` is token-IDs shape: (batch, seq_len)\n",
    "    x = self.pos_embedding(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
    "\n",
    "    # Add dropout.\n",
    "    x = self.dropout(x)\n",
    "\n",
    "    for i in range(self.num_layers):\n",
    "      x = self.enc_layers[i](x)\n",
    "\n",
    "    return x  # Shape `(batch_size, seq_len, d_model)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self,\n",
    "               *,\n",
    "               d_model,\n",
    "               num_heads,\n",
    "               dff,\n",
    "               dropout_rate=0.1):\n",
    "    super(DecoderLayer, self).__init__()\n",
    "\n",
    "    self.causal_self_attention = CausalSelfAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=d_model,\n",
    "        dropout=dropout_rate)\n",
    "\n",
    "    self.cross_attention = CrossAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=d_model,\n",
    "        dropout=dropout_rate)\n",
    "\n",
    "    self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "  def call(self, x, context):\n",
    "    x = self.causal_self_attention(x=x)\n",
    "    x = self.cross_attention(x=x, context=context)\n",
    "\n",
    "    # Cache the last attention scores for plotting later\n",
    "    self.last_attn_scores = self.cross_attention.last_attn_scores\n",
    "\n",
    "    x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, *, num_layers, d_model, num_heads, dff, sequence_length,\n",
    "               dropout_rate=0.1):\n",
    "    super(Decoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "\n",
    "    self.pos_embedding = PositionalEmbedding(sequence_length=sequence_length,\n",
    "                                             d_model=d_model)\n",
    "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "    self.dec_layers = [\n",
    "        DecoderLayer(d_model=d_model, num_heads=num_heads,\n",
    "                     dff=dff, dropout_rate=dropout_rate)\n",
    "        for _ in range(num_layers)]\n",
    "\n",
    "    self.last_attn_scores = None\n",
    "\n",
    "  def call(self, x, context):\n",
    "    # `x` is token-IDs shape (batch, target_seq_len)\n",
    "    x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "    x = self.dropout(x)\n",
    "\n",
    "    for i in range(self.num_layers):\n",
    "      x  = self.dec_layers[i](x, context)\n",
    "\n",
    "    self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n",
    "\n",
    "    # The shape of x is (batch_size, target_seq_len, d_model).\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "  def __init__(self, *, enc_dec_num_layers, input_seq_dim_enc, input_seq_dim_dec, num_heads, fully_connected_size,\n",
    "               input_sequence_len, output_sequence_len, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "    self.encoder = Encoder(num_layers=enc_dec_num_layers, d_model=input_seq_dim_enc,\n",
    "                           num_heads=num_heads, dff=dff,\n",
    "                           sequence_length=input_sequence_len,\n",
    "                           dropout_rate=dropout_rate)\n",
    "\n",
    "    self.decoder = Decoder(num_layers=enc_dec_num_layers, d_model=input_seq_dim_dec,\n",
    "                           num_heads=num_heads, dff=dff,\n",
    "                           sequence_length=output_sequence_len,\n",
    "                           dropout_rate=dropout_rate)\n",
    "\n",
    "    self.final_layer = tf.keras.layers.Dense(output_sequence_len)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    # To use a Keras model with `.fit` you must pass all your inputs in the\n",
    "    # first argument.\n",
    "    context, x  = inputs\n",
    "\n",
    "    context = self.encoder(context)  # (batch_size, context_len, d_model)\n",
    "\n",
    "    x = self.decoder(x, context)  # (batch_size, target_len, d_model)\n",
    "\n",
    "    # Final linear layer output.\n",
    "    logits = self.final_layer(x)  # (batch_size, target_len, target_vocab_size)\n",
    "\n",
    "    try:\n",
    "      # Drop the keras mask, so it doesn't scale the losses/metrics.\n",
    "      # b/250038731\n",
    "      del logits._keras_mask\n",
    "    except AttributeError:\n",
    "      pass\n",
    "\n",
    "    # Return the final output and the attention weights.\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    dff=dff,\n",
    "    input_vocab_size=tokenizers.pt.get_vocab_size().numpy(),\n",
    "    target_vocab_size=tokenizers.en.get_vocab_size().numpy(),\n",
    "    dropout_rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.set_random_seed(42)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(GRU(32, return_sequences=True, input_shape=(100, 20)))\n",
    "model.add(GRU(32))\n",
    "model.add(Dense(16))\n",
    "\n",
    "model.compile(loss='rmse', optimizer='adam')\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32,validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.set_random_seed(42)\n",
    "\n",
    "# Define an input sequence.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "\n",
    "# Create a GRU layer\n",
    "encoder = GRU(latent_dim, return_state=True)\n",
    "\n",
    "# Get the output and state\n",
    "encoder_outputs, state_h = encoder(encoder_inputs)\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "\n",
    "# Create another GRU layer\n",
    "decoder_gru = GRU(latent_dim, return_sequences=True, return_state=True)\n",
    "\n",
    "# Get the output and state\n",
    "decoder_outputs, _ = decoder_gru(decoder_inputs, initial_state=state_h)\n",
    "\n",
    "# Create a dense layer\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "\n",
    "# Get the final output\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.set_random_seed(42)\n",
    "\n",
    "# Define an input sequence.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "\n",
    "# Create a LSTM layer\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "\n",
    "# Get the output and states\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "\n",
    "# Create another LSTM layer\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "\n",
    "# Get the output and states\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "\n",
    "# Create a dense layer\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "\n",
    "# Get the final output\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.13 ('storeSales')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "17f27e0cbed1f29822e509a94c958ed0e80b7d9abe162097755686a614de4732"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
