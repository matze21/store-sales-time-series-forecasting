{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import plotly.offline as pyo\n",
    "from plotly import subplots\n",
    "import plotly.graph_objects as go\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.tsa.api as smt\n",
    "\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from tensorflow.keras.models import Sequential\n",
    "#from tensorflow.keras.layers import LSTM, Dense,Input,concatenate\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import lightgbm as lgb\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "from baseFunctions import *\n",
    "from data_helpers import processData6, featureEngineering, getSequencesFast, removeOutliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, propDicts, flippedPropDicts = processData6()\n",
    "data, timeFeatures = featureEngineering(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainF = [\n",
    "       'store_nbr', 'family', \n",
    "       #'sales', \n",
    "       'onpromotion',# 'dataT',\n",
    "       'city', 'state', 'type', 'cluster', 'dcoilwtico', 'holidayType',\n",
    "       'description', 'transferred', \n",
    "       #'transactions', \n",
    "       'store_closed',\n",
    "       'linear_time', 'day_of_year', 'day_of_year_f12_0', 'day_of_year_f104_0',\n",
    "       'day_of_year_f24_0', 'day_of_year_f52_0', 'weekday', 'month'\n",
    "       ]\n",
    "\n",
    "trainF2 = trainF + ['sales']\n",
    "\n",
    "\n",
    "\n",
    "n_predictedValues = 16\n",
    "look_back = 100\n",
    "zScoreNorm = False\n",
    "logTransform = True\n",
    "\n",
    "# Date string\n",
    "date_string = \"2017-05-01\"\n",
    "date_object = datetime.strptime(date_string, '%Y-%m-%d')\n",
    "days_ago = date_object - timedelta(days=(look_back + n_predictedValues -1 +2))\n",
    "days_ago_string = days_ago.strftime('%Y-%m-%d')\n",
    "days_ago_string\n",
    "\n",
    "maskTrain = data.date < date_string\n",
    "maskTest = data.date  > days_ago_string #\"2017-03-03\" #42days + 15 day between (15 because we only want to iterate one value more from the test set)\n",
    "\n",
    "log = {}\n",
    "\n",
    "train = data.loc[data.dataT == 'train']\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "stdDict = {}\n",
    "meanDict= {}\n",
    "X_train = []\n",
    "X_test  = []\n",
    "y_train = []\n",
    "y_test  = []\n",
    "std_train, std_test = [], []\n",
    "mean_train, mean_test = [], []\n",
    "init= False\n",
    "for familyId in [0]: #data.family.unique():\n",
    "       # start with only some families\n",
    "       if familyId > 8:\n",
    "          continue\n",
    "\n",
    "       print(familyId)\n",
    "       familyDf = train.loc[train.family==familyId]  \n",
    "\n",
    "       for storeId in data.store_nbr.unique():\n",
    "              storeDf = familyDf.loc[(familyDf.store_nbr == storeId)] \n",
    "\n",
    "              # ln tranformation\n",
    "              if logTransform:\n",
    "                     storeDf.loc[:,'sales'] = np.log(storeDf.sales + 1)\n",
    "              if zScoreNorm:\n",
    "                     X_train0,y_train0,mean,std = getSequencesFast(storeDf.loc[maskTrain], trainF, look_back, n_predictedValues, zScoreNorm=zScoreNorm)\n",
    "                     X_test0, y_test0           = getSequencesFast(storeDf.loc[maskTest], trainF, look_back, n_predictedValues, zScoreNorm=False, applyZScoreNorm=True, meanZ=mean, stdZ=std)\n",
    "                     stdDict[storeId] = std\n",
    "                     meanDict[storeId] = mean\n",
    "              else:\n",
    "                     X_train0,y_train0 = getSequencesFast(storeDf.loc[maskTrain], trainF, look_back, n_predictedValues, zScoreNorm=False)\n",
    "                     X_test0, y_test0  = getSequencesFast(storeDf.loc[maskTest], trainF, look_back, n_predictedValues, zScoreNorm=False, applyZScoreNorm=False)  \n",
    "              \n",
    "              if init:\n",
    "                     X_train = np.concatenate((X_train, X_train0), axis=0)\n",
    "                     X_test  = np.concatenate((X_test,  X_test0), axis=0)\n",
    "                     y_train = np.concatenate((y_train, y_train0), axis=0)\n",
    "                     y_test  = np.concatenate((y_test,  y_test0), axis=0)\n",
    "                     if zScoreNorm:\n",
    "                            std_train = np.concatenate((std_train, np.ones(y_train0.shape)*std), axis=0)\n",
    "                            std_test  = np.concatenate((std_test,  np.ones(y_test0.shape)*std), axis=0)\n",
    "                            mean_train = np.concatenate((mean_train, np.ones(y_train0.shape)*mean), axis=0)\n",
    "                            mean_test = np.concatenate((mean_test, np.ones(y_test0.shape)*mean), axis=0)\n",
    "              else:\n",
    "                     X_train, X_test, y_train,y_test = X_train0, X_test0, y_train0, y_test0\n",
    "                     if zScoreNorm:\n",
    "                            std_train  =np.ones(y_train0.shape)*std\n",
    "                            std_test   =np.ones(y_test0.shape)*std\n",
    "                            mean_train =np.ones(y_train0.shape)*mean\n",
    "                            mean_test  =np.ones(y_test0.shape)*mean\n",
    "                     init=True \n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('train_test_Sequences_100_16_StdNormalized_fam0-7.npz', arr1=X_train, arr2=y_train, arr3=std_train, arr4=X_test, arr5=y_test, arr6=std_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LGBM\n",
    "\n",
    "using a ln transformation and rmse vs. rmsle: -> maybe a bit less overfitting (0.628/0.561 vs. 0.581/0.588)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters for LGBM model\n",
    "params = {\n",
    "    'objective': 'regression',  # Assuming you're doing regression\n",
    "    'metric': 'rmsle',  # Mean squared error\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': -1\n",
    "}   \n",
    "\n",
    "# Train the model\n",
    "num_round = 10  \n",
    "gbms = [lgb.train(params, lgb.Dataset(X_train, label=y_train[:, i]),num_round, valid_sets=[lgb.Dataset(X_test, label=y_test[:,i])]) for i in range(y_train.shape[1])]   \n",
    "\n",
    "\n",
    "forecast = np.column_stack([gbm.predict(X_train, num_iteration=gbm.best_iteration) for gbm in gbms])\n",
    "if (forecast<0).any():\n",
    "    print('negative values!!!')\n",
    "    forecast = np.clip(forecast, 0, 1e29)\n",
    "if zScoreNorm:\n",
    "    forecast = forecast *std_train  + mean_train\n",
    "    y_train = y_train *std_train + mean_train\n",
    "rmsleTrain = np.sqrt(mean_squared_log_error(forecast, y_train))\n",
    "forecast = np.column_stack([gbm.predict(X_test, num_iteration=gbm.best_iteration) for gbm in gbms])\n",
    "if zScoreNorm:\n",
    "    forecast = forecast *std_test  + mean_test\n",
    "    y_test = y_test*std_test + mean_test\n",
    "rmsleTest = np.sqrt(mean_squared_log_error(forecast, y_test))\n",
    "print('familyId:', familyId, 'errors:  ', round(rmsleTrain,3), round(rmsleTest,3), y_train.shape[0], y_test.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOG version\n",
    "params = {\n",
    "    'objective': 'regression',  # Assuming you're doing regression\n",
    "    'metric': 'rmse',#'rmsle',  # Mean squared error\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': -1\n",
    "}   \n",
    "\n",
    "# Train the model\n",
    "num_round = 100  \n",
    "gbms = [lgb.train(params, lgb.Dataset(X_train, label=y_train[:, i]),num_round, verbose_eval=False, valid_sets=[lgb.Dataset(X_test, label=y_test[:,i])]) for i in range(y_train.shape[1])]   \n",
    "\n",
    "\n",
    "forecast = np.column_stack([gbm.predict(X_train, num_iteration=gbm.best_iteration) for gbm in gbms])\n",
    "if logTransform:\n",
    "    forecast0 = np.exp(forecast) - 1\n",
    "    y_train0  = np.exp(y_train) - 1\n",
    "    rmsleTrain = np.sqrt(mean_squared_log_error(forecast0, y_train0))\n",
    "forecast = np.column_stack([gbm.predict(X_test, num_iteration=gbm.best_iteration) for gbm in gbms])\n",
    "if logTransform:\n",
    "    forecast1 = np.exp(forecast) - 1\n",
    "    y_test1   = np.exp(y_test) - 1\n",
    "    rmsleTest = np.sqrt(mean_squared_log_error(forecast1, y_test1))\n",
    "print('familyId:', familyId, 'errors:  ', round(rmsleTrain,3), round(rmsleTest,3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#familyId: 0 errors:   0.695 0.634"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# facebook prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fbprophet import Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is a pandas dataframe with 'ds' and 'y' columns\n",
    "m = Prophet()\n",
    "m.fit(df)\n",
    "\n",
    "# Make future dataframe for 16 periods\n",
    "future = m.make_future_dataframe(periods=16)\n",
    "forecast = m.predict(future)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "\n",
    "idea: we could use positional embeddings to define the relation between store and product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training data for transformer / nn architectures\n",
    "from data_helpers import create_sequences\n",
    "def getSequencesFastNOTFlattened(train, trainF, look_back, n_predictedValues, zScoreNorm = False, logTransf=False, applyZScoreNorm = False, meanZ = 0, stdZ = 0,):\n",
    "    # zscore over all values -> not ideal bc test data\n",
    "    if zScoreNorm:\n",
    "        #mean = train.sales.mean()\n",
    "        mean = 0 # modified zScore, not in mean = 0\n",
    "        std = max(train.sales.std(), 1)\n",
    "        train.loc[:,'sales'] = (train.sales - mean) / std\n",
    "    if applyZScoreNorm:\n",
    "        train.loc[:,'sales'] = (train.sales-meanZ)/stdZ\n",
    "    if logTransf:\n",
    "        train.loc[:,'sales'] = np.log(train.sales + 1)\n",
    "\n",
    "    trainF2 = trainF + ['sales']\n",
    "\n",
    "\n",
    "    pastS   = create_sequences(train[trainF2].to_numpy(), look_back)                                   #past sequence\n",
    "    futureS = create_sequences(train[trainF].iloc[look_back:-1].to_numpy(), n_predictedValues)        #future sequence\n",
    "    label   = create_sequences(train[['sales']].iloc[look_back:-1].to_numpy(), n_predictedValues) #label\n",
    "\n",
    "    maxLen = train.shape[0] - look_back-n_predictedValues\n",
    "    pastS = pastS[0:maxLen,:,:].transpose([0,2,1])\n",
    "    futureS = futureS[0:maxLen,:,:].transpose([0,2,1])\n",
    "    label = label[0:maxLen,:].transpose([0,2,1])\n",
    "\n",
    "    if zScoreNorm:\n",
    "        return pastS, futureS, label, mean, std\n",
    "    elif applyZScoreNorm:\n",
    "        return pastS, futureS, label\n",
    "    else:\n",
    "        return pastS, futureS, label\n",
    "\n",
    "trainF = [\n",
    "       'store_nbr', 'family', \n",
    "       #'sales', \n",
    "       'onpromotion',# 'dataT',\n",
    "       'city', 'state', 'type', 'cluster', 'dcoilwtico', 'holidayType',\n",
    "       'description', 'transferred', \n",
    "       #'transactions', \n",
    "       'store_closed',\n",
    "       'linear_time', 'day_of_year', 'day_of_year_f12_0', 'day_of_year_f104_0',\n",
    "       'day_of_year_f24_0', 'day_of_year_f52_0', 'weekday', 'month'\n",
    "       ]\n",
    "\n",
    "trainF2 = trainF + ['sales']\n",
    "\n",
    "\n",
    "\n",
    "n_predictedValues = 16\n",
    "look_back = 100\n",
    "zScoreNorm = False\n",
    "logTransform = True\n",
    "\n",
    "# Date string\n",
    "date_string = \"2017-05-01\"\n",
    "date_object = datetime.strptime(date_string, '%Y-%m-%d')\n",
    "days_ago = date_object - timedelta(days=(look_back + n_predictedValues -1 +2))\n",
    "days_ago_string = days_ago.strftime('%Y-%m-%d')\n",
    "days_ago_string\n",
    "\n",
    "maskTrain = data.date < date_string\n",
    "maskTest = data.date  > days_ago_string #\"2017-03-03\" #42days + 15 day between (15 because we only want to iterate one value more from the test set)\n",
    "\n",
    "log = {}\n",
    "\n",
    "train = data.loc[data.dataT == 'train']\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "stdDict = {}\n",
    "meanDict= {}\n",
    "X_trainS0 = []\n",
    "X_testS0  = []\n",
    "X_trainS1 = []\n",
    "X_testS1  = []\n",
    "y_train = []\n",
    "y_test  = []\n",
    "std_train, std_test = [], []\n",
    "mean_train, mean_test = [], []\n",
    "init= False\n",
    "for familyId in [0]: #data.family.unique():\n",
    "       print(familyId)\n",
    "       familyDf = train.loc[train.family==familyId]  \n",
    "\n",
    "       for storeId in data.store_nbr.unique():\n",
    "              if storeId >1:\n",
    "                continue\n",
    "              print(storeId)\n",
    "              storeDf = familyDf.loc[(familyDf.store_nbr == storeId)] \n",
    "              if zScoreNorm:\n",
    "                X_train0,X_train1,y_train0,mean,std = getSequencesFastNOTFlattened(storeDf.loc[maskTrain], trainF, look_back, n_predictedValues, zScoreNorm=zScoreNorm)\n",
    "                X_test0,X_test1, y_test0            = getSequencesFastNOTFlattened(storeDf.loc[maskTest], trainF, look_back, n_predictedValues, zScoreNorm=False, applyZScoreNorm=True, meanZ=mean, stdZ=std)       \n",
    "                stdDict[storeId] = std\n",
    "                meanDict[storeId] = mean\n",
    "              else:\n",
    "                X_train0,X_train1,y_train0 = getSequencesFastNOTFlattened(storeDf.loc[maskTrain], trainF, look_back, n_predictedValues, zScoreNorm=False, logTransf=logTransform)\n",
    "                X_test0,X_test1, y_test0   = getSequencesFastNOTFlattened(storeDf.loc[maskTest], trainF, look_back, n_predictedValues, zScoreNorm=False, logTransf=logTransform, applyZScoreNorm=False, meanZ=0, stdZ=0) \n",
    "              if init:\n",
    "                     X_trainS0 = np.concatenate((X_trainS0, X_train0), axis=0)\n",
    "                     X_testS0  = np.concatenate((X_testS0,  X_test0), axis=0)\n",
    "                     X_trainS1 = np.concatenate((X_trainS1, X_train1), axis=0)\n",
    "                     X_testS1  = np.concatenate((X_testS1,  X_test1), axis=0)\n",
    "                     y_train = np.concatenate((y_train, y_train0), axis=0)\n",
    "                     y_test  = np.concatenate((y_test,  y_test0), axis=0)\n",
    "                     if  zScoreNorm:\n",
    "                        std_train = np.concatenate((std_train, np.ones(y_train0.shape)*std), axis=0)\n",
    "                        std_test  = np.concatenate((std_test,  np.ones(y_test0.shape)*std), axis=0)\n",
    "                        mean_train = np.concatenate((mean_train, np.ones(y_train0.shape)*mean), axis=0)\n",
    "                        mean_test = np.concatenate((mean_test, np.ones(y_test0.shape)*mean), axis=0)\n",
    "              else:\n",
    "                     X_trainS0, X_trainS1 = X_train0, X_train1\n",
    "                     X_testS0, X_testS1   = X_test0, X_test1\n",
    "                     y_train,y_test = y_train0, y_test0\n",
    "                     if zScoreNorm:\n",
    "                        std_train  =np.ones(y_train0.shape)*std\n",
    "                        std_test   =np.ones(y_test0.shape)*std\n",
    "                        mean_train =np.ones(y_train0.shape)*mean\n",
    "                        mean_test  =np.ones(y_test0.shape)*mean\n",
    "                     init=True \n",
    "\n",
    "X_test = [X_testS0, X_testS1]\n",
    "X_train = [X_trainS0, X_trainS1]\n",
    "X_train[0].shape\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainS0.shape, X_testS0.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transformer from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "print(module_path)\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Transformer import encoder, decoder, embedding, attention, feed_forward\n",
    "from Transformer import transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sanity checks\n",
    "embed_pt = PositionalEmbedding(sequence_length=100, d_model=21)\n",
    "\n",
    "s0 = embed_pt(X_trainS0[0:10,:,:])\n",
    "\n",
    "embed_pt = PositionalEmbedding(sequence_length=16, d_model=20)\n",
    "\n",
    "s1 = embed_pt(X_trainS1[0:10,:,:])\n",
    "print('embedding',s1.shape)\n",
    "\n",
    "sample_ca = CrossAttention(num_heads=2, key_dim=512)\n",
    "\n",
    "print('crossAttnetion',sample_ca(x=s1,context=s0).shape)\n",
    "\n",
    "# Instantiate the encoder.\n",
    "sample_encoder = Encoder(num_layers=4,\n",
    "                         d_model=21,\n",
    "                         num_heads=8,\n",
    "                         dff=64,\n",
    "                         sequence_length=100)\n",
    "\n",
    "sample_encoder_output = sample_encoder(X_trainS0[0:10,:,:], training=False)\n",
    "\n",
    "print('encoder',sample_encoder_output.shape)  # Shape `(batch_size, input_seq_len, d_model)`.\n",
    "\n",
    "# Instantiate the decoder.\n",
    "sample_decoder = Decoder(num_layers=4,\n",
    "                         d_model=20,\n",
    "                         num_heads=8,\n",
    "                         dff=64,\n",
    "                         sequence_length=16)\n",
    "\n",
    "output = sample_decoder(\n",
    "    x=s1,\n",
    "    context=s0)\n",
    "\n",
    "# Print the shapes.\n",
    "print('decoder',output.shape)\n",
    "\n",
    "transformer = Transformer(\n",
    "    enc_dec_num_layers=1,\n",
    "    input_seq_dim_enc = 21,#X_trainS0.shape[2],\n",
    "    input_seq_dim_dec = 20,#X_trainS1.shape[2],\n",
    "    num_heads=8,\n",
    "    fully_connected_size=64,\n",
    "    input_sequence_len=look_back,\n",
    "    output_sequence_len=1, #n_predictedValues,\n",
    "    dropout_rate=0)\n",
    "    \n",
    "output = transformer(([X_trainS0[0:10,:,:],X_trainS1[0:10,:,:]]))\n",
    "\n",
    "print('transformer',output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.set_random_seed(42)\n",
    "\n",
    "\n",
    "\n",
    "transformer = Transformer(\n",
    "    enc_dec_num_layers=1,\n",
    "    input_seq_dim_enc = X_trainS0.shape[2],\n",
    "    input_seq_dim_dec = X_trainS1.shape[2],\n",
    "    num_heads=8,\n",
    "    fully_connected_size=64,\n",
    "    input_sequence_len=look_back,\n",
    "    output_sequence_len=1, #n_predictedValues,\n",
    "    dropout_rate=0)\n",
    "\n",
    "transformer.compile(\n",
    "    loss=root_mean_squared_error,  #if I log transform my data, the loss is just rmse\n",
    "    optimizer='adam',\n",
    "    metrics=['mae'])\n",
    "\n",
    "transformer.fit(X_train, y_train,\n",
    "    epochs=10,\n",
    "    validation_data=(X_test, y_test), batch_size=64)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast = transformer.predict(X_train)\n",
    "if (forecast<0).any():\n",
    "    print('negative values!!!')\n",
    "    forecast = np.clip(forecast, 0, 1e29)\n",
    "if zScoreNorm:\n",
    "    forecast = forecast *std_train  + mean_train\n",
    "    y_train = y_train *std_train + mean_train\n",
    "rmsleTrain = np.sqrt(mean_squared_log_error(np.reshape(forecast,(-1,16)), np.reshape(y_train, (-1,16))))\n",
    "forecast = transformer.predict(X_test)\n",
    "if zScoreNorm:\n",
    "    forecast = forecast *std_test  + mean_test\n",
    "    y_test = y_test*std_test + mean_test\n",
    "rmsleTest = np.sqrt(mean_squared_log_error(np.reshape(forecast,(-1,16)), np.reshape(y_test, (-1,16))))\n",
    "print('errors:  ', round(rmsleTrain,3), round(rmsleTest,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.set_random_seed(42)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(GRU(32, return_sequences=True, input_shape=(100, 20)))\n",
    "model.add(GRU(32))\n",
    "model.add(Dense(16))\n",
    "\n",
    "model.compile(loss='rmse', optimizer='adam')\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32,validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.set_random_seed(42)\n",
    "\n",
    "# Define an input sequence.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "\n",
    "# Create a GRU layer\n",
    "encoder = GRU(latent_dim, return_state=True)\n",
    "\n",
    "# Get the output and state\n",
    "encoder_outputs, state_h = encoder(encoder_inputs)\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "\n",
    "# Create another GRU layer\n",
    "decoder_gru = GRU(latent_dim, return_sequences=True, return_state=True)\n",
    "\n",
    "# Get the output and state\n",
    "decoder_outputs, _ = decoder_gru(decoder_inputs, initial_state=state_h)\n",
    "\n",
    "# Create a dense layer\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "\n",
    "# Get the final output\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.set_random_seed(42)\n",
    "\n",
    "# Define an input sequence.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "\n",
    "# Create a LSTM layer\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "\n",
    "# Get the output and states\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "\n",
    "# Create another LSTM layer\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "\n",
    "# Get the output and states\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "\n",
    "# Create a dense layer\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "\n",
    "# Get the final output\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "17f27e0cbed1f29822e509a94c958ed0e80b7d9abe162097755686a614de4732"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
